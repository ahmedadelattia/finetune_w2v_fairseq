w2v_name: xlsr2_300m
model_path: /home/ahmed/Research/Projects/pretrain_w2v_fairseq/base_models/xlsr2_300m.pt
wandb_project: finetune_w2v2_new_config_xlsr2_300m
config_name: large
Current directory: /home/ahmed/Research/Projects/finetune_w2v_fairseq/model_outputsxlsr2_300m/2944
outdir_fold: ./model_outputsxlsr2_300m/2944
Running on fold: 2944
finetune.sh: line 35: =./2944: No such file or directory
finetune.sh: line 50: [: missing `]'
finetune.sh: line 64: /home/ahmed/Research/Projects/finetune_w2v_fairseq/config/: Is a directory
finetune.sh: line 68: unexpected EOF while looking for matching `"'
finetune.sh: line 71: syntax error: unexpected end of file
dev:
Current directory: /home/ahmed/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining
save_dir: ckpts
Multiple files found, taking the latest one
outputs/2024-09-17/15-16-26/ckpts/checkpoint_359_10000.pt
outputs/2024-09-17/15-13-50/ckpts/checkpoint_359_10000.pt
outputs/2024-09-17/15-01-22/ckpts/checkpoint_359_10000.pt
outputs/2024-09-17/15-00-21/ckpts/checkpoint_359_10000.pt
outputs/2024-09-17/14-50-43/ckpts/checkpoint_359_10000.pt
outputs/2024-09-16/20-48-55/ckpts/checkpoint_359_10000.pt
restore_file: /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/15-16-26/ckpts/checkpoint_359_10000.pt
w2v_name: /home/ahmed/Research/Projects/finetune_w2v_fairseq/cpt_models/w2v_robust_1M
model_path: /home/ahmed/Research/Projects/finetune_w2v_fairseq/cpt_models/w2v_robust_1M.pt
wandb_project: finetune_w2v2_continued_pretraining
config_name: base
outdir_fold: ./model_outputs/continued_pretraining//
dataset: NCTE_Full
Running on fold: 
[2024-09-17 21:07:10,263][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'tqdm', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': 'w2v2-cpt-transfer_finetuning', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 3, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 5000000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 9500, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 5000000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 20000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': True, 'update_freq': [5], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'ckpts', 'restore_file': '/media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/15-16-26/ckpts/checkpoint_359_10000.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'wer', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec_ctc', 'w2v_path': '/home/ahmed/Research/Projects/finetune_w2v_fairseq/cpt_models/w2v_robust_1M.pt', 'no_pretrained_weights': False, 'dropout_input': 0.0, 'final_dropout': 0.0, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.1, 'apply_mask': True, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 64, 'mask_channel_prob': 0.5, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'freeze_finetune_updates': 10000, 'feature_grad_mult': 0.0, 'layerdrop': 0.1, 'drop_path': 0.0, 'mask_channel_min_space': 1, 'mask_channel_before': False, 'normalize': True, 'update_alibi': True, 'data': '/home/ahmed/Research/Projects/finetune_w2v_fairseq/manifest/NCTE_Full/', 'w2v_args': None, 'offload_activations': False, 'min_params_to_wrap': 100000000, 'checkpoint_activations': False, 'ddp_backend': 'no_c10d', 'zero_mask': False, 'load_ema': False, 'layer_decay': 1.0, 'layer_type': transformer, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all', 'freeze_regex': None, 'blank_weight': 0.0, 'blank_mode': 'add'}, 'task': {'_name': 'audio_finetuning', 'data': '/home/ahmed/Research/Projects/finetune_w2v_fairseq/manifest/NCTE_Full/', 'labels': 'ltr', 'multi_corpus_keys': None, 'multi_corpus_sampling_weights': None, 'binarized_dataset': False, 'sample_rate': 16000, 'normalize': True, 'enable_padding': False, 'max_sample_size': None, 'min_sample_size': 16000, 'num_batch_buckets': 0, 'tpu': False, 'text_compression_level': none, 'rebuild_batches': True, 'precompute_mask_config': None, 'post_save_script': None, 'subsample': 1.0, 'seed': 1, 'eval_wer': False, 'eval_wer_config': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_wer_tokenizer': None, 'eval_wer_post_process': 'letter', 'eval_bleu': False, 'eval_bleu_detok': None, 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_args': '{}', 'eval_bleu_print_samples': False, 'autoregressive': False, 'target_dictionary': None}, 'criterion': {'_name': 'ctc', 'zero_infinity': True, 'sentence_avg': True, 'post_process': 'letter', 'wer_kenlm_model': None, 'wer_lexicon': None, 'wer_lm_weight': 2.0, 'wer_word_score': -1.0, 'wer_sil_weight': 0.0, 'wer_args': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0001]}, 'lr_scheduler': {'_name': 'tri_stage', 'warmup_steps': 0, 'hold_steps': 0, 'decay_steps': 0, 'phase_ratio': [0.1, 0.4, 0.5], 'init_lr_scale': 0.01, 'final_lr_scale': 0.05, 'max_update': 20000.0, 'lr': [0.0001]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-09-17 21:07:10,272][fairseq.tasks.audio_finetuning][INFO] - Using dict_path : /home/ahmed/Research/Projects/finetune_w2v_fairseq/manifest/NCTE_Full/dict.ltr.txt
[2024-09-17 21:07:12,051][fairseq.models.wav2vec.wav2vec2_asr][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 3000, 'log_format': 'tqdm', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': 'w2v2_pretraining_trial_w2v_large_lv_fsh_swbd_cv', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 3, 'distributed_num_procs': 3, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:42263', 'distributed_port': 42263, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 3, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 3000000, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 1000, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3000000, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 1000000, 'stop_time_hours': 0.0, 'clip_norm': 0.5, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'ckpts', 'restore_file': '/scr-ssd/aadel4/pretrain_w2v_fairseq/outputs/2024-03-30/00-47-34/ckpts/checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 6}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.1, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 768, 'layer_norm_first': True, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': True, 'logit_temp': 0.1, 'quantize_targets': True, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 0.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 64, 'mask_channel_prob': 0.5, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.1, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'audio_pretraining', 'data': '/scr-ssd/aadel4/pretrain_w2v_fairseq/manifist', 'labels': None, 'multi_corpus_keys': None, 'multi_corpus_sampling_weights': None, 'binarized_dataset': False, 'sample_rate': 16000, 'normalize': True, 'enable_padding': False, 'max_sample_size': 320000, 'min_sample_size': 32000, 'num_batch_buckets': 0, 'tpu': False, 'text_compression_level': 'none', 'rebuild_batches': True, 'precompute_mask_config': None, 'post_save_script': None, 'subsample': 1.0, 'seed': 1}, 'criterion': None, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0001]}, 'lr_scheduler': None, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
/home/ahmed/miniconda3/envs/frsq/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.
  warnings.warn("torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.")
Wav2Vec2Model(
  (feature_extractor): ConvFeatureExtractionModel(
    (conv_layers): ModuleList(
      (0): Sequential(
        (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,))
        (1): Dropout(p=0.0, inplace=False)
        (2): Sequential(
          (0): TransposeLast()
          (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (2): TransposeLast()
        )
        (3): GELU(approximate='none')
      )
      (1-4): 4 x Sequential(
        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))
        (1): Dropout(p=0.0, inplace=False)
        (2): Sequential(
          (0): TransposeLast()
          (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (2): TransposeLast()
        )
        (3): GELU(approximate='none')
      )
      (5-6): 2 x Sequential(
        (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,))
        (1): Dropout(p=0.0, inplace=False)
        (2): Sequential(
          (0): TransposeLast()
          (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (2): TransposeLast()
        )
        (3): GELU(approximate='none')
      )
    )
  )
  (post_extract_proj): Linear(in_features=512, out_features=1024, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (quantizer): None
  (project_q): None
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-23): 24 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (final_proj): None
)
[2024-09-17 21:07:14,636][fairseq_cli.train][INFO] - Wav2VecCtc(
  (w2v_encoder): Wav2VecEncoder(
    (w2v_model): Wav2Vec2Model(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,))
            (1): Dropout(p=0.0, inplace=False)
            (2): Sequential(
              (0): TransposeLast()
              (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (2): TransposeLast()
            )
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))
            (1): Dropout(p=0.0, inplace=False)
            (2): Sequential(
              (0): TransposeLast()
              (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (2): TransposeLast()
            )
            (3): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,))
            (1): Dropout(p=0.0, inplace=False)
            (2): Sequential(
              (0): TransposeLast()
              (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (2): TransposeLast()
            )
            (3): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=1024, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.0, inplace=False)
      (quantizer): None
      (project_q): None
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-23): 24 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (final_dropout): Dropout(p=0.0, inplace=False)
    (proj): Linear(in_features=1024, out_features=47, bias=True)
  )
)
[2024-09-17 21:07:14,637][fairseq_cli.train][INFO] - task: AudioFinetuningTask
[2024-09-17 21:07:14,637][fairseq_cli.train][INFO] - model: Wav2VecCtc
[2024-09-17 21:07:14,637][fairseq_cli.train][INFO] - criterion: CtcCriterion
[2024-09-17 21:07:14,638][fairseq_cli.train][INFO] - num. shared model params: 315,486,895 (num. trained: 315,486,895)
[2024-09-17 21:07:14,639][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-09-17 21:07:14,652][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 427, skipped 0 samples
[2024-09-17 21:07:14,703][numexpr.utils][INFO] - Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-09-17 21:07:14,703][numexpr.utils][INFO] - NumExpr defaulting to 8 threads.
[2024-09-17 21:07:15,076][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2024-09-17 21:07:15,076][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.528 GB ; name = NVIDIA RTX A6000                        
[2024-09-17 21:07:15,076][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2024-09-17 21:07:15,077][fairseq_cli.train][INFO] - training on 1 devices (GPUs/TPUs)
[2024-09-17 21:07:15,077][fairseq_cli.train][INFO] - max tokens per device = 5000000 and max sentences per device = None
[2024-09-17 21:07:15,077][fairseq.trainer][INFO] - Preparing to load checkpoint /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/15-16-26/ckpts/checkpoint_359_10000.pt
[2024-09-17 21:07:17,349][fairseq.trainer][INFO] - Loaded checkpoint /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/15-16-26/ckpts/checkpoint_359_10000.pt (epoch 359 @ 10000 updates)
[2024-09-17 21:07:17,412][fairseq.trainer][INFO] - loading train data for epoch 359
[2024-09-17 21:07:17,413][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 2087, skipped 14 samples
[2024-09-17 21:07:17,416][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:07:17,416][fairseq.tasks.fairseq_task][INFO] - reuse_dataloader = True
[2024-09-17 21:07:17,416][fairseq.tasks.fairseq_task][INFO] - rebuild_batches = True
[2024-09-17 21:07:17,416][fairseq.tasks.fairseq_task][INFO] - batches will be rebuilt for each epoch
[2024-09-17 21:07:17,416][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 359
[2024-09-17 21:07:17,461][fairseq_cli.train][INFO] - begin dry-run validation on "valid" subset
[2024-09-17 21:07:17,462][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:07:17,462][fairseq.tasks.fairseq_task][INFO] - reuse_dataloader = True
[2024-09-17 21:07:17,462][fairseq.tasks.fairseq_task][INFO] - rebuild_batches = True
[2024-09-17 21:07:17,462][fairseq.tasks.fairseq_task][INFO] - batches will be rebuilt for each epoch
[2024-09-17 21:07:17,462][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 1
[2024-09-17 21:07:18,074][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
wandb: Currently logged in as: aadel4. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/wandb/run-20240917_210718-pq5g1l7j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ckpts
wandb: ‚≠êÔ∏è View project at https://wandb.ai/aadel4/w2v2-cpt-transfer_finetuning
wandb: üöÄ View run at https://wandb.ai/aadel4/w2v2-cpt-transfer_finetuning/runs/pq5g1l7j
[2024-09-17 21:07:24,500][fairseq.trainer][INFO] - begin training epoch 359
[2024-09-17 21:07:24,501][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 21:08:12,140][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2024-09-17 21:09:04,111][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 21:09:04,111][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:09:04,131][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 2
[2024-09-17 21:09:24,394][valid][INFO] - epoch 359 | valid on 'valid' subset | loss 970.414 | ntokens 1902.88 | nsentences 8.48 | nll_loss 4.325 | uer 100 | wer 100 | raw_wer 100 | wps 4718.5 | wpb 1902.9 | bsz 8.5 | num_updates 10028 | best_wer 100
[2024-09-17 21:09:24,395][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 359 @ 10028 updates
[2024-09-17 21:09:24,396][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 21:09:27,895][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 21:09:29,751][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 359 @ 10028 updates, score 100.0) (writing took 5.35630714799845 seconds)
[2024-09-17 21:09:29,752][fairseq_cli.train][INFO] - end of epoch 359 (average epoch stats below)
[2024-09-17 21:09:29,753][train][INFO] - epoch 359 | loss 1034.96 | ntokens 10486.3 | nsentences 46.2857 | nll_loss 4.568 | wps 505.9 | ups 0.05 | wpb 10486.3 | bsz 46.3 | num_updates 10028 | lr 9.91647e-05 | gnorm 1500.55 | loss_scale 1 | train_wall 99 | gb_free 16.7 | wall 0
[2024-09-17 21:09:29,754][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:09:29,773][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 360
[2024-09-17 21:09:29,786][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 21:09:29,793][fairseq.trainer][INFO] - begin training epoch 360
[2024-09-17 21:09:29,793][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 21:11:59,564][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 21:11:59,567][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:11:59,582][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 3
[2024-09-17 21:12:19,760][valid][INFO] - epoch 360 | valid on 'valid' subset | loss 967.429 | ntokens 1902.88 | nsentences 8.48 | nll_loss 4.311 | uer 99.999 | wer 100 | raw_wer 100 | wps 4719.6 | wpb 1902.9 | bsz 8.5 | num_updates 10072 | best_wer 100
[2024-09-17 21:12:19,760][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 360 @ 10072 updates
[2024-09-17 21:12:19,761][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 21:12:23,309][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 21:12:25,189][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 360 @ 10072 updates, score 100.0) (writing took 5.42803863899826 seconds)
[2024-09-17 21:12:25,189][fairseq_cli.train][INFO] - end of epoch 360 (average epoch stats below)
[2024-09-17 21:12:25,191][train][INFO] - epoch 360 | loss 961.743 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 4.395 | wps 2594.2 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 10072 | lr 9.78662e-05 | gnorm 761.556 | loss_scale 1 | train_wall 150 | gb_free 17 | wall 0
[2024-09-17 21:12:25,192][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:12:25,214][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 361
[2024-09-17 21:12:25,232][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 21:12:25,239][fairseq.trainer][INFO] - begin training epoch 361
[2024-09-17 21:12:25,239][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 21:14:54,036][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 21:14:54,049][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:14:54,065][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 4
[2024-09-17 21:15:14,269][valid][INFO] - epoch 361 | valid on 'valid' subset | loss 1021.18 | ntokens 1902.88 | nsentences 8.48 | nll_loss 4.551 | uer 99.992 | wer 100 | raw_wer 100 | wps 4701.7 | wpb 1902.9 | bsz 8.5 | num_updates 10116 | best_wer 100
[2024-09-17 21:15:14,269][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 361 @ 10116 updates
[2024-09-17 21:15:14,270][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 21:15:17,878][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 21:15:20,169][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 361 @ 10116 updates, score 100.0) (writing took 5.8992257049976615 seconds)
[2024-09-17 21:15:20,169][fairseq_cli.train][INFO] - end of epoch 361 (average epoch stats below)
[2024-09-17 21:15:20,170][train][INFO] - epoch 361 | loss 948.587 | ntokens 10343.6 | nsentences 47.2727 | nll_loss 4.335 | wps 2601 | ups 0.25 | wpb 10343.6 | bsz 47.3 | num_updates 10116 | lr 9.65846e-05 | gnorm 889.39 | loss_scale 1 | train_wall 149 | gb_free 16.7 | wall 0
[2024-09-17 21:15:20,171][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:15:20,194][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 362
[2024-09-17 21:15:20,207][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 21:15:20,215][fairseq.trainer][INFO] - begin training epoch 362
[2024-09-17 21:15:20,215][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 21:17:50,672][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 21:17:50,686][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:17:50,702][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 5
[2024-09-17 21:18:10,910][valid][INFO] - epoch 362 | valid on 'valid' subset | loss 969.402 | ntokens 1902.88 | nsentences 8.48 | nll_loss 4.32 | uer 99.936 | wer 100 | raw_wer 100 | wps 4700.3 | wpb 1902.9 | bsz 8.5 | num_updates 10160 | best_wer 100
[2024-09-17 21:18:10,910][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 362 @ 10160 updates
[2024-09-17 21:18:10,911][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 21:18:14,357][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 21:18:16,323][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 362 @ 10160 updates, score 100.0) (writing took 5.412477459998627 seconds)
[2024-09-17 21:18:16,323][fairseq_cli.train][INFO] - end of epoch 362 (average epoch stats below)
[2024-09-17 21:18:16,325][train][INFO] - epoch 362 | loss 934.07 | ntokens 10343.7 | nsentences 47.2727 | nll_loss 4.269 | wps 2583.7 | ups 0.25 | wpb 10343.7 | bsz 47.3 | num_updates 10160 | lr 9.53199e-05 | gnorm 634.892 | loss_scale 1 | train_wall 150 | gb_free 19.1 | wall 0
[2024-09-17 21:18:16,325][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:18:16,349][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 363
[2024-09-17 21:18:16,367][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 21:18:16,375][fairseq.trainer][INFO] - begin training epoch 363
[2024-09-17 21:18:16,375][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 21:20:32,669][train_inner][INFO] - epoch 363:     40 / 44 loss=960.758, ntokens=10372.2, nsentences=46.92, nll_loss=4.346, wps=1198.4, ups=0.12, wpb=10372.2, bsz=46.9, num_updates=10200, lr=9.41845e-05, gnorm=805.862, loss_scale=1, train_wall=684, gb_free=18.4, wall=0
[2024-09-17 21:20:45,557][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 21:20:45,557][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:20:45,573][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 6
[2024-09-17 21:21:05,886][valid][INFO] - epoch 363 | valid on 'valid' subset | loss 969.715 | ntokens 1902.88 | nsentences 8.48 | nll_loss 4.321 | uer 98.481 | wer 100 | raw_wer 100 | wps 4692.2 | wpb 1902.9 | bsz 8.5 | num_updates 10204 | best_wer 100
[2024-09-17 21:21:05,886][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 363 @ 10204 updates
[2024-09-17 21:21:05,887][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 21:21:09,348][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 21:21:11,476][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 363 @ 10204 updates, score 100.0) (writing took 5.589342142997339 seconds)
[2024-09-17 21:21:11,476][fairseq_cli.train][INFO] - end of epoch 363 (average epoch stats below)
[2024-09-17 21:21:11,477][train][INFO] - epoch 363 | loss 926.177 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 4.233 | wps 2598.3 | ups 0.25 | wpb 10343.2 | bsz 47.3 | num_updates 10204 | lr 9.40717e-05 | gnorm 448.875 | loss_scale 1 | train_wall 149 | gb_free 19.7 | wall 0
[2024-09-17 21:21:11,478][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:21:11,494][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 364
[2024-09-17 21:21:11,507][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 21:21:11,515][fairseq.trainer][INFO] - begin training epoch 364
[2024-09-17 21:21:11,515][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 21:23:40,666][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 21:23:40,679][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:23:40,694][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 7
[2024-09-17 21:24:00,947][valid][INFO] - epoch 364 | valid on 'valid' subset | loss 943.598 | ntokens 1902.88 | nsentences 8.48 | nll_loss 4.205 | uer 82.802 | wer 99.946 | raw_wer 99.946 | wps 4694.1 | wpb 1902.9 | bsz 8.5 | num_updates 10248 | best_wer 99.946
[2024-09-17 21:24:00,948][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 364 @ 10248 updates
[2024-09-17 21:24:00,949][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 21:24:04,427][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 21:24:06,433][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 364 @ 10248 updates, score 99.946) (writing took 5.484752553002181 seconds)
[2024-09-17 21:24:06,433][fairseq_cli.train][INFO] - end of epoch 364 (average epoch stats below)
[2024-09-17 21:24:06,434][train][INFO] - epoch 364 | loss 918.442 | ntokens 10343.6 | nsentences 47.2727 | nll_loss 4.198 | wps 2601.3 | ups 0.25 | wpb 10343.6 | bsz 47.3 | num_updates 10248 | lr 9.28399e-05 | gnorm 415.571 | loss_scale 1 | train_wall 149 | gb_free 17.2 | wall 0
[2024-09-17 21:24:06,435][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:24:06,452][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 365
[2024-09-17 21:24:06,464][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 21:24:06,472][fairseq.trainer][INFO] - begin training epoch 365
[2024-09-17 21:24:06,472][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 21:26:35,301][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 21:26:35,303][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:26:35,319][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 8
[2024-09-17 21:26:55,622][valid][INFO] - epoch 365 | valid on 'valid' subset | loss 925.494 | ntokens 1902.88 | nsentences 8.48 | nll_loss 4.124 | uer 76.438 | wer 100.054 | raw_wer 100.054 | wps 4676.1 | wpb 1902.9 | bsz 8.5 | num_updates 10292 | best_wer 99.946
[2024-09-17 21:26:55,623][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 365 @ 10292 updates
[2024-09-17 21:26:55,624][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-17 21:26:59,142][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-17 21:26:59,240][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 365 @ 10292 updates, score 100.054) (writing took 3.616855137002858 seconds)
[2024-09-17 21:26:59,241][fairseq_cli.train][INFO] - end of epoch 365 (average epoch stats below)
[2024-09-17 21:26:59,242][train][INFO] - epoch 365 | loss 913.334 | ntokens 10343.4 | nsentences 47.2727 | nll_loss 4.174 | wps 2633.6 | ups 0.25 | wpb 10343.4 | bsz 47.3 | num_updates 10292 | lr 9.16241e-05 | gnorm 673.853 | loss_scale 1 | train_wall 149 | gb_free 18.8 | wall 0
[2024-09-17 21:26:59,242][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:26:59,259][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 366
[2024-09-17 21:26:59,276][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 21:26:59,284][fairseq.trainer][INFO] - begin training epoch 366
[2024-09-17 21:26:59,285][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 21:29:28,690][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 21:29:28,691][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:29:28,706][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 9
[2024-09-17 21:29:48,962][valid][INFO] - epoch 366 | valid on 'valid' subset | loss 950.414 | ntokens 1902.88 | nsentences 8.48 | nll_loss 4.235 | uer 72.163 | wer 100.592 | raw_wer 100.592 | wps 4699.4 | wpb 1902.9 | bsz 8.5 | num_updates 10336 | best_wer 99.946
[2024-09-17 21:29:48,963][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 366 @ 10336 updates
[2024-09-17 21:29:48,964][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-17 21:29:52,551][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-17 21:29:52,638][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 366 @ 10336 updates, score 100.592) (writing took 3.674787695003033 seconds)
[2024-09-17 21:29:52,638][fairseq_cli.train][INFO] - end of epoch 366 (average epoch stats below)
[2024-09-17 21:29:52,639][train][INFO] - epoch 366 | loss 903.213 | ntokens 10343.3 | nsentences 47.2727 | nll_loss 4.128 | wps 2624.6 | ups 0.25 | wpb 10343.3 | bsz 47.3 | num_updates 10336 | lr 9.04243e-05 | gnorm 687.758 | loss_scale 1 | train_wall 149 | gb_free 24.2 | wall 0
[2024-09-17 21:29:52,640][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:29:52,656][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 367
[2024-09-17 21:29:52,668][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 21:29:52,677][fairseq.trainer][INFO] - begin training epoch 367
[2024-09-17 21:29:52,678][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 21:32:23,152][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 21:32:23,152][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:32:23,168][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 10
[2024-09-17 21:32:43,420][valid][INFO] - epoch 367 | valid on 'valid' subset | loss 908.153 | ntokens 1902.88 | nsentences 8.48 | nll_loss 4.047 | uer 77.147 | wer 99.961 | raw_wer 99.961 | wps 4683.8 | wpb 1902.9 | bsz 8.5 | num_updates 10380 | best_wer 99.946
[2024-09-17 21:32:43,420][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 367 @ 10380 updates
[2024-09-17 21:32:43,421][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-17 21:32:46,947][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-17 21:32:47,039][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 367 @ 10380 updates, score 99.961) (writing took 3.61848609700246 seconds)
[2024-09-17 21:32:47,039][fairseq_cli.train][INFO] - end of epoch 367 (average epoch stats below)
[2024-09-17 21:32:47,041][train][INFO] - epoch 367 | loss 894.24 | ntokens 10344 | nsentences 47.2727 | nll_loss 4.087 | wps 2609.7 | ups 0.25 | wpb 10344 | bsz 47.3 | num_updates 10380 | lr 8.92403e-05 | gnorm 656.836 | loss_scale 1 | train_wall 150 | gb_free 16.9 | wall 0
[2024-09-17 21:32:47,041][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:32:47,058][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 368
[2024-09-17 21:32:47,070][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 21:32:47,080][fairseq.trainer][INFO] - begin training epoch 368
[2024-09-17 21:32:47,080][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 21:33:54,716][train_inner][INFO] - epoch 368:     20 / 44 loss=891.954, ntokens=10331.1, nsentences=47.92, nll_loss=4.137, wps=2576.2, ups=0.25, wpb=10331.1, bsz=47.9, num_updates=10400, lr=8.87072e-05, gnorm=584.777, loss_scale=1, train_wall=678, gb_free=14.9, wall=0
[2024-09-17 21:35:16,821][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 21:35:16,822][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:35:16,838][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 11
[2024-09-17 21:35:37,091][valid][INFO] - epoch 368 | valid on 'valid' subset | loss 911.039 | ntokens 1902.88 | nsentences 8.48 | nll_loss 4.06 | uer 71.234 | wer 99.218 | raw_wer 99.218 | wps 4681.1 | wpb 1902.9 | bsz 8.5 | num_updates 10424 | best_wer 99.218
[2024-09-17 21:35:37,103][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 368 @ 10424 updates
[2024-09-17 21:35:37,104][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 21:35:40,620][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 21:35:42,607][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 368 @ 10424 updates, score 99.218) (writing took 5.503624409997428 seconds)
[2024-09-17 21:35:42,607][fairseq_cli.train][INFO] - end of epoch 368 (average epoch stats below)
[2024-09-17 21:35:42,608][train][INFO] - epoch 368 | loss 880.351 | ntokens 10343 | nsentences 47.2727 | nll_loss 4.024 | wps 2592.1 | ups 0.25 | wpb 10343 | bsz 47.3 | num_updates 10424 | lr 8.80717e-05 | gnorm 473.984 | loss_scale 1 | train_wall 150 | gb_free 15.3 | wall 0
[2024-09-17 21:35:42,609][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:35:42,626][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 369
[2024-09-17 21:35:42,639][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 21:35:42,648][fairseq.trainer][INFO] - begin training epoch 369
[2024-09-17 21:35:42,649][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 21:38:12,427][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 21:38:12,440][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:38:12,457][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 12
[2024-09-17 21:38:32,582][valid][INFO] - epoch 369 | valid on 'valid' subset | loss 896.418 | ntokens 1902.88 | nsentences 8.48 | nll_loss 3.995 | uer 75.111 | wer 98.303 | raw_wer 98.303 | wps 4709.4 | wpb 1902.9 | bsz 8.5 | num_updates 10468 | best_wer 98.303
[2024-09-17 21:38:32,583][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 369 @ 10468 updates
[2024-09-17 21:38:32,583][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 21:38:36,119][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 21:38:38,132][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 369 @ 10468 updates, score 98.303) (writing took 5.549165914999321 seconds)
[2024-09-17 21:38:38,132][fairseq_cli.train][INFO] - end of epoch 369 (average epoch stats below)
[2024-09-17 21:38:38,134][train][INFO] - epoch 369 | loss 869.138 | ntokens 10343.6 | nsentences 47.2727 | nll_loss 3.972 | wps 2592.9 | ups 0.25 | wpb 10343.6 | bsz 47.3 | num_updates 10468 | lr 8.69184e-05 | gnorm 345.143 | loss_scale 1 | train_wall 150 | gb_free 16.5 | wall 0
[2024-09-17 21:38:38,134][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:38:38,156][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 370
[2024-09-17 21:38:38,171][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 21:38:38,181][fairseq.trainer][INFO] - begin training epoch 370
[2024-09-17 21:38:38,181][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 21:41:08,031][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 21:41:08,033][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:41:08,050][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 13
[2024-09-17 21:41:28,285][valid][INFO] - epoch 370 | valid on 'valid' subset | loss 871.176 | ntokens 1902.88 | nsentences 8.48 | nll_loss 3.882 | uer 75.095 | wer 98.773 | raw_wer 98.773 | wps 4705.6 | wpb 1902.9 | bsz 8.5 | num_updates 10512 | best_wer 98.303
[2024-09-17 21:41:28,286][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 370 @ 10512 updates
[2024-09-17 21:41:28,287][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-17 21:41:31,762][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-17 21:41:31,856][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 370 @ 10512 updates, score 98.773) (writing took 3.5693894859978172 seconds)
[2024-09-17 21:41:31,856][fairseq_cli.train][INFO] - end of epoch 370 (average epoch stats below)
[2024-09-17 21:41:31,857][train][INFO] - epoch 370 | loss 856.268 | ntokens 10343.7 | nsentences 47.2727 | nll_loss 3.913 | wps 2619.8 | ups 0.25 | wpb 10343.7 | bsz 47.3 | num_updates 10512 | lr 8.57802e-05 | gnorm 362.861 | loss_scale 1 | train_wall 150 | gb_free 20 | wall 0
[2024-09-17 21:41:31,858][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:41:31,876][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 371
[2024-09-17 21:41:31,888][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 21:41:31,898][fairseq.trainer][INFO] - begin training epoch 371
[2024-09-17 21:41:31,899][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 21:44:00,498][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 21:44:00,510][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:44:00,525][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 14
[2024-09-17 21:44:20,996][valid][INFO] - epoch 371 | valid on 'valid' subset | loss 858.289 | ntokens 1902.88 | nsentences 8.48 | nll_loss 3.825 | uer 69.887 | wer 98.88 | raw_wer 98.88 | wps 4642.1 | wpb 1902.9 | bsz 8.5 | num_updates 10556 | best_wer 98.303
[2024-09-17 21:44:20,997][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 371 @ 10556 updates
[2024-09-17 21:44:20,998][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-17 21:44:24,497][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-17 21:44:24,576][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 371 @ 10556 updates, score 98.88) (writing took 3.578748052001174 seconds)
[2024-09-17 21:44:24,576][fairseq_cli.train][INFO] - end of epoch 371 (average epoch stats below)
[2024-09-17 21:44:24,577][train][INFO] - epoch 371 | loss 843.505 | ntokens 10343 | nsentences 47.2727 | nll_loss 3.855 | wps 2634.9 | ups 0.25 | wpb 10343 | bsz 47.3 | num_updates 10556 | lr 8.4657e-05 | gnorm 455.804 | loss_scale 1 | train_wall 148 | gb_free 19.1 | wall 0
[2024-09-17 21:44:24,578][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:44:24,597][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 372
[2024-09-17 21:44:24,615][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 21:44:24,624][fairseq.trainer][INFO] - begin training epoch 372
[2024-09-17 21:44:24,624][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 21:46:54,081][train_inner][INFO] - epoch 372:     44 / 44 loss=859.302, ntokens=10347.1, nsentences=46.84, nll_loss=3.89, wps=2655.3, ups=0.26, wpb=10347.1, bsz=46.8, num_updates=10600, lr=8.35484e-05, gnorm=444.516, loss_scale=1, train_wall=679, gb_free=14.9, wall=0
[2024-09-17 21:46:54,082][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 21:46:54,082][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:46:54,098][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 15
[2024-09-17 21:47:14,424][valid][INFO] - epoch 372 | valid on 'valid' subset | loss 831.076 | ntokens 1902.88 | nsentences 8.48 | nll_loss 3.704 | uer 65.159 | wer 98.783 | raw_wer 98.783 | wps 4679.3 | wpb 1902.9 | bsz 8.5 | num_updates 10600 | best_wer 98.303
[2024-09-17 21:47:14,425][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 372 @ 10600 updates
[2024-09-17 21:47:14,425][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-17 21:47:18,064][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-17 21:47:18,148][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 372 @ 10600 updates, score 98.783) (writing took 3.7229082369995012 seconds)
[2024-09-17 21:47:18,148][fairseq_cli.train][INFO] - end of epoch 372 (average epoch stats below)
[2024-09-17 21:47:18,149][train][INFO] - epoch 372 | loss 820.587 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 3.75 | wps 2622.1 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 10600 | lr 8.35484e-05 | gnorm 580.194 | loss_scale 1 | train_wall 149 | gb_free 14.9 | wall 0
[2024-09-17 21:47:18,150][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:47:18,167][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 373
[2024-09-17 21:47:18,179][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 21:47:18,189][fairseq.trainer][INFO] - begin training epoch 373
[2024-09-17 21:47:18,189][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 21:49:48,394][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 21:49:48,395][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:49:48,410][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 16
[2024-09-17 21:50:08,687][valid][INFO] - epoch 373 | valid on 'valid' subset | loss 792.693 | ntokens 1902.88 | nsentences 8.48 | nll_loss 3.533 | uer 58.389 | wer 97.208 | raw_wer 97.208 | wps 4696.6 | wpb 1902.9 | bsz 8.5 | num_updates 10644 | best_wer 97.208
[2024-09-17 21:50:08,687][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 373 @ 10644 updates
[2024-09-17 21:50:08,688][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 21:50:12,308][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 21:50:14,349][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 373 @ 10644 updates, score 97.208) (writing took 5.6614418659992225 seconds)
[2024-09-17 21:50:14,350][fairseq_cli.train][INFO] - end of epoch 373 (average epoch stats below)
[2024-09-17 21:50:14,351][train][INFO] - epoch 373 | loss 778.885 | ntokens 10343 | nsentences 47.2727 | nll_loss 3.56 | wps 2582.8 | ups 0.25 | wpb 10343 | bsz 47.3 | num_updates 10644 | lr 8.24544e-05 | gnorm 351.777 | loss_scale 1 | train_wall 150 | gb_free 21 | wall 0
[2024-09-17 21:50:14,352][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:50:14,369][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 374
[2024-09-17 21:50:14,382][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 21:50:14,392][fairseq.trainer][INFO] - begin training epoch 374
[2024-09-17 21:50:14,392][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 21:52:44,684][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 21:52:44,696][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:52:44,711][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 17
[2024-09-17 21:53:04,942][valid][INFO] - epoch 374 | valid on 'valid' subset | loss 706.753 | ntokens 1902.88 | nsentences 8.48 | nll_loss 3.15 | uer 54.157 | wer 91.747 | raw_wer 91.747 | wps 4704 | wpb 1902.9 | bsz 8.5 | num_updates 10688 | best_wer 91.747
[2024-09-17 21:53:04,943][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 374 @ 10688 updates
[2024-09-17 21:53:04,944][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 21:53:08,407][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 21:53:10,441][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 374 @ 10688 updates, score 91.747) (writing took 5.4977088989980984 seconds)
[2024-09-17 21:53:10,441][fairseq_cli.train][INFO] - end of epoch 374 (average epoch stats below)
[2024-09-17 21:53:10,442][train][INFO] - epoch 374 | loss 726.392 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 3.32 | wps 2584.6 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 10688 | lr 8.13747e-05 | gnorm 375.584 | loss_scale 1 | train_wall 150 | gb_free 21.3 | wall 0
[2024-09-17 21:53:10,443][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:53:10,459][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 375
[2024-09-17 21:53:10,471][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 21:53:10,483][fairseq.trainer][INFO] - begin training epoch 375
[2024-09-17 21:53:10,484][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 21:55:40,138][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 21:55:40,151][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:55:40,166][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 18
[2024-09-17 21:56:00,422][valid][INFO] - epoch 375 | valid on 'valid' subset | loss 651.153 | ntokens 1902.88 | nsentences 8.48 | nll_loss 2.902 | uer 48.041 | wer 82.916 | raw_wer 82.916 | wps 4688.4 | wpb 1902.9 | bsz 8.5 | num_updates 10732 | best_wer 82.916
[2024-09-17 21:56:00,422][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 375 @ 10732 updates
[2024-09-17 21:56:00,423][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 21:56:03,929][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 21:56:05,857][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 375 @ 10732 updates, score 82.916) (writing took 5.434385566000856 seconds)
[2024-09-17 21:56:05,857][fairseq_cli.train][INFO] - end of epoch 375 (average epoch stats below)
[2024-09-17 21:56:05,859][train][INFO] - epoch 375 | loss 681.755 | ntokens 10343.4 | nsentences 47.2727 | nll_loss 3.116 | wps 2594.5 | ups 0.25 | wpb 10343.4 | bsz 47.3 | num_updates 10732 | lr 8.03091e-05 | gnorm 343.942 | loss_scale 1 | train_wall 149 | gb_free 18.6 | wall 0
[2024-09-17 21:56:05,860][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:56:05,876][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 376
[2024-09-17 21:56:05,889][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 21:56:05,901][fairseq.trainer][INFO] - begin training epoch 376
[2024-09-17 21:56:05,901][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 21:58:35,735][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 21:58:35,748][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:58:35,764][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 19
[2024-09-17 21:58:56,063][valid][INFO] - epoch 376 | valid on 'valid' subset | loss 603.517 | ntokens 1902.88 | nsentences 8.48 | nll_loss 2.69 | uer 42.062 | wer 74.927 | raw_wer 74.927 | wps 4679.6 | wpb 1902.9 | bsz 8.5 | num_updates 10776 | best_wer 74.927
[2024-09-17 21:58:56,063][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 376 @ 10776 updates
[2024-09-17 21:58:56,064][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 21:58:59,561][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 21:59:01,566][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 376 @ 10776 updates, score 74.927) (writing took 5.502596599999379 seconds)
[2024-09-17 21:59:01,567][fairseq_cli.train][INFO] - end of epoch 376 (average epoch stats below)
[2024-09-17 21:59:01,568][train][INFO] - epoch 376 | loss 635.645 | ntokens 10344 | nsentences 47.2727 | nll_loss 2.905 | wps 2590.3 | ups 0.25 | wpb 10344 | bsz 47.3 | num_updates 10776 | lr 7.92574e-05 | gnorm 418.424 | loss_scale 1 | train_wall 150 | gb_free 17.6 | wall 0
[2024-09-17 21:59:01,569][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 21:59:01,585][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 377
[2024-09-17 21:59:01,597][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 21:59:01,608][fairseq.trainer][INFO] - begin training epoch 377
[2024-09-17 21:59:01,608][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 22:00:23,045][train_inner][INFO] - epoch 377:     24 / 44 loss=688.11, ntokens=10346.9, nsentences=47.6, nll_loss=3.166, wps=2558.1, ups=0.25, wpb=10346.9, bsz=47.6, num_updates=10800, lr=7.86896e-05, gnorm=365.497, loss_scale=1, train_wall=681, gb_free=18.7, wall=0
[2024-09-17 22:01:30,864][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 22:01:30,865][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:01:30,880][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 20
[2024-09-17 22:01:51,261][valid][INFO] - epoch 377 | valid on 'valid' subset | loss 548.335 | ntokens 1902.88 | nsentences 8.48 | nll_loss 2.444 | uer 38.914 | wer 69.02 | raw_wer 69.02 | wps 4661.7 | wpb 1902.9 | bsz 8.5 | num_updates 10820 | best_wer 69.02
[2024-09-17 22:01:51,262][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 377 @ 10820 updates
[2024-09-17 22:01:51,262][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:01:54,644][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:01:56,681][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 377 @ 10820 updates, score 69.02) (writing took 5.419367688999046 seconds)
[2024-09-17 22:01:56,681][fairseq_cli.train][INFO] - end of epoch 377 (average epoch stats below)
[2024-09-17 22:01:56,682][train][INFO] - epoch 377 | loss 593.893 | ntokens 10343.4 | nsentences 47.2727 | nll_loss 2.714 | wps 2598.9 | ups 0.25 | wpb 10343.4 | bsz 47.3 | num_updates 10820 | lr 7.82196e-05 | gnorm 335.982 | loss_scale 1 | train_wall 149 | gb_free 18.9 | wall 0
[2024-09-17 22:01:56,683][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:01:56,699][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 378
[2024-09-17 22:01:56,711][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 22:01:56,722][fairseq.trainer][INFO] - begin training epoch 378
[2024-09-17 22:01:56,722][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 22:04:26,981][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 22:04:26,982][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:04:26,997][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 21
[2024-09-17 22:04:47,257][valid][INFO] - epoch 378 | valid on 'valid' subset | loss 508.917 | ntokens 1902.88 | nsentences 8.48 | nll_loss 2.268 | uer 36.396 | wer 62.522 | raw_wer 62.522 | wps 4689.9 | wpb 1902.9 | bsz 8.5 | num_updates 10864 | best_wer 62.522
[2024-09-17 22:04:47,257][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 378 @ 10864 updates
[2024-09-17 22:04:47,258][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:04:50,605][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:04:52,659][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 378 @ 10864 updates, score 62.522) (writing took 5.402186890998564 seconds)
[2024-09-17 22:04:52,660][fairseq_cli.train][INFO] - end of epoch 378 (average epoch stats below)
[2024-09-17 22:04:52,661][train][INFO] - epoch 378 | loss 550.509 | ntokens 10344 | nsentences 47.2727 | nll_loss 2.516 | wps 2586.3 | ups 0.25 | wpb 10344 | bsz 47.3 | num_updates 10864 | lr 7.71953e-05 | gnorm 436.478 | loss_scale 1 | train_wall 150 | gb_free 21.3 | wall 0
[2024-09-17 22:04:52,662][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:04:52,687][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 379
[2024-09-17 22:04:52,705][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 22:04:52,715][fairseq.trainer][INFO] - begin training epoch 379
[2024-09-17 22:04:52,715][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 22:07:22,500][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 22:07:22,500][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:07:22,516][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 22
[2024-09-17 22:07:42,816][valid][INFO] - epoch 379 | valid on 'valid' subset | loss 474.814 | ntokens 1902.88 | nsentences 8.48 | nll_loss 2.116 | uer 34.701 | wer 58.033 | raw_wer 58.033 | wps 4676.8 | wpb 1902.9 | bsz 8.5 | num_updates 10908 | best_wer 58.033
[2024-09-17 22:07:42,817][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 379 @ 10908 updates
[2024-09-17 22:07:42,818][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:07:46,145][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:07:48,577][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 379 @ 10908 updates, score 58.033) (writing took 5.760314577000827 seconds)
[2024-09-17 22:07:48,578][fairseq_cli.train][INFO] - end of epoch 379 (average epoch stats below)
[2024-09-17 22:07:48,579][train][INFO] - epoch 379 | loss 525.361 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 2.401 | wps 2587 | ups 0.25 | wpb 10343.2 | bsz 47.3 | num_updates 10908 | lr 7.61845e-05 | gnorm 424.623 | loss_scale 1 | train_wall 150 | gb_free 20.2 | wall 0
[2024-09-17 22:07:48,581][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:07:48,599][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 380
[2024-09-17 22:07:48,617][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 22:07:48,626][fairseq.trainer][INFO] - begin training epoch 380
[2024-09-17 22:07:48,627][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 22:10:17,863][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 22:10:17,864][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:10:17,879][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 23
[2024-09-17 22:10:38,131][valid][INFO] - epoch 380 | valid on 'valid' subset | loss 454.667 | ntokens 1902.88 | nsentences 8.48 | nll_loss 2.026 | uer 34.012 | wer 56.161 | raw_wer 56.161 | wps 4698.5 | wpb 1902.9 | bsz 8.5 | num_updates 10952 | best_wer 56.161
[2024-09-17 22:10:38,132][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 380 @ 10952 updates
[2024-09-17 22:10:38,132][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:10:41,569][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:10:43,612][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 380 @ 10952 updates, score 56.161) (writing took 5.479942800000572 seconds)
[2024-09-17 22:10:43,612][fairseq_cli.train][INFO] - end of epoch 380 (average epoch stats below)
[2024-09-17 22:10:43,613][train][INFO] - epoch 380 | loss 505.311 | ntokens 10343.7 | nsentences 47.2727 | nll_loss 2.309 | wps 2600.2 | ups 0.25 | wpb 10343.7 | bsz 47.3 | num_updates 10952 | lr 7.51869e-05 | gnorm 404.853 | loss_scale 1 | train_wall 149 | gb_free 19 | wall 0
[2024-09-17 22:10:43,614][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:10:43,630][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 381
[2024-09-17 22:10:43,642][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 22:10:43,653][fairseq.trainer][INFO] - begin training epoch 381
[2024-09-17 22:10:43,653][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 22:13:13,636][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 22:13:13,637][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:13:13,652][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 24
[2024-09-17 22:13:33,977][valid][INFO] - epoch 381 | valid on 'valid' subset | loss 452.162 | ntokens 1902.88 | nsentences 8.48 | nll_loss 2.015 | uer 33.041 | wer 52.899 | raw_wer 52.899 | wps 4666.3 | wpb 1902.9 | bsz 8.5 | num_updates 10996 | best_wer 52.899
[2024-09-17 22:13:33,977][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 381 @ 10996 updates
[2024-09-17 22:13:33,978][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:13:37,416][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:13:39,528][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 381 @ 10996 updates, score 52.899) (writing took 5.550737030000164 seconds)
[2024-09-17 22:13:39,529][fairseq_cli.train][INFO] - end of epoch 381 (average epoch stats below)
[2024-09-17 22:13:39,530][train][INFO] - epoch 381 | loss 487.965 | ntokens 10343.4 | nsentences 47.2727 | nll_loss 2.23 | wps 2587.1 | ups 0.25 | wpb 10343.4 | bsz 47.3 | num_updates 10996 | lr 7.42023e-05 | gnorm 344.599 | loss_scale 1 | train_wall 150 | gb_free 20.5 | wall 0
[2024-09-17 22:13:39,531][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:13:39,546][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 382
[2024-09-17 22:13:39,564][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 22:13:39,574][fairseq.trainer][INFO] - begin training epoch 382
[2024-09-17 22:13:39,574][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 22:13:52,753][train_inner][INFO] - epoch 382:      4 / 44 loss=523.979, ntokens=10348.4, nsentences=47.28, nll_loss=2.394, wps=2556.1, ups=0.25, wpb=10348.4, bsz=47.3, num_updates=11000, lr=7.41134e-05, gnorm=397.405, loss_scale=1, train_wall=680, gb_free=21.7, wall=0
[2024-09-17 22:16:09,556][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 22:16:09,556][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:16:09,572][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 25
[2024-09-17 22:16:29,977][valid][INFO] - epoch 382 | valid on 'valid' subset | loss 424.434 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.891 | uer 32.039 | wer 51.071 | raw_wer 51.071 | wps 4639.4 | wpb 1902.9 | bsz 8.5 | num_updates 11040 | best_wer 51.071
[2024-09-17 22:16:29,978][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 382 @ 11040 updates
[2024-09-17 22:16:29,978][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:16:33,371][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:16:35,436][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 382 @ 11040 updates, score 51.071) (writing took 5.458257589998539 seconds)
[2024-09-17 22:16:35,436][fairseq_cli.train][INFO] - end of epoch 382 (average epoch stats below)
[2024-09-17 22:16:35,437][train][INFO] - epoch 382 | loss 465.106 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 2.126 | wps 2587.2 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 11040 | lr 7.32306e-05 | gnorm 331.806 | loss_scale 1 | train_wall 150 | gb_free 18.2 | wall 0
[2024-09-17 22:16:35,438][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:16:35,454][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 383
[2024-09-17 22:16:35,467][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 22:16:35,477][fairseq.trainer][INFO] - begin training epoch 383
[2024-09-17 22:16:35,478][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 22:19:05,573][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 22:19:05,574][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:19:05,590][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 26
[2024-09-17 22:19:25,945][valid][INFO] - epoch 383 | valid on 'valid' subset | loss 429.774 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.915 | uer 31.163 | wer 48.709 | raw_wer 48.709 | wps 4661.9 | wpb 1902.9 | bsz 8.5 | num_updates 11084 | best_wer 48.709
[2024-09-17 22:19:25,946][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 383 @ 11084 updates
[2024-09-17 22:19:25,946][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:19:29,292][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:19:31,391][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 383 @ 11084 updates, score 48.709) (writing took 5.445507314998395 seconds)
[2024-09-17 22:19:31,392][fairseq_cli.train][INFO] - end of epoch 383 (average epoch stats below)
[2024-09-17 22:19:31,393][train][INFO] - epoch 383 | loss 453.192 | ntokens 10343.7 | nsentences 47.2727 | nll_loss 2.071 | wps 2586.6 | ups 0.25 | wpb 10343.7 | bsz 47.3 | num_updates 11084 | lr 7.22717e-05 | gnorm 386.941 | loss_scale 1 | train_wall 150 | gb_free 16.4 | wall 0
[2024-09-17 22:19:31,394][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:19:31,410][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 384
[2024-09-17 22:19:31,422][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 22:19:31,432][fairseq.trainer][INFO] - begin training epoch 384
[2024-09-17 22:19:31,432][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 22:22:00,654][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 22:22:00,655][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:22:00,670][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 27
[2024-09-17 22:22:20,908][valid][INFO] - epoch 384 | valid on 'valid' subset | loss 400.595 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.785 | uer 30.67 | wer 47.819 | raw_wer 47.819 | wps 4693.2 | wpb 1902.9 | bsz 8.5 | num_updates 11128 | best_wer 47.819
[2024-09-17 22:22:20,909][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 384 @ 11128 updates
[2024-09-17 22:22:20,909][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:22:24,276][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:22:26,382][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 384 @ 11128 updates, score 47.819) (writing took 5.472940400999505 seconds)
[2024-09-17 22:22:26,382][fairseq_cli.train][INFO] - end of epoch 384 (average epoch stats below)
[2024-09-17 22:22:26,383][train][INFO] - epoch 384 | loss 452.075 | ntokens 10344 | nsentences 47.2727 | nll_loss 2.066 | wps 2600.9 | ups 0.25 | wpb 10344 | bsz 47.3 | num_updates 11128 | lr 7.13253e-05 | gnorm 358.894 | loss_scale 1 | train_wall 149 | gb_free 18.8 | wall 0
[2024-09-17 22:22:26,384][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:22:26,400][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 385
[2024-09-17 22:22:26,412][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 22:22:26,423][fairseq.trainer][INFO] - begin training epoch 385
[2024-09-17 22:22:26,423][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 22:24:57,238][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 22:24:57,239][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:24:57,254][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 28
[2024-09-17 22:25:17,500][valid][INFO] - epoch 385 | valid on 'valid' subset | loss 384.785 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.715 | uer 29.764 | wer 46.47 | raw_wer 46.47 | wps 4688.2 | wpb 1902.9 | bsz 8.5 | num_updates 11172 | best_wer 46.47
[2024-09-17 22:25:17,501][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 385 @ 11172 updates
[2024-09-17 22:25:17,502][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:25:20,895][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:25:23,005][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 385 @ 11172 updates, score 46.47) (writing took 5.504287395000574 seconds)
[2024-09-17 22:25:23,006][fairseq_cli.train][INFO] - end of epoch 385 (average epoch stats below)
[2024-09-17 22:25:23,007][train][INFO] - epoch 385 | loss 438.87 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 2.006 | wps 2576.8 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 11172 | lr 7.03913e-05 | gnorm 375.108 | loss_scale 1 | train_wall 151 | gb_free 19.8 | wall 0
[2024-09-17 22:25:23,008][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:25:23,023][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 386
[2024-09-17 22:25:23,037][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 22:25:23,048][fairseq.trainer][INFO] - begin training epoch 386
[2024-09-17 22:25:23,048][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 22:26:59,286][train_inner][INFO] - epoch 386:     28 / 44 loss=450.673, ntokens=10340.5, nsentences=47, nll_loss=2.048, wps=2629.4, ups=0.25, wpb=10340.5, bsz=47, num_updates=11200, lr=6.98034e-05, gnorm=362.169, loss_scale=1, train_wall=682, gb_free=18.3, wall=0
[2024-09-17 22:27:52,853][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 22:27:52,854][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:27:52,870][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 29
[2024-09-17 22:28:13,201][valid][INFO] - epoch 386 | valid on 'valid' subset | loss 382.83 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.706 | uer 29.279 | wer 44.9 | raw_wer 44.9 | wps 4685.9 | wpb 1902.9 | bsz 8.5 | num_updates 11216 | best_wer 44.9
[2024-09-17 22:28:13,202][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 386 @ 11216 updates
[2024-09-17 22:28:13,202][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:28:16,632][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:28:18,760][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 386 @ 11216 updates, score 44.9) (writing took 5.558419208999112 seconds)
[2024-09-17 22:28:18,761][fairseq_cli.train][INFO] - end of epoch 386 (average epoch stats below)
[2024-09-17 22:28:18,762][train][INFO] - epoch 386 | loss 425.916 | ntokens 10343.4 | nsentences 47.2727 | nll_loss 1.947 | wps 2589.5 | ups 0.25 | wpb 10343.4 | bsz 47.3 | num_updates 11216 | lr 6.94696e-05 | gnorm 358.832 | loss_scale 1 | train_wall 150 | gb_free 22.6 | wall 0
[2024-09-17 22:28:18,763][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:28:18,778][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 387
[2024-09-17 22:28:18,791][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 22:28:18,801][fairseq.trainer][INFO] - begin training epoch 387
[2024-09-17 22:28:18,802][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 22:30:48,826][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 22:30:48,827][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:30:48,842][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 30
[2024-09-17 22:31:09,286][valid][INFO] - epoch 387 | valid on 'valid' subset | loss 382.974 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.707 | uer 29.14 | wer 44.049 | raw_wer 44.049 | wps 4658.2 | wpb 1902.9 | bsz 8.5 | num_updates 11260 | best_wer 44.049
[2024-09-17 22:31:09,287][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 387 @ 11260 updates
[2024-09-17 22:31:09,288][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:31:12,661][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:31:14,774][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 387 @ 11260 updates, score 44.049) (writing took 5.486658967998665 seconds)
[2024-09-17 22:31:14,774][fairseq_cli.train][INFO] - end of epoch 387 (average epoch stats below)
[2024-09-17 22:31:14,775][train][INFO] - epoch 387 | loss 420.425 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.921 | wps 2585.7 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 11260 | lr 6.85599e-05 | gnorm 348.818 | loss_scale 1 | train_wall 150 | gb_free 17.2 | wall 0
[2024-09-17 22:31:14,776][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:31:14,791][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 388
[2024-09-17 22:31:14,803][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 22:31:14,815][fairseq.trainer][INFO] - begin training epoch 388
[2024-09-17 22:31:14,815][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 22:33:44,913][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 22:33:44,914][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:33:44,929][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 31
[2024-09-17 22:34:05,216][valid][INFO] - epoch 388 | valid on 'valid' subset | loss 365.565 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.629 | uer 28.117 | wer 42.964 | raw_wer 42.964 | wps 4678.9 | wpb 1902.9 | bsz 8.5 | num_updates 11304 | best_wer 42.964
[2024-09-17 22:34:05,216][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 388 @ 11304 updates
[2024-09-17 22:34:05,217][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:34:08,505][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:34:10,610][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 388 @ 11304 updates, score 42.964) (writing took 5.393603881999297 seconds)
[2024-09-17 22:34:10,610][fairseq_cli.train][INFO] - end of epoch 388 (average epoch stats below)
[2024-09-17 22:34:10,611][train][INFO] - epoch 388 | loss 406.994 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.86 | wps 2588.3 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 11304 | lr 6.76621e-05 | gnorm 296.023 | loss_scale 1 | train_wall 150 | gb_free 18.4 | wall 0
[2024-09-17 22:34:10,612][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:34:10,628][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 389
[2024-09-17 22:34:10,640][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 22:34:10,652][fairseq.trainer][INFO] - begin training epoch 389
[2024-09-17 22:34:10,652][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 22:36:40,182][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 22:36:40,183][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:36:40,208][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 32
[2024-09-17 22:37:00,465][valid][INFO] - epoch 389 | valid on 'valid' subset | loss 357.308 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.592 | uer 27.977 | wer 42.876 | raw_wer 42.876 | wps 4689.2 | wpb 1902.9 | bsz 8.5 | num_updates 11348 | best_wer 42.876
[2024-09-17 22:37:00,465][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 389 @ 11348 updates
[2024-09-17 22:37:00,466][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:37:03,834][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:37:05,857][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 389 @ 11348 updates, score 42.876) (writing took 5.391756765002356 seconds)
[2024-09-17 22:37:05,858][fairseq_cli.train][INFO] - end of epoch 389 (average epoch stats below)
[2024-09-17 22:37:05,859][train][INFO] - epoch 389 | loss 407.732 | ntokens 10343.3 | nsentences 47.2727 | nll_loss 1.863 | wps 2596.9 | ups 0.25 | wpb 10343.3 | bsz 47.3 | num_updates 11348 | lr 6.67761e-05 | gnorm 331.755 | loss_scale 1 | train_wall 149 | gb_free 18.1 | wall 0
[2024-09-17 22:37:05,859][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:37:05,875][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 390
[2024-09-17 22:37:05,888][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 22:37:05,899][fairseq.trainer][INFO] - begin training epoch 390
[2024-09-17 22:37:05,900][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 22:39:36,482][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 22:39:36,483][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:39:36,499][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 33
[2024-09-17 22:39:56,541][valid][INFO] - epoch 390 | valid on 'valid' subset | loss 348.076 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.551 | uer 27.555 | wer 41.962 | raw_wer 41.962 | wps 4742.5 | wpb 1902.9 | bsz 8.5 | num_updates 11392 | best_wer 41.962
[2024-09-17 22:39:56,542][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 390 @ 11392 updates
[2024-09-17 22:39:56,542][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:39:59,859][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:40:01,936][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 390 @ 11392 updates, score 41.962) (writing took 5.394495943000948 seconds)
[2024-09-17 22:40:01,937][fairseq_cli.train][INFO] - end of epoch 390 (average epoch stats below)
[2024-09-17 22:40:01,938][train][INFO] - epoch 390 | loss 396.702 | ntokens 10344 | nsentences 47.2727 | nll_loss 1.813 | wps 2584.8 | ups 0.25 | wpb 10344 | bsz 47.3 | num_updates 11392 | lr 6.59017e-05 | gnorm 360.68 | loss_scale 1 | train_wall 150 | gb_free 18.7 | wall 0
[2024-09-17 22:40:01,939][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:40:01,955][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 391
[2024-09-17 22:40:01,967][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 22:40:01,979][fairseq.trainer][INFO] - begin training epoch 391
[2024-09-17 22:40:01,979][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 22:40:29,359][train_inner][INFO] - epoch 391:      8 / 44 loss=411.091, ntokens=10339.1, nsentences=47.04, nll_loss=1.87, wps=2552.6, ups=0.25, wpb=10339.1, bsz=47, num_updates=11400, lr=6.5744e-05, gnorm=337.454, loss_scale=1, train_wall=680, gb_free=16.4, wall=0
[2024-09-17 22:42:31,579][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 22:42:31,580][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:42:31,596][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 34
[2024-09-17 22:42:51,868][valid][INFO] - epoch 391 | valid on 'valid' subset | loss 339.802 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.514 | uer 27.834 | wer 42.367 | raw_wer 42.367 | wps 4700.9 | wpb 1902.9 | bsz 8.5 | num_updates 11436 | best_wer 41.962
[2024-09-17 22:42:51,869][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 391 @ 11436 updates
[2024-09-17 22:42:51,869][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-17 22:42:55,274][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-17 22:42:55,328][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 391 @ 11436 updates, score 42.367) (writing took 3.4594439809989126 seconds)
[2024-09-17 22:42:55,329][fairseq_cli.train][INFO] - end of epoch 391 (average epoch stats below)
[2024-09-17 22:42:55,330][train][INFO] - epoch 391 | loss 394.627 | ntokens 10343 | nsentences 47.2727 | nll_loss 1.804 | wps 2624.7 | ups 0.25 | wpb 10343 | bsz 47.3 | num_updates 11436 | lr 6.50387e-05 | gnorm 325.253 | loss_scale 1 | train_wall 149 | gb_free 17 | wall 0
[2024-09-17 22:42:55,331][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:42:55,346][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 392
[2024-09-17 22:42:55,359][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 22:42:55,370][fairseq.trainer][INFO] - begin training epoch 392
[2024-09-17 22:42:55,371][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 22:45:25,381][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 22:45:25,382][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:45:25,398][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 35
[2024-09-17 22:45:45,672][valid][INFO] - epoch 392 | valid on 'valid' subset | loss 335.013 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.493 | uer 27.169 | wer 41.341 | raw_wer 41.341 | wps 4683.4 | wpb 1902.9 | bsz 8.5 | num_updates 11480 | best_wer 41.341
[2024-09-17 22:45:45,672][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 392 @ 11480 updates
[2024-09-17 22:45:45,673][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:45:49,041][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:45:51,154][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 392 @ 11480 updates, score 41.341) (writing took 5.481448447997536 seconds)
[2024-09-17 22:45:51,154][fairseq_cli.train][INFO] - end of epoch 392 (average epoch stats below)
[2024-09-17 22:45:51,155][train][INFO] - epoch 392 | loss 387.275 | ntokens 10343.8 | nsentences 47.2727 | nll_loss 1.77 | wps 2588.5 | ups 0.25 | wpb 10343.8 | bsz 47.3 | num_updates 11480 | lr 6.41871e-05 | gnorm 317.565 | loss_scale 1 | train_wall 150 | gb_free 17.4 | wall 0
[2024-09-17 22:45:51,156][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:45:51,172][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 393
[2024-09-17 22:45:51,184][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 22:45:51,196][fairseq.trainer][INFO] - begin training epoch 393
[2024-09-17 22:45:51,196][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 22:48:20,762][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 22:48:20,762][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:48:20,778][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 36
[2024-09-17 22:48:41,156][valid][INFO] - epoch 393 | valid on 'valid' subset | loss 336.337 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.499 | uer 26.414 | wer 40.177 | raw_wer 40.177 | wps 4681.7 | wpb 1902.9 | bsz 8.5 | num_updates 11524 | best_wer 40.177
[2024-09-17 22:48:41,157][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 393 @ 11524 updates
[2024-09-17 22:48:41,158][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:48:44,575][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:48:46,642][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 393 @ 11524 updates, score 40.177) (writing took 5.485304423000343 seconds)
[2024-09-17 22:48:46,643][fairseq_cli.train][INFO] - end of epoch 393 (average epoch stats below)
[2024-09-17 22:48:46,644][train][INFO] - epoch 393 | loss 383.635 | ntokens 10343 | nsentences 47.2727 | nll_loss 1.753 | wps 2593.3 | ups 0.25 | wpb 10343 | bsz 47.3 | num_updates 11524 | lr 6.33466e-05 | gnorm 375.517 | loss_scale 1 | train_wall 149 | gb_free 18.7 | wall 0
[2024-09-17 22:48:46,645][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:48:46,661][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 394
[2024-09-17 22:48:46,673][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 22:48:46,685][fairseq.trainer][INFO] - begin training epoch 394
[2024-09-17 22:48:46,685][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 22:51:16,818][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 22:51:16,819][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:51:16,834][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 37
[2024-09-17 22:51:37,181][valid][INFO] - epoch 394 | valid on 'valid' subset | loss 326.069 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.453 | uer 25.854 | wer 39.277 | raw_wer 39.277 | wps 4667.6 | wpb 1902.9 | bsz 8.5 | num_updates 11568 | best_wer 39.277
[2024-09-17 22:51:37,181][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 394 @ 11568 updates
[2024-09-17 22:51:37,182][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:51:40,566][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:51:42,658][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 394 @ 11568 updates, score 39.277) (writing took 5.476470992998657 seconds)
[2024-09-17 22:51:42,658][fairseq_cli.train][INFO] - end of epoch 394 (average epoch stats below)
[2024-09-17 22:51:42,659][train][INFO] - epoch 394 | loss 376.568 | ntokens 10343.6 | nsentences 47.2727 | nll_loss 1.721 | wps 2585.7 | ups 0.25 | wpb 10343.6 | bsz 47.3 | num_updates 11568 | lr 6.25171e-05 | gnorm 348.401 | loss_scale 1 | train_wall 150 | gb_free 20.1 | wall 0
[2024-09-17 22:51:42,660][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:51:42,676][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 395
[2024-09-17 22:51:42,688][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 22:51:42,700][fairseq.trainer][INFO] - begin training epoch 395
[2024-09-17 22:51:42,700][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 22:53:31,777][train_inner][INFO] - epoch 395:     32 / 44 loss=380.784, ntokens=10358.1, nsentences=47.64, nll_loss=1.751, wps=2647.7, ups=0.26, wpb=10358.1, bsz=47.6, num_updates=11600, lr=6.19206e-05, gnorm=342.033, loss_scale=1, train_wall=680, gb_free=16.4, wall=0
[2024-09-17 22:54:11,765][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 22:54:11,765][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:54:11,781][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 38
[2024-09-17 22:54:32,061][valid][INFO] - epoch 395 | valid on 'valid' subset | loss 323.599 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.442 | uer 25.814 | wer 38.637 | raw_wer 38.637 | wps 4698.5 | wpb 1902.9 | bsz 8.5 | num_updates 11612 | best_wer 38.637
[2024-09-17 22:54:32,062][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 395 @ 11612 updates
[2024-09-17 22:54:32,062][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:54:35,438][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:54:37,523][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 395 @ 11612 updates, score 38.637) (writing took 5.4612844189978205 seconds)
[2024-09-17 22:54:37,523][fairseq_cli.train][INFO] - end of epoch 395 (average epoch stats below)
[2024-09-17 22:54:37,524][train][INFO] - epoch 395 | loss 379.588 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.735 | wps 2602.7 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 11612 | lr 6.16984e-05 | gnorm 328.337 | loss_scale 1 | train_wall 149 | gb_free 19.4 | wall 0
[2024-09-17 22:54:37,525][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:54:37,540][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 396
[2024-09-17 22:54:37,553][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 22:54:37,565][fairseq.trainer][INFO] - begin training epoch 396
[2024-09-17 22:54:37,565][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 22:57:08,206][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 22:57:08,207][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:57:08,222][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 39
[2024-09-17 22:57:28,649][valid][INFO] - epoch 396 | valid on 'valid' subset | loss 319.939 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.426 | uer 25.636 | wer 38.573 | raw_wer 38.573 | wps 4657.1 | wpb 1902.9 | bsz 8.5 | num_updates 11656 | best_wer 38.573
[2024-09-17 22:57:28,652][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 396 @ 11656 updates
[2024-09-17 22:57:28,653][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:57:32,005][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 22:57:33,997][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 396 @ 11656 updates, score 38.573) (writing took 5.344925038996735 seconds)
[2024-09-17 22:57:33,997][fairseq_cli.train][INFO] - end of epoch 396 (average epoch stats below)
[2024-09-17 22:57:33,998][train][INFO] - epoch 396 | loss 368.825 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.686 | wps 2578.9 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 11656 | lr 6.08905e-05 | gnorm 308.382 | loss_scale 1 | train_wall 150 | gb_free 19.9 | wall 0
[2024-09-17 22:57:33,999][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 22:57:34,015][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 397
[2024-09-17 22:57:34,029][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 22:57:34,042][fairseq.trainer][INFO] - begin training epoch 397
[2024-09-17 22:57:34,042][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 23:00:03,306][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 23:00:03,306][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:00:03,321][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 40
[2024-09-17 23:00:23,608][valid][INFO] - epoch 397 | valid on 'valid' subset | loss 311.45 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.388 | uer 25.333 | wer 38.158 | raw_wer 38.158 | wps 4699.3 | wpb 1902.9 | bsz 8.5 | num_updates 11700 | best_wer 38.158
[2024-09-17 23:00:23,609][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 397 @ 11700 updates
[2024-09-17 23:00:23,610][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:00:27,010][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:00:29,017][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 397 @ 11700 updates, score 38.158) (writing took 5.407464977000927 seconds)
[2024-09-17 23:00:29,017][fairseq_cli.train][INFO] - end of epoch 397 (average epoch stats below)
[2024-09-17 23:00:29,018][train][INFO] - epoch 397 | loss 368.245 | ntokens 10343.8 | nsentences 47.2727 | nll_loss 1.683 | wps 2600.4 | ups 0.25 | wpb 10343.8 | bsz 47.3 | num_updates 11700 | lr 6.00931e-05 | gnorm 308.814 | loss_scale 1 | train_wall 149 | gb_free 23.6 | wall 0
[2024-09-17 23:00:29,019][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:00:29,036][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 398
[2024-09-17 23:00:29,049][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 23:00:29,064][fairseq.trainer][INFO] - begin training epoch 398
[2024-09-17 23:00:29,064][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 23:02:58,787][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 23:02:58,787][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:02:58,803][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 41
[2024-09-17 23:03:19,101][valid][INFO] - epoch 398 | valid on 'valid' subset | loss 321.807 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.434 | uer 25.201 | wer 36.857 | raw_wer 36.857 | wps 4679.4 | wpb 1902.9 | bsz 8.5 | num_updates 11744 | best_wer 36.857
[2024-09-17 23:03:19,102][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 398 @ 11744 updates
[2024-09-17 23:03:19,103][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:03:22,456][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:03:24,520][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 398 @ 11744 updates, score 36.857) (writing took 5.418027575997257 seconds)
[2024-09-17 23:03:24,520][fairseq_cli.train][INFO] - end of epoch 398 (average epoch stats below)
[2024-09-17 23:03:24,521][train][INFO] - epoch 398 | loss 363.224 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 1.66 | wps 2593.1 | ups 0.25 | wpb 10343.2 | bsz 47.3 | num_updates 11744 | lr 5.93062e-05 | gnorm 365.014 | loss_scale 1 | train_wall 150 | gb_free 20 | wall 0
[2024-09-17 23:03:24,522][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:03:24,538][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 399
[2024-09-17 23:03:24,551][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 23:03:24,563][fairseq.trainer][INFO] - begin training epoch 399
[2024-09-17 23:03:24,564][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 23:05:54,524][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 23:05:54,524][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:05:54,540][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 42
[2024-09-17 23:06:14,778][valid][INFO] - epoch 399 | valid on 'valid' subset | loss 305.836 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.363 | uer 25.355 | wer 37.566 | raw_wer 37.566 | wps 4678.9 | wpb 1902.9 | bsz 8.5 | num_updates 11788 | best_wer 36.857
[2024-09-17 23:06:14,779][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 399 @ 11788 updates
[2024-09-17 23:06:14,779][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-17 23:06:18,253][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-17 23:06:18,305][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 399 @ 11788 updates, score 37.566) (writing took 3.5261539600032847 seconds)
[2024-09-17 23:06:18,306][fairseq_cli.train][INFO] - end of epoch 399 (average epoch stats below)
[2024-09-17 23:06:18,307][train][INFO] - epoch 399 | loss 357.561 | ntokens 10343.8 | nsentences 47.2727 | nll_loss 1.634 | wps 2618.9 | ups 0.25 | wpb 10343.8 | bsz 47.3 | num_updates 11788 | lr 5.85296e-05 | gnorm 275.497 | loss_scale 1 | train_wall 150 | gb_free 18.7 | wall 0
[2024-09-17 23:06:18,308][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:06:18,324][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 400
[2024-09-17 23:06:18,341][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 23:06:18,358][fairseq.trainer][INFO] - begin training epoch 400
[2024-09-17 23:06:18,358][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 23:07:00,564][train_inner][INFO] - epoch 400:     12 / 44 loss=365.87, ntokens=10341.5, nsentences=47.16, nll_loss=1.668, wps=2557.3, ups=0.25, wpb=10341.5, bsz=47.2, num_updates=11800, lr=5.83196e-05, gnorm=315.192, loss_scale=1, train_wall=681, gb_free=13.8, wall=0
[2024-09-17 23:08:48,239][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 23:08:48,240][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:08:48,256][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 43
[2024-09-17 23:09:08,576][valid][INFO] - epoch 400 | valid on 'valid' subset | loss 311.773 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.389 | uer 24.354 | wer 36.295 | raw_wer 36.295 | wps 4675.7 | wpb 1902.9 | bsz 8.5 | num_updates 11832 | best_wer 36.295
[2024-09-17 23:09:08,577][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 400 @ 11832 updates
[2024-09-17 23:09:08,577][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:09:11,998][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:09:14,090][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 400 @ 11832 updates, score 36.295) (writing took 5.513513497000531 seconds)
[2024-09-17 23:09:14,091][fairseq_cli.train][INFO] - end of epoch 400 (average epoch stats below)
[2024-09-17 23:09:14,092][train][INFO] - epoch 400 | loss 361.501 | ntokens 10343.7 | nsentences 47.2727 | nll_loss 1.652 | wps 2589.1 | ups 0.25 | wpb 10343.7 | bsz 47.3 | num_updates 11832 | lr 5.77632e-05 | gnorm 353.568 | loss_scale 1 | train_wall 150 | gb_free 20.3 | wall 0
[2024-09-17 23:09:14,093][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:09:14,109][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 401
[2024-09-17 23:09:14,121][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 23:09:14,135][fairseq.trainer][INFO] - begin training epoch 401
[2024-09-17 23:09:14,135][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 23:11:43,714][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 23:11:43,714][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:11:43,730][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 44
[2024-09-17 23:12:04,008][valid][INFO] - epoch 401 | valid on 'valid' subset | loss 301.938 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.346 | uer 24.492 | wer 36.647 | raw_wer 36.647 | wps 4687.3 | wpb 1902.9 | bsz 8.5 | num_updates 11876 | best_wer 36.295
[2024-09-17 23:12:04,009][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 401 @ 11876 updates
[2024-09-17 23:12:04,009][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-17 23:12:07,446][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-17 23:12:07,500][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 401 @ 11876 updates, score 36.647) (writing took 3.4918071669999335 seconds)
[2024-09-17 23:12:07,501][fairseq_cli.train][INFO] - end of epoch 401 (average epoch stats below)
[2024-09-17 23:12:07,502][train][INFO] - epoch 401 | loss 354.471 | ntokens 10343.6 | nsentences 47.2727 | nll_loss 1.62 | wps 2624.5 | ups 0.25 | wpb 10343.6 | bsz 47.3 | num_updates 11876 | lr 5.70068e-05 | gnorm 324.507 | loss_scale 1 | train_wall 149 | gb_free 16.8 | wall 0
[2024-09-17 23:12:07,503][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:12:07,519][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 402
[2024-09-17 23:12:07,532][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 23:12:07,551][fairseq.trainer][INFO] - begin training epoch 402
[2024-09-17 23:12:07,552][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 23:14:37,749][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 23:14:37,749][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:14:37,765][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 45
[2024-09-17 23:14:58,007][valid][INFO] - epoch 402 | valid on 'valid' subset | loss 302.021 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.346 | uer 24.45 | wer 36.295 | raw_wer 36.295 | wps 4705.5 | wpb 1902.9 | bsz 8.5 | num_updates 11920 | best_wer 36.295
[2024-09-17 23:14:58,008][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 402 @ 11920 updates
[2024-09-17 23:14:58,009][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:15:01,414][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:15:03,498][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 402 @ 11920 updates, score 36.295) (writing took 5.489784572000644 seconds)
[2024-09-17 23:15:03,498][fairseq_cli.train][INFO] - end of epoch 402 (average epoch stats below)
[2024-09-17 23:15:03,499][train][INFO] - epoch 402 | loss 348.667 | ntokens 10343.4 | nsentences 47.2727 | nll_loss 1.594 | wps 2585.9 | ups 0.25 | wpb 10343.4 | bsz 47.3 | num_updates 11920 | lr 5.62603e-05 | gnorm 305.251 | loss_scale 1 | train_wall 150 | gb_free 21.6 | wall 0
[2024-09-17 23:15:03,500][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:15:03,516][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 403
[2024-09-17 23:15:03,528][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 23:15:03,541][fairseq.trainer][INFO] - begin training epoch 403
[2024-09-17 23:15:03,541][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 23:17:33,154][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 23:17:33,155][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:17:33,169][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 46
[2024-09-17 23:17:53,386][valid][INFO] - epoch 403 | valid on 'valid' subset | loss 293.845 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.309 | uer 24.254 | wer 36.016 | raw_wer 36.016 | wps 4705.3 | wpb 1902.9 | bsz 8.5 | num_updates 11964 | best_wer 36.016
[2024-09-17 23:17:53,386][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 403 @ 11964 updates
[2024-09-17 23:17:53,387][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:17:56,741][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:17:58,714][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 403 @ 11964 updates, score 36.016) (writing took 5.328184763999161 seconds)
[2024-09-17 23:17:58,715][fairseq_cli.train][INFO] - end of epoch 403 (average epoch stats below)
[2024-09-17 23:17:58,716][train][INFO] - epoch 403 | loss 344.875 | ntokens 10343.4 | nsentences 47.2727 | nll_loss 1.576 | wps 2597.4 | ups 0.25 | wpb 10343.4 | bsz 47.3 | num_updates 11964 | lr 5.55236e-05 | gnorm 322.261 | loss_scale 1 | train_wall 149 | gb_free 20.9 | wall 0
[2024-09-17 23:17:58,717][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:17:58,733][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 404
[2024-09-17 23:17:58,745][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 23:17:58,759][fairseq.trainer][INFO] - begin training epoch 404
[2024-09-17 23:17:58,759][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 23:20:01,575][train_inner][INFO] - epoch 404:     36 / 44 loss=352.488, ntokens=10331.4, nsentences=47.08, nll_loss=1.606, wps=2645.7, ups=0.26, wpb=10331.4, bsz=47.1, num_updates=12000, lr=5.4928e-05, gnorm=319.417, loss_scale=1, train_wall=679, gb_free=20.2, wall=0
[2024-09-17 23:20:28,528][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 23:20:28,529][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:20:28,544][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 47
[2024-09-17 23:20:48,836][valid][INFO] - epoch 404 | valid on 'valid' subset | loss 287.957 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.283 | uer 24.163 | wer 35.83 | raw_wer 35.83 | wps 4684.1 | wpb 1902.9 | bsz 8.5 | num_updates 12008 | best_wer 35.83
[2024-09-17 23:20:48,837][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 404 @ 12008 updates
[2024-09-17 23:20:48,838][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:20:52,348][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:20:54,403][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 404 @ 12008 updates, score 35.83) (writing took 5.56595645699781 seconds)
[2024-09-17 23:20:54,404][fairseq_cli.train][INFO] - end of epoch 404 (average epoch stats below)
[2024-09-17 23:20:54,405][train][INFO] - epoch 404 | loss 345.746 | ntokens 10343 | nsentences 47.2727 | nll_loss 1.58 | wps 2590.3 | ups 0.25 | wpb 10343 | bsz 47.3 | num_updates 12008 | lr 5.47965e-05 | gnorm 295.341 | loss_scale 1 | train_wall 150 | gb_free 19.3 | wall 0
[2024-09-17 23:20:54,405][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:20:54,421][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 405
[2024-09-17 23:20:54,433][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 23:20:54,447][fairseq.trainer][INFO] - begin training epoch 405
[2024-09-17 23:20:54,447][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 23:23:25,097][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 23:23:25,097][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:23:25,113][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 48
[2024-09-17 23:23:45,407][valid][INFO] - epoch 405 | valid on 'valid' subset | loss 289.773 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.291 | uer 23.621 | wer 35.199 | raw_wer 35.199 | wps 4682.3 | wpb 1902.9 | bsz 8.5 | num_updates 12052 | best_wer 35.199
[2024-09-17 23:23:45,408][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 405 @ 12052 updates
[2024-09-17 23:23:45,408][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:23:48,786][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:23:50,771][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 405 @ 12052 updates, score 35.199) (writing took 5.363202652999462 seconds)
[2024-09-17 23:23:50,772][fairseq_cli.train][INFO] - end of epoch 405 (average epoch stats below)
[2024-09-17 23:23:50,773][train][INFO] - epoch 405 | loss 332.742 | ntokens 10343.7 | nsentences 47.2727 | nll_loss 1.521 | wps 2580.5 | ups 0.25 | wpb 10343.7 | bsz 47.3 | num_updates 12052 | lr 5.4079e-05 | gnorm 284.451 | loss_scale 1 | train_wall 150 | gb_free 17.7 | wall 0
[2024-09-17 23:23:50,773][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:23:50,789][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 406
[2024-09-17 23:23:50,802][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 23:23:50,816][fairseq.trainer][INFO] - begin training epoch 406
[2024-09-17 23:23:50,816][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 23:26:20,608][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 23:26:20,608][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:26:20,633][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 49
[2024-09-17 23:26:40,875][valid][INFO] - epoch 406 | valid on 'valid' subset | loss 290.311 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.294 | uer 23.396 | wer 34.539 | raw_wer 34.539 | wps 4704.3 | wpb 1902.9 | bsz 8.5 | num_updates 12096 | best_wer 34.539
[2024-09-17 23:26:40,876][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 406 @ 12096 updates
[2024-09-17 23:26:40,877][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:26:44,268][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:26:46,306][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 406 @ 12096 updates, score 34.539) (writing took 5.43004415699761 seconds)
[2024-09-17 23:26:46,306][fairseq_cli.train][INFO] - end of epoch 406 (average epoch stats below)
[2024-09-17 23:26:46,308][train][INFO] - epoch 406 | loss 339.564 | ntokens 10343.4 | nsentences 47.2727 | nll_loss 1.552 | wps 2592.7 | ups 0.25 | wpb 10343.4 | bsz 47.3 | num_updates 12096 | lr 5.33708e-05 | gnorm 306.207 | loss_scale 1 | train_wall 150 | gb_free 18.5 | wall 0
[2024-09-17 23:26:46,308][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:26:46,324][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 407
[2024-09-17 23:26:46,337][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 23:26:46,351][fairseq.trainer][INFO] - begin training epoch 407
[2024-09-17 23:26:46,351][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 23:29:16,058][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 23:29:16,058][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:29:16,073][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 50
[2024-09-17 23:29:36,189][valid][INFO] - epoch 407 | valid on 'valid' subset | loss 281.777 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.256 | uer 23.825 | wer 35.288 | raw_wer 35.288 | wps 4752.7 | wpb 1902.9 | bsz 8.5 | num_updates 12140 | best_wer 34.539
[2024-09-17 23:29:36,190][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 407 @ 12140 updates
[2024-09-17 23:29:36,191][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-17 23:29:39,635][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-17 23:29:39,696][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 407 @ 12140 updates, score 35.288) (writing took 3.505720872002712 seconds)
[2024-09-17 23:29:39,697][fairseq_cli.train][INFO] - end of epoch 407 (average epoch stats below)
[2024-09-17 23:29:39,698][train][INFO] - epoch 407 | loss 333.561 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.524 | wps 2624.8 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 12140 | lr 5.2672e-05 | gnorm 312.764 | loss_scale 1 | train_wall 150 | gb_free 16.9 | wall 0
[2024-09-17 23:29:39,699][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:29:39,719][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 408
[2024-09-17 23:29:39,734][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 23:29:39,753][fairseq.trainer][INFO] - begin training epoch 408
[2024-09-17 23:29:39,754][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 23:32:10,150][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 23:32:10,151][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:32:10,165][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 51
[2024-09-17 23:32:30,509][valid][INFO] - epoch 408 | valid on 'valid' subset | loss 282.595 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.259 | uer 23.182 | wer 34.442 | raw_wer 34.442 | wps 4670.1 | wpb 1902.9 | bsz 8.5 | num_updates 12184 | best_wer 34.442
[2024-09-17 23:32:30,510][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 408 @ 12184 updates
[2024-09-17 23:32:30,511][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:32:33,847][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:32:35,848][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 408 @ 12184 updates, score 34.442) (writing took 5.33816199499779 seconds)
[2024-09-17 23:32:35,848][fairseq_cli.train][INFO] - end of epoch 408 (average epoch stats below)
[2024-09-17 23:32:35,850][train][INFO] - epoch 408 | loss 328.782 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 1.503 | wps 2583.6 | ups 0.25 | wpb 10343.2 | bsz 47.3 | num_updates 12184 | lr 5.19822e-05 | gnorm 320.574 | loss_scale 1 | train_wall 150 | gb_free 17.5 | wall 0
[2024-09-17 23:32:35,850][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:32:35,866][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 409
[2024-09-17 23:32:35,878][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 23:32:35,893][fairseq.trainer][INFO] - begin training epoch 409
[2024-09-17 23:32:35,893][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 23:33:30,875][train_inner][INFO] - epoch 409:     16 / 44 loss=332.763, ntokens=10351.2, nsentences=47.28, nll_loss=1.52, wps=2558.1, ups=0.25, wpb=10351.2, bsz=47.3, num_updates=12200, lr=5.17337e-05, gnorm=303.892, loss_scale=1, train_wall=682, gb_free=18.6, wall=0
[2024-09-17 23:35:06,630][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 23:35:06,631][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:35:06,647][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 52
[2024-09-17 23:35:26,910][valid][INFO] - epoch 409 | valid on 'valid' subset | loss 282.12 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.257 | uer 23.093 | wer 33.928 | raw_wer 33.928 | wps 4688.7 | wpb 1902.9 | bsz 8.5 | num_updates 12228 | best_wer 33.928
[2024-09-17 23:35:26,911][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 409 @ 12228 updates
[2024-09-17 23:35:26,912][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:35:30,230][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:35:32,243][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 409 @ 12228 updates, score 33.928) (writing took 5.332374759000231 seconds)
[2024-09-17 23:35:32,244][fairseq_cli.train][INFO] - end of epoch 409 (average epoch stats below)
[2024-09-17 23:35:32,245][train][INFO] - epoch 409 | loss 322.014 | ntokens 10343.4 | nsentences 47.2727 | nll_loss 1.472 | wps 2580.1 | ups 0.25 | wpb 10343.4 | bsz 47.3 | num_updates 12228 | lr 5.13016e-05 | gnorm 287.819 | loss_scale 1 | train_wall 151 | gb_free 20.9 | wall 0
[2024-09-17 23:35:32,246][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:35:32,261][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 410
[2024-09-17 23:35:32,273][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 23:35:32,288][fairseq.trainer][INFO] - begin training epoch 410
[2024-09-17 23:35:32,288][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 23:38:01,791][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 23:38:01,792][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:38:01,807][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 53
[2024-09-17 23:38:22,215][valid][INFO] - epoch 410 | valid on 'valid' subset | loss 271.675 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.211 | uer 23.553 | wer 35.033 | raw_wer 35.033 | wps 4647.5 | wpb 1902.9 | bsz 8.5 | num_updates 12272 | best_wer 33.928
[2024-09-17 23:38:22,215][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 410 @ 12272 updates
[2024-09-17 23:38:22,216][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-17 23:38:26,363][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-17 23:38:26,440][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 410 @ 12272 updates, score 35.033) (writing took 4.224587768996571 seconds)
[2024-09-17 23:38:26,440][fairseq_cli.train][INFO] - end of epoch 410 (average epoch stats below)
[2024-09-17 23:38:26,442][train][INFO] - epoch 410 | loss 325.072 | ntokens 10343.4 | nsentences 47.2727 | nll_loss 1.486 | wps 2612.6 | ups 0.25 | wpb 10343.4 | bsz 47.3 | num_updates 12272 | lr 5.06298e-05 | gnorm 300.443 | loss_scale 1 | train_wall 149 | gb_free 22.9 | wall 0
[2024-09-17 23:38:26,442][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:38:26,458][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 411
[2024-09-17 23:38:26,470][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 23:38:26,485][fairseq.trainer][INFO] - begin training epoch 411
[2024-09-17 23:38:26,485][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 23:40:56,371][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 23:40:56,372][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:40:56,387][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 54
[2024-09-17 23:41:16,627][valid][INFO] - epoch 411 | valid on 'valid' subset | loss 271.344 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.209 | uer 22.871 | wer 33.723 | raw_wer 33.723 | wps 4704.5 | wpb 1902.9 | bsz 8.5 | num_updates 12316 | best_wer 33.723
[2024-09-17 23:41:16,628][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 411 @ 12316 updates
[2024-09-17 23:41:16,628][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:41:19,988][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:41:22,080][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 411 @ 12316 updates, score 33.723) (writing took 5.4519017259990505 seconds)
[2024-09-17 23:41:22,080][fairseq_cli.train][INFO] - end of epoch 411 (average epoch stats below)
[2024-09-17 23:41:22,081][train][INFO] - epoch 411 | loss 323.939 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 1.481 | wps 2591.1 | ups 0.25 | wpb 10343.2 | bsz 47.3 | num_updates 12316 | lr 4.99668e-05 | gnorm 282.704 | loss_scale 1 | train_wall 150 | gb_free 20.7 | wall 0
[2024-09-17 23:41:22,082][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:41:22,097][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 412
[2024-09-17 23:41:22,109][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 23:41:22,124][fairseq.trainer][INFO] - begin training epoch 412
[2024-09-17 23:41:22,124][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 23:43:53,295][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 23:43:53,296][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:43:53,311][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 55
[2024-09-17 23:44:13,588][valid][INFO] - epoch 412 | valid on 'valid' subset | loss 275.665 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.228 | uer 22.431 | wer 33.024 | raw_wer 33.024 | wps 4690 | wpb 1902.9 | bsz 8.5 | num_updates 12360 | best_wer 33.024
[2024-09-17 23:44:13,589][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 412 @ 12360 updates
[2024-09-17 23:44:13,589][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:44:16,942][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:44:18,940][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 412 @ 12360 updates, score 33.024) (writing took 5.351771180998185 seconds)
[2024-09-17 23:44:18,941][fairseq_cli.train][INFO] - end of epoch 412 (average epoch stats below)
[2024-09-17 23:44:18,942][train][INFO] - epoch 412 | loss 321.471 | ntokens 10343.7 | nsentences 47.2727 | nll_loss 1.469 | wps 2573.3 | ups 0.25 | wpb 10343.7 | bsz 47.3 | num_updates 12360 | lr 4.93125e-05 | gnorm 385.018 | loss_scale 1 | train_wall 151 | gb_free 16.9 | wall 0
[2024-09-17 23:44:18,943][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:44:18,958][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 413
[2024-09-17 23:44:18,971][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 23:44:18,986][fairseq.trainer][INFO] - begin training epoch 413
[2024-09-17 23:44:18,986][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 23:46:34,818][train_inner][INFO] - epoch 413:     40 / 44 loss=321.985, ntokens=10335.6, nsentences=47.48, nll_loss=1.479, wps=2636.8, ups=0.26, wpb=10335.6, bsz=47.5, num_updates=12400, lr=4.87251e-05, gnorm=316.555, loss_scale=1, train_wall=681, gb_free=18.1, wall=0
[2024-09-17 23:46:48,595][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 23:46:48,596][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:46:48,611][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 56
[2024-09-17 23:47:08,871][valid][INFO] - epoch 413 | valid on 'valid' subset | loss 269.508 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.201 | uer 22.334 | wer 32.559 | raw_wer 32.559 | wps 4695.3 | wpb 1902.9 | bsz 8.5 | num_updates 12404 | best_wer 32.559
[2024-09-17 23:47:08,871][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 413 @ 12404 updates
[2024-09-17 23:47:08,872][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:47:12,235][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:47:14,256][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 413 @ 12404 updates, score 32.559) (writing took 5.384817259997362 seconds)
[2024-09-17 23:47:14,257][fairseq_cli.train][INFO] - end of epoch 413 (average epoch stats below)
[2024-09-17 23:47:14,258][train][INFO] - epoch 413 | loss 321.367 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.469 | wps 2596 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 12404 | lr 4.86668e-05 | gnorm 315.19 | loss_scale 1 | train_wall 149 | gb_free 21.3 | wall 0
[2024-09-17 23:47:14,259][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:47:14,275][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 414
[2024-09-17 23:47:14,289][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 23:47:14,304][fairseq.trainer][INFO] - begin training epoch 414
[2024-09-17 23:47:14,304][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 23:49:44,209][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 23:49:44,210][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:49:44,225][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 57
[2024-09-17 23:50:04,559][valid][INFO] - epoch 414 | valid on 'valid' subset | loss 264.094 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.177 | uer 23.056 | wer 33.742 | raw_wer 33.742 | wps 4694.2 | wpb 1902.9 | bsz 8.5 | num_updates 12448 | best_wer 32.559
[2024-09-17 23:50:04,560][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 414 @ 12448 updates
[2024-09-17 23:50:04,560][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-17 23:50:07,991][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-17 23:50:08,043][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 414 @ 12448 updates, score 33.742) (writing took 3.483005124999181 seconds)
[2024-09-17 23:50:08,043][fairseq_cli.train][INFO] - end of epoch 414 (average epoch stats below)
[2024-09-17 23:50:08,044][train][INFO] - epoch 414 | loss 317.551 | ntokens 10343.7 | nsentences 47.2727 | nll_loss 1.451 | wps 2618.9 | ups 0.25 | wpb 10343.7 | bsz 47.3 | num_updates 12448 | lr 4.80295e-05 | gnorm 292.453 | loss_scale 1 | train_wall 150 | gb_free 17.4 | wall 0
[2024-09-17 23:50:08,045][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:50:08,061][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 415
[2024-09-17 23:50:08,073][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 23:50:08,090][fairseq.trainer][INFO] - begin training epoch 415
[2024-09-17 23:50:08,090][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 23:52:36,908][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 23:52:36,909][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:52:36,923][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 58
[2024-09-17 23:52:57,311][valid][INFO] - epoch 415 | valid on 'valid' subset | loss 266.913 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.189 | uer 22.185 | wer 32.432 | raw_wer 32.432 | wps 4655.8 | wpb 1902.9 | bsz 8.5 | num_updates 12492 | best_wer 32.432
[2024-09-17 23:52:57,311][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 415 @ 12492 updates
[2024-09-17 23:52:57,312][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:53:00,689][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:53:02,685][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 415 @ 12492 updates, score 32.432) (writing took 5.374011740001151 seconds)
[2024-09-17 23:53:02,686][fairseq_cli.train][INFO] - end of epoch 415 (average epoch stats below)
[2024-09-17 23:53:02,687][train][INFO] - epoch 415 | loss 324.905 | ntokens 10343.4 | nsentences 47.2727 | nll_loss 1.485 | wps 2606 | ups 0.25 | wpb 10343.4 | bsz 47.3 | num_updates 12492 | lr 4.74005e-05 | gnorm 297.237 | loss_scale 1 | train_wall 149 | gb_free 17.4 | wall 0
[2024-09-17 23:53:02,688][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:53:02,704][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 416
[2024-09-17 23:53:02,716][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 23:53:02,732][fairseq.trainer][INFO] - begin training epoch 416
[2024-09-17 23:53:02,732][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 23:55:31,654][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 23:55:31,655][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:55:31,671][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 59
[2024-09-17 23:55:51,769][valid][INFO] - epoch 416 | valid on 'valid' subset | loss 266.909 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.189 | uer 21.801 | wer 32.28 | raw_wer 32.28 | wps 4736.9 | wpb 1902.9 | bsz 8.5 | num_updates 12536 | best_wer 32.28
[2024-09-17 23:55:51,770][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 416 @ 12536 updates
[2024-09-17 23:55:51,770][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:55:55,037][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-17 23:55:57,036][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 416 @ 12536 updates, score 32.28) (writing took 5.26667900800021 seconds)
[2024-09-17 23:55:57,037][fairseq_cli.train][INFO] - end of epoch 416 (average epoch stats below)
[2024-09-17 23:55:57,038][train][INFO] - epoch 416 | loss 322.045 | ntokens 10343.8 | nsentences 47.2727 | nll_loss 1.472 | wps 2610.4 | ups 0.25 | wpb 10343.8 | bsz 47.3 | num_updates 12536 | lr 4.67798e-05 | gnorm 317.453 | loss_scale 1 | train_wall 149 | gb_free 20.2 | wall 0
[2024-09-17 23:55:57,039][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:55:57,054][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 417
[2024-09-17 23:55:57,067][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 23:55:57,082][fairseq.trainer][INFO] - begin training epoch 417
[2024-09-17 23:55:57,082][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-17 23:58:27,350][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-17 23:58:27,350][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:58:27,365][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 60
[2024-09-17 23:58:47,766][valid][INFO] - epoch 417 | valid on 'valid' subset | loss 258.262 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.151 | uer 22.2 | wer 32.794 | raw_wer 32.794 | wps 4671.4 | wpb 1902.9 | bsz 8.5 | num_updates 12580 | best_wer 32.28
[2024-09-17 23:58:47,766][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 417 @ 12580 updates
[2024-09-17 23:58:47,767][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-17 23:58:51,233][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-17 23:58:51,310][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 417 @ 12580 updates, score 32.794) (writing took 3.5438778520001506 seconds)
[2024-09-17 23:58:51,311][fairseq_cli.train][INFO] - end of epoch 417 (average epoch stats below)
[2024-09-17 23:58:51,312][train][INFO] - epoch 417 | loss 313.342 | ntokens 10343.7 | nsentences 47.2727 | nll_loss 1.432 | wps 2611.5 | ups 0.25 | wpb 10343.7 | bsz 47.3 | num_updates 12580 | lr 4.61673e-05 | gnorm 314.691 | loss_scale 1 | train_wall 150 | gb_free 16.1 | wall 0
[2024-09-17 23:58:51,313][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-17 23:58:51,328][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 418
[2024-09-17 23:58:51,341][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-17 23:58:51,357][fairseq.trainer][INFO] - begin training epoch 418
[2024-09-17 23:58:51,358][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 00:00:00,568][train_inner][INFO] - epoch 418:     20 / 44 loss=318.117, ntokens=10342.3, nsentences=47.12, nll_loss=1.449, wps=2567.1, ups=0.25, wpb=10342.3, bsz=47.1, num_updates=12600, lr=4.58915e-05, gnorm=304.281, loss_scale=1, train_wall=680, gb_free=17.3, wall=0
[2024-09-18 00:01:20,982][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 00:01:20,982][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:01:20,997][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 61
[2024-09-18 00:01:41,190][valid][INFO] - epoch 418 | valid on 'valid' subset | loss 260.797 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.162 | uer 22.092 | wer 32.192 | raw_wer 32.192 | wps 4715.1 | wpb 1902.9 | bsz 8.5 | num_updates 12624 | best_wer 32.192
[2024-09-18 00:01:41,191][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 418 @ 12624 updates
[2024-09-18 00:01:41,192][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 00:01:44,546][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 00:01:46,563][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 418 @ 12624 updates, score 32.192) (writing took 5.371946886996739 seconds)
[2024-09-18 00:01:46,564][fairseq_cli.train][INFO] - end of epoch 418 (average epoch stats below)
[2024-09-18 00:01:46,565][train][INFO] - epoch 418 | loss 312.403 | ntokens 10343.3 | nsentences 47.2727 | nll_loss 1.428 | wps 2596.9 | ups 0.25 | wpb 10343.3 | bsz 47.3 | num_updates 12624 | lr 4.55627e-05 | gnorm 297.305 | loss_scale 1 | train_wall 149 | gb_free 16.5 | wall 0
[2024-09-18 00:01:46,565][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:01:46,581][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 419
[2024-09-18 00:01:46,593][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 00:01:46,609][fairseq.trainer][INFO] - begin training epoch 419
[2024-09-18 00:01:46,610][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 00:04:15,307][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 00:04:15,308][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:04:15,323][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 62
[2024-09-18 00:04:35,756][valid][INFO] - epoch 419 | valid on 'valid' subset | loss 257.313 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.147 | uer 22.438 | wer 32.721 | raw_wer 32.721 | wps 4643.8 | wpb 1902.9 | bsz 8.5 | num_updates 12668 | best_wer 32.192
[2024-09-18 00:04:35,756][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 419 @ 12668 updates
[2024-09-18 00:04:35,757][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 00:04:39,088][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 00:04:39,139][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 419 @ 12668 updates, score 32.721) (writing took 3.382720278998022 seconds)
[2024-09-18 00:04:39,140][fairseq_cli.train][INFO] - end of epoch 419 (average epoch stats below)
[2024-09-18 00:04:39,141][train][INFO] - epoch 419 | loss 315.229 | ntokens 10343.8 | nsentences 47.2727 | nll_loss 1.441 | wps 2637.3 | ups 0.25 | wpb 10343.8 | bsz 47.3 | num_updates 12668 | lr 4.49661e-05 | gnorm 301.587 | loss_scale 1 | train_wall 149 | gb_free 17.3 | wall 0
[2024-09-18 00:04:39,142][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:04:39,163][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 420
[2024-09-18 00:04:39,180][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 00:04:39,200][fairseq.trainer][INFO] - begin training epoch 420
[2024-09-18 00:04:39,201][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 00:07:09,725][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 00:07:09,726][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:07:09,741][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 63
[2024-09-18 00:07:29,957][valid][INFO] - epoch 420 | valid on 'valid' subset | loss 254.459 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.134 | uer 22.185 | wer 32.53 | raw_wer 32.53 | wps 4712.4 | wpb 1902.9 | bsz 8.5 | num_updates 12712 | best_wer 32.192
[2024-09-18 00:07:29,958][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 420 @ 12712 updates
[2024-09-18 00:07:29,958][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 00:07:33,368][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 00:07:33,441][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 420 @ 12712 updates, score 32.53) (writing took 3.48378867500287 seconds)
[2024-09-18 00:07:33,442][fairseq_cli.train][INFO] - end of epoch 420 (average epoch stats below)
[2024-09-18 00:07:33,443][train][INFO] - epoch 420 | loss 299.345 | ntokens 10343.3 | nsentences 47.2727 | nll_loss 1.368 | wps 2611 | ups 0.25 | wpb 10343.3 | bsz 47.3 | num_updates 12712 | lr 4.43773e-05 | gnorm 285.793 | loss_scale 1 | train_wall 150 | gb_free 18.6 | wall 0
[2024-09-18 00:07:33,444][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:07:33,459][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 421
[2024-09-18 00:07:33,471][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 00:07:33,488][fairseq.trainer][INFO] - begin training epoch 421
[2024-09-18 00:07:33,488][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 00:10:03,297][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 00:10:03,298][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:10:03,313][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 64
[2024-09-18 00:10:23,751][valid][INFO] - epoch 421 | valid on 'valid' subset | loss 253.025 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.128 | uer 21.392 | wer 31.371 | raw_wer 31.371 | wps 4639.3 | wpb 1902.9 | bsz 8.5 | num_updates 12756 | best_wer 31.371
[2024-09-18 00:10:23,752][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 421 @ 12756 updates
[2024-09-18 00:10:23,753][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 00:10:27,088][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 00:10:29,051][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 421 @ 12756 updates, score 31.371) (writing took 5.298910684003204 seconds)
[2024-09-18 00:10:29,051][fairseq_cli.train][INFO] - end of epoch 421 (average epoch stats below)
[2024-09-18 00:10:29,052][train][INFO] - epoch 421 | loss 307.7 | ntokens 10343.3 | nsentences 47.2727 | nll_loss 1.406 | wps 2591.6 | ups 0.25 | wpb 10343.3 | bsz 47.3 | num_updates 12756 | lr 4.37962e-05 | gnorm 300.663 | loss_scale 1 | train_wall 150 | gb_free 16.2 | wall 0
[2024-09-18 00:10:29,053][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:10:29,069][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 422
[2024-09-18 00:10:29,082][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 00:10:29,098][fairseq.trainer][INFO] - begin training epoch 422
[2024-09-18 00:10:29,098][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 00:12:59,211][train_inner][INFO] - epoch 422:     44 / 44 loss=307.75, ntokens=10343.4, nsentences=47.32, nll_loss=1.408, wps=2656.8, ups=0.26, wpb=10343.4, bsz=47.3, num_updates=12800, lr=4.32227e-05, gnorm=296.219, loss_scale=1, train_wall=679, gb_free=16.5, wall=0
[2024-09-18 00:12:59,211][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 00:12:59,212][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:12:59,228][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 65
[2024-09-18 00:13:19,442][valid][INFO] - epoch 422 | valid on 'valid' subset | loss 254.378 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.134 | uer 21.246 | wer 31.293 | raw_wer 31.293 | wps 4707.5 | wpb 1902.9 | bsz 8.5 | num_updates 12800 | best_wer 31.293
[2024-09-18 00:13:19,443][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 422 @ 12800 updates
[2024-09-18 00:13:19,443][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 00:13:22,731][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 00:13:24,661][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 422 @ 12800 updates, score 31.293) (writing took 5.218363553998643 seconds)
[2024-09-18 00:13:24,662][fairseq_cli.train][INFO] - end of epoch 422 (average epoch stats below)
[2024-09-18 00:13:24,663][train][INFO] - epoch 422 | loss 302.442 | ntokens 10343.8 | nsentences 47.2727 | nll_loss 1.382 | wps 2591.7 | ups 0.25 | wpb 10343.8 | bsz 47.3 | num_updates 12800 | lr 4.32227e-05 | gnorm 295.71 | loss_scale 1 | train_wall 150 | gb_free 16.5 | wall 0
[2024-09-18 00:13:24,663][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:13:24,680][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 423
[2024-09-18 00:13:24,692][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 00:13:24,709][fairseq.trainer][INFO] - begin training epoch 423
[2024-09-18 00:13:24,710][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 00:15:54,654][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 00:15:54,654][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:15:54,670][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 66
[2024-09-18 00:16:15,040][valid][INFO] - epoch 423 | valid on 'valid' subset | loss 252.046 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.123 | uer 20.997 | wer 31.117 | raw_wer 31.117 | wps 4655 | wpb 1902.9 | bsz 8.5 | num_updates 12844 | best_wer 31.117
[2024-09-18 00:16:15,040][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 423 @ 12844 updates
[2024-09-18 00:16:15,041][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 00:16:18,316][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 00:16:20,153][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 423 @ 12844 updates, score 31.117) (writing took 5.112657689998741 seconds)
[2024-09-18 00:16:20,154][fairseq_cli.train][INFO] - end of epoch 423 (average epoch stats below)
[2024-09-18 00:16:20,155][train][INFO] - epoch 423 | loss 301.468 | ntokens 10343.3 | nsentences 47.2727 | nll_loss 1.378 | wps 2593.3 | ups 0.25 | wpb 10343.3 | bsz 47.3 | num_updates 12844 | lr 4.26567e-05 | gnorm 306.217 | loss_scale 1 | train_wall 150 | gb_free 19.7 | wall 0
[2024-09-18 00:16:20,155][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:16:20,171][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 424
[2024-09-18 00:16:20,184][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 00:16:20,207][fairseq.trainer][INFO] - begin training epoch 424
[2024-09-18 00:16:20,207][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 00:18:49,709][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 00:18:49,710][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:18:49,726][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 67
[2024-09-18 00:19:09,942][valid][INFO] - epoch 424 | valid on 'valid' subset | loss 251.364 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.12 | uer 21.817 | wer 31.943 | raw_wer 31.943 | wps 4726.5 | wpb 1902.9 | bsz 8.5 | num_updates 12888 | best_wer 31.117
[2024-09-18 00:19:09,942][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 424 @ 12888 updates
[2024-09-18 00:19:09,943][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 00:19:13,359][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 00:19:13,434][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 424 @ 12888 updates, score 31.943) (writing took 3.4916561739955796 seconds)
[2024-09-18 00:19:13,435][fairseq_cli.train][INFO] - end of epoch 424 (average epoch stats below)
[2024-09-18 00:19:13,436][train][INFO] - epoch 424 | loss 306.709 | ntokens 10343.8 | nsentences 47.2727 | nll_loss 1.402 | wps 2626.5 | ups 0.25 | wpb 10343.8 | bsz 47.3 | num_updates 12888 | lr 4.20981e-05 | gnorm 315.414 | loss_scale 1 | train_wall 149 | gb_free 20.5 | wall 0
[2024-09-18 00:19:13,436][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:19:13,452][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 425
[2024-09-18 00:19:13,465][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 00:19:13,482][fairseq.trainer][INFO] - begin training epoch 425
[2024-09-18 00:19:13,483][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 00:21:43,341][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 00:21:43,342][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:21:43,356][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 68
[2024-09-18 00:22:03,610][valid][INFO] - epoch 425 | valid on 'valid' subset | loss 245.91 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.096 | uer 21.66 | wer 31.415 | raw_wer 31.415 | wps 4695.1 | wpb 1902.9 | bsz 8.5 | num_updates 12932 | best_wer 31.117
[2024-09-18 00:22:03,610][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 425 @ 12932 updates
[2024-09-18 00:22:03,611][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 00:22:07,002][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 00:22:07,085][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 425 @ 12932 updates, score 31.415) (writing took 3.474311083999055 seconds)
[2024-09-18 00:22:07,085][fairseq_cli.train][INFO] - end of epoch 425 (average epoch stats below)
[2024-09-18 00:22:07,086][train][INFO] - epoch 425 | loss 298.671 | ntokens 10343.4 | nsentences 47.2727 | nll_loss 1.365 | wps 2620.8 | ups 0.25 | wpb 10343.4 | bsz 47.3 | num_updates 12932 | lr 4.15468e-05 | gnorm 289.006 | loss_scale 1 | train_wall 150 | gb_free 18.5 | wall 0
[2024-09-18 00:22:07,087][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:22:07,103][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 426
[2024-09-18 00:22:07,116][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 00:22:07,134][fairseq.trainer][INFO] - begin training epoch 426
[2024-09-18 00:22:07,134][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 00:24:37,587][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 00:24:37,587][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:24:37,602][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 69
[2024-09-18 00:24:57,940][valid][INFO] - epoch 426 | valid on 'valid' subset | loss 248.273 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.106 | uer 21.914 | wer 31.87 | raw_wer 31.87 | wps 4671.7 | wpb 1902.9 | bsz 8.5 | num_updates 12976 | best_wer 31.117
[2024-09-18 00:24:57,941][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 426 @ 12976 updates
[2024-09-18 00:24:57,941][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 00:25:01,287][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 00:25:01,346][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 426 @ 12976 updates, score 31.87) (writing took 3.405086073995335 seconds)
[2024-09-18 00:25:01,346][fairseq_cli.train][INFO] - end of epoch 426 (average epoch stats below)
[2024-09-18 00:25:01,348][train][INFO] - epoch 426 | loss 296.21 | ntokens 10344 | nsentences 47.2727 | nll_loss 1.354 | wps 2611.8 | ups 0.25 | wpb 10344 | bsz 47.3 | num_updates 12976 | lr 4.10028e-05 | gnorm 295.046 | loss_scale 1 | train_wall 150 | gb_free 17.2 | wall 0
[2024-09-18 00:25:01,348][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:25:01,364][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 427
[2024-09-18 00:25:01,376][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 00:25:01,394][fairseq.trainer][INFO] - begin training epoch 427
[2024-09-18 00:25:01,395][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 00:26:23,255][train_inner][INFO] - epoch 427:     24 / 44 loss=302.843, ntokens=10355.5, nsentences=46.92, nll_loss=1.372, wps=2575.9, ups=0.25, wpb=10355.5, bsz=46.9, num_updates=13000, lr=4.07091e-05, gnorm=304.333, loss_scale=1, train_wall=681, gb_free=18, wall=0
[2024-09-18 00:27:30,067][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 00:27:30,067][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:27:30,083][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 70
[2024-09-18 00:27:50,412][valid][INFO] - epoch 427 | valid on 'valid' subset | loss 245.482 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.094 | uer 21.11 | wer 30.638 | raw_wer 30.638 | wps 4677.7 | wpb 1902.9 | bsz 8.5 | num_updates 13020 | best_wer 30.638
[2024-09-18 00:27:50,413][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 427 @ 13020 updates
[2024-09-18 00:27:50,413][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 00:27:53,639][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 00:27:55,550][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 427 @ 13020 updates, score 30.638) (writing took 5.137419085003785 seconds)
[2024-09-18 00:27:55,551][fairseq_cli.train][INFO] - end of epoch 427 (average epoch stats below)
[2024-09-18 00:27:55,552][train][INFO] - epoch 427 | loss 303.565 | ntokens 10343.7 | nsentences 47.2727 | nll_loss 1.387 | wps 2612.6 | ups 0.25 | wpb 10343.7 | bsz 47.3 | num_updates 13020 | lr 4.04659e-05 | gnorm 327.829 | loss_scale 1 | train_wall 149 | gb_free 19.3 | wall 0
[2024-09-18 00:27:55,552][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:27:55,568][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 428
[2024-09-18 00:27:55,581][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 00:27:55,600][fairseq.trainer][INFO] - begin training epoch 428
[2024-09-18 00:27:55,600][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 00:30:26,520][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 00:30:26,521][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:30:26,536][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 71
[2024-09-18 00:30:46,872][valid][INFO] - epoch 428 | valid on 'valid' subset | loss 244.815 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.091 | uer 20.366 | wer 29.963 | raw_wer 29.963 | wps 4682.1 | wpb 1902.9 | bsz 8.5 | num_updates 13064 | best_wer 29.963
[2024-09-18 00:30:46,872][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 428 @ 13064 updates
[2024-09-18 00:30:46,873][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 00:30:50,095][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 00:30:51,952][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 428 @ 13064 updates, score 29.963) (writing took 5.079682640003739 seconds)
[2024-09-18 00:30:51,953][fairseq_cli.train][INFO] - end of epoch 428 (average epoch stats below)
[2024-09-18 00:30:51,954][train][INFO] - epoch 428 | loss 294.806 | ntokens 10343.4 | nsentences 47.2727 | nll_loss 1.347 | wps 2580 | ups 0.25 | wpb 10343.4 | bsz 47.3 | num_updates 13064 | lr 3.9936e-05 | gnorm 304.659 | loss_scale 1 | train_wall 151 | gb_free 18.3 | wall 0
[2024-09-18 00:30:51,955][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:30:51,970][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 429
[2024-09-18 00:30:51,982][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 00:30:52,000][fairseq.trainer][INFO] - begin training epoch 429
[2024-09-18 00:30:52,000][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 00:33:21,275][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 00:33:21,276][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:33:21,291][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 72
[2024-09-18 00:33:41,572][valid][INFO] - epoch 429 | valid on 'valid' subset | loss 244.166 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.088 | uer 20.881 | wer 30.154 | raw_wer 30.154 | wps 4688.9 | wpb 1902.9 | bsz 8.5 | num_updates 13108 | best_wer 29.963
[2024-09-18 00:33:41,573][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 429 @ 13108 updates
[2024-09-18 00:33:41,574][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 00:33:44,974][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 00:33:45,049][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 429 @ 13108 updates, score 30.154) (writing took 3.4755624440003885 seconds)
[2024-09-18 00:33:45,049][fairseq_cli.train][INFO] - end of epoch 429 (average epoch stats below)
[2024-09-18 00:33:45,050][train][INFO] - epoch 429 | loss 297.299 | ntokens 10344 | nsentences 47.2727 | nll_loss 1.359 | wps 2629.4 | ups 0.25 | wpb 10344 | bsz 47.3 | num_updates 13108 | lr 3.9413e-05 | gnorm 314.506 | loss_scale 1 | train_wall 149 | gb_free 17.5 | wall 0
[2024-09-18 00:33:45,051][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:33:45,067][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 430
[2024-09-18 00:33:45,084][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 00:33:45,102][fairseq.trainer][INFO] - begin training epoch 430
[2024-09-18 00:33:45,102][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 00:36:15,412][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 00:36:15,412][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:36:15,428][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 73
[2024-09-18 00:36:35,738][valid][INFO] - epoch 430 | valid on 'valid' subset | loss 242.497 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.081 | uer 21.599 | wer 31.684 | raw_wer 31.684 | wps 4694 | wpb 1902.9 | bsz 8.5 | num_updates 13152 | best_wer 29.963
[2024-09-18 00:36:35,739][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 430 @ 13152 updates
[2024-09-18 00:36:35,740][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 00:36:39,113][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 00:36:39,187][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 430 @ 13152 updates, score 31.684) (writing took 3.4483231240010355 seconds)
[2024-09-18 00:36:39,188][fairseq_cli.train][INFO] - end of epoch 430 (average epoch stats below)
[2024-09-18 00:36:39,189][train][INFO] - epoch 430 | loss 290.45 | ntokens 10343.6 | nsentences 47.2727 | nll_loss 1.327 | wps 2613.6 | ups 0.25 | wpb 10343.6 | bsz 47.3 | num_updates 13152 | lr 3.88969e-05 | gnorm 295.939 | loss_scale 1 | train_wall 150 | gb_free 21.3 | wall 0
[2024-09-18 00:36:39,190][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:36:39,205][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 431
[2024-09-18 00:36:39,223][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 00:36:39,241][fairseq.trainer][INFO] - begin training epoch 431
[2024-09-18 00:36:39,241][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 00:39:09,393][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 00:39:09,394][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:39:09,408][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 74
[2024-09-18 00:39:29,716][valid][INFO] - epoch 431 | valid on 'valid' subset | loss 240.292 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.071 | uer 20.588 | wer 30.246 | raw_wer 30.246 | wps 4679.3 | wpb 1902.9 | bsz 8.5 | num_updates 13196 | best_wer 29.963
[2024-09-18 00:39:29,716][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 431 @ 13196 updates
[2024-09-18 00:39:29,717][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 00:39:33,079][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 00:39:33,138][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 431 @ 13196 updates, score 30.246) (writing took 3.42164409399993 seconds)
[2024-09-18 00:39:33,139][fairseq_cli.train][INFO] - end of epoch 431 (average epoch stats below)
[2024-09-18 00:39:33,140][train][INFO] - epoch 431 | loss 293.545 | ntokens 10343.8 | nsentences 47.2727 | nll_loss 1.342 | wps 2616.4 | ups 0.25 | wpb 10343.8 | bsz 47.3 | num_updates 13196 | lr 3.83876e-05 | gnorm 291.274 | loss_scale 1 | train_wall 150 | gb_free 20.8 | wall 0
[2024-09-18 00:39:33,141][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:39:33,156][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 432
[2024-09-18 00:39:33,168][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 00:39:33,190][fairseq.trainer][INFO] - begin training epoch 432
[2024-09-18 00:39:33,190][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 00:39:46,865][train_inner][INFO] - epoch 432:      4 / 44 loss=293.351, ntokens=10347.3, nsentences=47.76, nll_loss=1.354, wps=2575.2, ups=0.25, wpb=10347.3, bsz=47.8, num_updates=13200, lr=3.83416e-05, gnorm=303.586, loss_scale=1, train_wall=680, gb_free=21, wall=0
[2024-09-18 00:42:02,464][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 00:42:02,465][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:42:02,480][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 75
[2024-09-18 00:42:22,804][valid][INFO] - epoch 432 | valid on 'valid' subset | loss 240.374 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.071 | uer 21.097 | wer 30.765 | raw_wer 30.765 | wps 4666.9 | wpb 1902.9 | bsz 8.5 | num_updates 13240 | best_wer 29.963
[2024-09-18 00:42:22,804][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 432 @ 13240 updates
[2024-09-18 00:42:22,805][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 00:42:26,162][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 00:42:26,221][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 432 @ 13240 updates, score 30.765) (writing took 3.4169338770007016 seconds)
[2024-09-18 00:42:26,222][fairseq_cli.train][INFO] - end of epoch 432 (average epoch stats below)
[2024-09-18 00:42:26,223][train][INFO] - epoch 432 | loss 295.801 | ntokens 10343.6 | nsentences 47.2727 | nll_loss 1.352 | wps 2629.5 | ups 0.25 | wpb 10343.6 | bsz 47.3 | num_updates 13240 | lr 3.78849e-05 | gnorm 307.64 | loss_scale 1 | train_wall 149 | gb_free 18.7 | wall 0
[2024-09-18 00:42:26,224][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:42:26,240][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 433
[2024-09-18 00:42:26,253][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 00:42:26,272][fairseq.trainer][INFO] - begin training epoch 433
[2024-09-18 00:42:26,272][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 00:44:56,672][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 00:44:56,673][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:44:56,688][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 76
[2024-09-18 00:45:16,838][valid][INFO] - epoch 433 | valid on 'valid' subset | loss 238.035 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.061 | uer 20.435 | wer 30.114 | raw_wer 30.114 | wps 4709.7 | wpb 1902.9 | bsz 8.5 | num_updates 13284 | best_wer 29.963
[2024-09-18 00:45:16,839][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 433 @ 13284 updates
[2024-09-18 00:45:16,839][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 00:45:20,189][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 00:45:20,248][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 433 @ 13284 updates, score 30.114) (writing took 3.409268613002496 seconds)
[2024-09-18 00:45:20,248][fairseq_cli.train][INFO] - end of epoch 433 (average epoch stats below)
[2024-09-18 00:45:20,249][train][INFO] - epoch 433 | loss 291.544 | ntokens 10344 | nsentences 47.2727 | nll_loss 1.332 | wps 2615.3 | ups 0.25 | wpb 10344 | bsz 47.3 | num_updates 13284 | lr 3.73888e-05 | gnorm 295.352 | loss_scale 1 | train_wall 150 | gb_free 18.5 | wall 0
[2024-09-18 00:45:20,250][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:45:20,265][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 434
[2024-09-18 00:45:20,278][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 00:45:20,297][fairseq.trainer][INFO] - begin training epoch 434
[2024-09-18 00:45:20,297][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 00:47:50,384][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 00:47:50,385][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:47:50,401][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 77
[2024-09-18 00:48:10,702][valid][INFO] - epoch 434 | valid on 'valid' subset | loss 239.729 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.068 | uer 20.207 | wer 29.528 | raw_wer 29.528 | wps 4694.9 | wpb 1902.9 | bsz 8.5 | num_updates 13328 | best_wer 29.528
[2024-09-18 00:48:10,703][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 434 @ 13328 updates
[2024-09-18 00:48:10,704][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 00:48:13,972][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 00:48:15,894][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 434 @ 13328 updates, score 29.528) (writing took 5.1905669499974465 seconds)
[2024-09-18 00:48:15,894][fairseq_cli.train][INFO] - end of epoch 434 (average epoch stats below)
[2024-09-18 00:48:15,896][train][INFO] - epoch 434 | loss 294.257 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 1.345 | wps 2591 | ups 0.25 | wpb 10343.2 | bsz 47.3 | num_updates 13328 | lr 3.68992e-05 | gnorm 300.383 | loss_scale 2 | train_wall 150 | gb_free 21.9 | wall 0
[2024-09-18 00:48:15,897][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:48:15,912][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 435
[2024-09-18 00:48:15,924][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 00:48:15,945][fairseq.trainer][INFO] - begin training epoch 435
[2024-09-18 00:48:15,946][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 00:50:45,040][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 00:50:45,041][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:50:45,056][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 78
[2024-09-18 00:51:05,435][valid][INFO] - epoch 435 | valid on 'valid' subset | loss 236.862 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.056 | uer 20.572 | wer 30.618 | raw_wer 30.618 | wps 4661.9 | wpb 1902.9 | bsz 8.5 | num_updates 13372 | best_wer 29.528
[2024-09-18 00:51:05,436][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 435 @ 13372 updates
[2024-09-18 00:51:05,436][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 00:51:08,768][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 00:51:08,829][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 435 @ 13372 updates, score 30.618) (writing took 3.3932530610036338 seconds)
[2024-09-18 00:51:08,830][fairseq_cli.train][INFO] - end of epoch 435 (average epoch stats below)
[2024-09-18 00:51:08,831][train][INFO] - epoch 435 | loss 286.686 | ntokens 10343.4 | nsentences 47.2727 | nll_loss 1.31 | wps 2631.7 | ups 0.25 | wpb 10343.4 | bsz 47.3 | num_updates 13372 | lr 3.6416e-05 | gnorm 293.628 | loss_scale 2 | train_wall 149 | gb_free 21.6 | wall 0
[2024-09-18 00:51:08,831][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:51:08,847][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 436
[2024-09-18 00:51:08,864][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 00:51:08,883][fairseq.trainer][INFO] - begin training epoch 436
[2024-09-18 00:51:08,883][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 00:52:44,056][train_inner][INFO] - epoch 436:     28 / 44 loss=293.353, ntokens=10326.4, nsentences=46.52, nll_loss=1.322, wps=2657.4, ups=0.26, wpb=10326.4, bsz=46.5, num_updates=13400, lr=3.61119e-05, gnorm=301.7, loss_scale=2, train_wall=680, gb_free=18.8, wall=0
[2024-09-18 00:53:38,978][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 00:53:38,979][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:53:38,994][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 79
[2024-09-18 00:53:59,270][valid][INFO] - epoch 436 | valid on 'valid' subset | loss 237.209 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.057 | uer 19.928 | wer 29.445 | raw_wer 29.445 | wps 4689.8 | wpb 1902.9 | bsz 8.5 | num_updates 13416 | best_wer 29.445
[2024-09-18 00:53:59,271][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 436 @ 13416 updates
[2024-09-18 00:53:59,272][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 00:54:02,555][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 00:54:04,450][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 436 @ 13416 updates, score 29.445) (writing took 5.178674072994909 seconds)
[2024-09-18 00:54:04,450][fairseq_cli.train][INFO] - end of epoch 436 (average epoch stats below)
[2024-09-18 00:54:04,451][train][INFO] - epoch 436 | loss 282.527 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 1.291 | wps 2591.4 | ups 0.25 | wpb 10343.2 | bsz 47.3 | num_updates 13416 | lr 3.59392e-05 | gnorm 289.015 | loss_scale 2 | train_wall 150 | gb_free 19.6 | wall 0
[2024-09-18 00:54:04,452][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:54:04,468][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 437
[2024-09-18 00:54:04,480][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 00:54:04,499][fairseq.trainer][INFO] - begin training epoch 437
[2024-09-18 00:54:04,499][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 00:56:34,334][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 00:56:34,334][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:56:34,349][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 80
[2024-09-18 00:56:54,401][valid][INFO] - epoch 437 | valid on 'valid' subset | loss 236.62 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.054 | uer 20.037 | wer 29.762 | raw_wer 29.762 | wps 4739.9 | wpb 1902.9 | bsz 8.5 | num_updates 13460 | best_wer 29.445
[2024-09-18 00:56:54,413][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 437 @ 13460 updates
[2024-09-18 00:56:54,414][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 00:56:57,770][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 00:56:57,840][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 437 @ 13460 updates, score 29.762) (writing took 3.4267307679983787 seconds)
[2024-09-18 00:56:57,840][fairseq_cli.train][INFO] - end of epoch 437 (average epoch stats below)
[2024-09-18 00:56:57,841][train][INFO] - epoch 437 | loss 288.147 | ntokens 10343.8 | nsentences 47.2727 | nll_loss 1.317 | wps 2624.9 | ups 0.25 | wpb 10343.8 | bsz 47.3 | num_updates 13460 | lr 3.54686e-05 | gnorm 293.695 | loss_scale 2 | train_wall 150 | gb_free 16.8 | wall 0
[2024-09-18 00:56:57,842][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:56:57,857][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 438
[2024-09-18 00:56:57,870][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 00:56:57,888][fairseq.trainer][INFO] - begin training epoch 438
[2024-09-18 00:56:57,888][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 00:59:28,485][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 00:59:28,486][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:59:28,501][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 81
[2024-09-18 00:59:48,878][valid][INFO] - epoch 438 | valid on 'valid' subset | loss 234.723 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.046 | uer 20.124 | wer 29.938 | raw_wer 29.938 | wps 4668.6 | wpb 1902.9 | bsz 8.5 | num_updates 13504 | best_wer 29.445
[2024-09-18 00:59:48,879][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 438 @ 13504 updates
[2024-09-18 00:59:48,880][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 00:59:52,246][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 00:59:52,312][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 438 @ 13504 updates, score 29.938) (writing took 3.433267791995604 seconds)
[2024-09-18 00:59:52,313][fairseq_cli.train][INFO] - end of epoch 438 (average epoch stats below)
[2024-09-18 00:59:52,314][train][INFO] - epoch 438 | loss 279.235 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 1.276 | wps 2608.4 | ups 0.25 | wpb 10343.2 | bsz 47.3 | num_updates 13504 | lr 3.50041e-05 | gnorm 293.635 | loss_scale 2 | train_wall 150 | gb_free 18.4 | wall 0
[2024-09-18 00:59:52,315][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 00:59:52,330][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 439
[2024-09-18 00:59:52,343][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 00:59:52,362][fairseq.trainer][INFO] - begin training epoch 439
[2024-09-18 00:59:52,362][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 01:02:22,084][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 01:02:22,085][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:02:22,100][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 82
[2024-09-18 01:02:42,256][valid][INFO] - epoch 439 | valid on 'valid' subset | loss 234.422 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.045 | uer 20.193 | wer 29.171 | raw_wer 29.171 | wps 4717.5 | wpb 1902.9 | bsz 8.5 | num_updates 13548 | best_wer 29.171
[2024-09-18 01:02:42,257][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 439 @ 13548 updates
[2024-09-18 01:02:42,257][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 01:02:45,458][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 01:02:47,376][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 439 @ 13548 updates, score 29.171) (writing took 5.118898171000183 seconds)
[2024-09-18 01:02:47,376][fairseq_cli.train][INFO] - end of epoch 439 (average epoch stats below)
[2024-09-18 01:02:47,377][train][INFO] - epoch 439 | loss 279.44 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 1.277 | wps 2599.6 | ups 0.25 | wpb 10343.2 | bsz 47.3 | num_updates 13548 | lr 3.45457e-05 | gnorm 282.636 | loss_scale 2 | train_wall 150 | gb_free 15.8 | wall 0
[2024-09-18 01:02:47,378][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:02:47,393][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 440
[2024-09-18 01:02:47,406][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 01:02:47,424][fairseq.trainer][INFO] - begin training epoch 440
[2024-09-18 01:02:47,425][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 01:05:17,439][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 01:05:17,439][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:05:17,455][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 83
[2024-09-18 01:05:37,761][valid][INFO] - epoch 440 | valid on 'valid' subset | loss 233.824 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.042 | uer 19.444 | wer 28.75 | raw_wer 28.75 | wps 4699.5 | wpb 1902.9 | bsz 8.5 | num_updates 13592 | best_wer 28.75
[2024-09-18 01:05:37,762][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 440 @ 13592 updates
[2024-09-18 01:05:37,762][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 01:05:41,041][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 01:05:42,861][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 440 @ 13592 updates, score 28.75) (writing took 5.099167779000709 seconds)
[2024-09-18 01:05:42,861][fairseq_cli.train][INFO] - end of epoch 440 (average epoch stats below)
[2024-09-18 01:05:42,862][train][INFO] - epoch 440 | loss 282.894 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 1.293 | wps 2593.4 | ups 0.25 | wpb 10343.2 | bsz 47.3 | num_updates 13592 | lr 3.40934e-05 | gnorm 300.853 | loss_scale 2 | train_wall 150 | gb_free 19.9 | wall 0
[2024-09-18 01:05:42,863][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:05:42,889][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 441
[2024-09-18 01:05:42,901][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 01:05:42,920][fairseq.trainer][INFO] - begin training epoch 441
[2024-09-18 01:05:42,921][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 01:06:10,176][train_inner][INFO] - epoch 441:      8 / 44 loss=279.291, ntokens=10351.7, nsentences=48.12, nll_loss=1.298, wps=2568.3, ups=0.25, wpb=10351.7, bsz=48.1, num_updates=13600, lr=3.40118e-05, gnorm=290.003, loss_scale=2, train_wall=682, gb_free=18.2, wall=0
[2024-09-18 01:08:12,348][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 01:08:12,349][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:08:12,364][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 84
[2024-09-18 01:08:32,723][valid][INFO] - epoch 441 | valid on 'valid' subset | loss 232.017 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.034 | uer 19.897 | wer 29 | raw_wer 29 | wps 4660.3 | wpb 1902.9 | bsz 8.5 | num_updates 13636 | best_wer 28.75
[2024-09-18 01:08:32,724][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 441 @ 13636 updates
[2024-09-18 01:08:32,725][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 01:08:36,023][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 01:08:36,096][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 441 @ 13636 updates, score 29.0) (writing took 3.3721941289986717 seconds)
[2024-09-18 01:08:36,097][fairseq_cli.train][INFO] - end of epoch 441 (average epoch stats below)
[2024-09-18 01:08:36,098][train][INFO] - epoch 441 | loss 285.492 | ntokens 10343.7 | nsentences 47.2727 | nll_loss 1.305 | wps 2627.2 | ups 0.25 | wpb 10343.7 | bsz 47.3 | num_updates 13636 | lr 3.36469e-05 | gnorm 313.352 | loss_scale 2 | train_wall 149 | gb_free 19 | wall 0
[2024-09-18 01:08:36,099][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:08:36,114][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 442
[2024-09-18 01:08:36,126][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 01:08:36,145][fairseq.trainer][INFO] - begin training epoch 442
[2024-09-18 01:08:36,146][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 01:11:06,615][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 01:11:06,616][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:11:06,631][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 85
[2024-09-18 01:11:26,938][valid][INFO] - epoch 442 | valid on 'valid' subset | loss 230.037 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.025 | uer 19.899 | wer 29.396 | raw_wer 29.396 | wps 4692.3 | wpb 1902.9 | bsz 8.5 | num_updates 13680 | best_wer 28.75
[2024-09-18 01:11:26,939][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 442 @ 13680 updates
[2024-09-18 01:11:26,940][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 01:11:30,187][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 01:11:30,248][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 442 @ 13680 updates, score 29.396) (writing took 3.308963091003534 seconds)
[2024-09-18 01:11:30,248][fairseq_cli.train][INFO] - end of epoch 442 (average epoch stats below)
[2024-09-18 01:11:30,250][train][INFO] - epoch 442 | loss 281.939 | ntokens 10343.6 | nsentences 47.2727 | nll_loss 1.289 | wps 2613.4 | ups 0.25 | wpb 10343.6 | bsz 47.3 | num_updates 13680 | lr 3.32063e-05 | gnorm 326.009 | loss_scale 2 | train_wall 150 | gb_free 23.4 | wall 0
[2024-09-18 01:11:30,251][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:11:30,266][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 443
[2024-09-18 01:11:30,280][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 01:11:30,299][fairseq.trainer][INFO] - begin training epoch 443
[2024-09-18 01:11:30,300][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 01:14:00,763][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 01:14:00,764][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:14:00,779][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 86
[2024-09-18 01:14:21,076][valid][INFO] - epoch 443 | valid on 'valid' subset | loss 229.86 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.024 | uer 19.648 | wer 28.833 | raw_wer 28.833 | wps 4698.8 | wpb 1902.9 | bsz 8.5 | num_updates 13724 | best_wer 28.75
[2024-09-18 01:14:21,076][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 443 @ 13724 updates
[2024-09-18 01:14:21,077][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 01:14:24,466][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 01:14:24,526][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 443 @ 13724 updates, score 28.833) (writing took 3.4500676989991916 seconds)
[2024-09-18 01:14:24,527][fairseq_cli.train][INFO] - end of epoch 443 (average epoch stats below)
[2024-09-18 01:14:24,528][train][INFO] - epoch 443 | loss 272.3 | ntokens 10343.4 | nsentences 47.2727 | nll_loss 1.245 | wps 2611.4 | ups 0.25 | wpb 10343.4 | bsz 47.3 | num_updates 13724 | lr 3.27715e-05 | gnorm 308.604 | loss_scale 2 | train_wall 150 | gb_free 16.8 | wall 0
[2024-09-18 01:14:24,529][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:14:24,544][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 444
[2024-09-18 01:14:24,556][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 01:14:24,576][fairseq.trainer][INFO] - begin training epoch 444
[2024-09-18 01:14:24,576][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 01:16:55,250][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 01:16:55,251][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:16:55,266][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 87
[2024-09-18 01:17:15,563][valid][INFO] - epoch 444 | valid on 'valid' subset | loss 230.261 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.026 | uer 20.366 | wer 29.699 | raw_wer 29.699 | wps 4699.7 | wpb 1902.9 | bsz 8.5 | num_updates 13768 | best_wer 28.75
[2024-09-18 01:17:15,564][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 444 @ 13768 updates
[2024-09-18 01:17:15,564][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 01:17:19,039][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 01:17:19,128][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 444 @ 13768 updates, score 29.699) (writing took 3.5643327189973206 seconds)
[2024-09-18 01:17:19,128][fairseq_cli.train][INFO] - end of epoch 444 (average epoch stats below)
[2024-09-18 01:17:19,129][train][INFO] - epoch 444 | loss 269.347 | ntokens 10343.8 | nsentences 47.2727 | nll_loss 1.231 | wps 2606.7 | ups 0.25 | wpb 10343.8 | bsz 47.3 | num_updates 13768 | lr 3.23424e-05 | gnorm 294.446 | loss_scale 2 | train_wall 151 | gb_free 19.7 | wall 0
[2024-09-18 01:17:19,130][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:17:19,148][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 445
[2024-09-18 01:17:19,168][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 01:17:19,190][fairseq.trainer][INFO] - begin training epoch 445
[2024-09-18 01:17:19,190][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 01:19:08,680][train_inner][INFO] - epoch 445:     32 / 44 loss=276.104, ntokens=10349.1, nsentences=47.2, nll_loss=1.259, wps=2658.7, ups=0.26, wpb=10349.1, bsz=47.2, num_updates=13800, lr=3.20338e-05, gnorm=303.291, loss_scale=2, train_wall=683, gb_free=17.7, wall=0
[2024-09-18 01:19:49,528][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 01:19:49,529][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:19:49,543][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 88
[2024-09-18 01:20:09,832][valid][INFO] - epoch 445 | valid on 'valid' subset | loss 227.591 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.014 | uer 19.731 | wer 28.828 | raw_wer 28.828 | wps 4692.6 | wpb 1902.9 | bsz 8.5 | num_updates 13812 | best_wer 28.75
[2024-09-18 01:20:09,833][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 445 @ 13812 updates
[2024-09-18 01:20:09,833][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 01:20:13,251][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 01:20:13,317][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 445 @ 13812 updates, score 28.828) (writing took 3.4844510260008974 seconds)
[2024-09-18 01:20:13,318][fairseq_cli.train][INFO] - end of epoch 445 (average epoch stats below)
[2024-09-18 01:20:13,319][train][INFO] - epoch 445 | loss 271.947 | ntokens 10343.3 | nsentences 47.2727 | nll_loss 1.243 | wps 2612.7 | ups 0.25 | wpb 10343.3 | bsz 47.3 | num_updates 13812 | lr 3.19189e-05 | gnorm 279.013 | loss_scale 2 | train_wall 150 | gb_free 20.6 | wall 0
[2024-09-18 01:20:13,319][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:20:13,335][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 446
[2024-09-18 01:20:13,347][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 01:20:13,367][fairseq.trainer][INFO] - begin training epoch 446
[2024-09-18 01:20:13,367][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 01:22:43,558][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 01:22:43,559][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:22:43,575][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 89
[2024-09-18 01:23:03,846][valid][INFO] - epoch 446 | valid on 'valid' subset | loss 225.622 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.005 | uer 19.233 | wer 28.002 | raw_wer 28.002 | wps 4686.9 | wpb 1902.9 | bsz 8.5 | num_updates 13856 | best_wer 28.002
[2024-09-18 01:23:03,846][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 446 @ 13856 updates
[2024-09-18 01:23:03,847][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 01:23:07,086][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 01:23:09,109][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 446 @ 13856 updates, score 28.002) (writing took 5.262232553999638 seconds)
[2024-09-18 01:23:09,109][fairseq_cli.train][INFO] - end of epoch 446 (average epoch stats below)
[2024-09-18 01:23:09,110][train][INFO] - epoch 446 | loss 273.369 | ntokens 10344 | nsentences 47.2727 | nll_loss 1.249 | wps 2589.1 | ups 0.25 | wpb 10344 | bsz 47.3 | num_updates 13856 | lr 3.15009e-05 | gnorm 269.944 | loss_scale 2 | train_wall 150 | gb_free 16.4 | wall 0
[2024-09-18 01:23:09,111][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:23:09,128][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 447
[2024-09-18 01:23:09,140][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 01:23:09,160][fairseq.trainer][INFO] - begin training epoch 447
[2024-09-18 01:23:09,160][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 01:25:39,879][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 01:25:39,880][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:25:39,895][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 90
[2024-09-18 01:26:00,063][valid][INFO] - epoch 447 | valid on 'valid' subset | loss 225.859 | ntokens 1902.88 | nsentences 8.48 | nll_loss 1.007 | uer 19.432 | wer 28.726 | raw_wer 28.726 | wps 4709.1 | wpb 1902.9 | bsz 8.5 | num_updates 13900 | best_wer 28.002
[2024-09-18 01:26:00,064][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 447 @ 13900 updates
[2024-09-18 01:26:00,064][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 01:26:03,418][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 01:26:03,479][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 447 @ 13900 updates, score 28.726) (writing took 3.4154110379968188 seconds)
[2024-09-18 01:26:03,480][fairseq_cli.train][INFO] - end of epoch 447 (average epoch stats below)
[2024-09-18 01:26:03,481][train][INFO] - epoch 447 | loss 272.736 | ntokens 10343.8 | nsentences 47.2727 | nll_loss 1.246 | wps 2610.1 | ups 0.25 | wpb 10343.8 | bsz 47.3 | num_updates 13900 | lr 3.10884e-05 | gnorm 295.043 | loss_scale 2 | train_wall 151 | gb_free 18.4 | wall 0
[2024-09-18 01:26:03,482][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:26:03,498][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 448
[2024-09-18 01:26:03,510][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 01:26:03,530][fairseq.trainer][INFO] - begin training epoch 448
[2024-09-18 01:26:03,530][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 01:28:34,405][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 01:28:34,406][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:28:34,421][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 91
[2024-09-18 01:28:54,836][valid][INFO] - epoch 448 | valid on 'valid' subset | loss 224.249 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.999 | uer 18.971 | wer 27.875 | raw_wer 27.875 | wps 4659.7 | wpb 1902.9 | bsz 8.5 | num_updates 13944 | best_wer 27.875
[2024-09-18 01:28:54,837][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 448 @ 13944 updates
[2024-09-18 01:28:54,837][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 01:28:58,105][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 01:28:59,916][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 448 @ 13944 updates, score 27.875) (writing took 5.0788255159932305 seconds)
[2024-09-18 01:28:59,916][fairseq_cli.train][INFO] - end of epoch 448 (average epoch stats below)
[2024-09-18 01:28:59,917][train][INFO] - epoch 448 | loss 272.77 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.247 | wps 2579.5 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 13944 | lr 3.06813e-05 | gnorm 284.215 | loss_scale 2 | train_wall 151 | gb_free 16.4 | wall 0
[2024-09-18 01:28:59,918][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:28:59,934][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 449
[2024-09-18 01:28:59,946][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 01:28:59,966][fairseq.trainer][INFO] - begin training epoch 449
[2024-09-18 01:28:59,966][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 01:31:29,783][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 01:31:29,784][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:31:29,799][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 92
[2024-09-18 01:31:50,140][valid][INFO] - epoch 449 | valid on 'valid' subset | loss 223.996 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.998 | uer 18.948 | wer 28.056 | raw_wer 28.056 | wps 4678.2 | wpb 1902.9 | bsz 8.5 | num_updates 13988 | best_wer 27.875
[2024-09-18 01:31:50,140][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 449 @ 13988 updates
[2024-09-18 01:31:50,141][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 01:31:53,527][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 01:31:53,599][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 449 @ 13988 updates, score 28.056) (writing took 3.458473870996386 seconds)
[2024-09-18 01:31:53,599][fairseq_cli.train][INFO] - end of epoch 449 (average epoch stats below)
[2024-09-18 01:31:53,600][train][INFO] - epoch 449 | loss 273.965 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 1.252 | wps 2620.3 | ups 0.25 | wpb 10343.2 | bsz 47.3 | num_updates 13988 | lr 3.02795e-05 | gnorm 275.843 | loss_scale 2 | train_wall 150 | gb_free 19.6 | wall 0
[2024-09-18 01:31:53,601][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:31:53,617][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 450
[2024-09-18 01:31:53,638][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 01:31:53,660][fairseq.trainer][INFO] - begin training epoch 450
[2024-09-18 01:31:53,660][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 01:32:34,617][train_inner][INFO] - epoch 450:     12 / 44 loss=271.943, ntokens=10321, nsentences=47.52, nll_loss=1.252, wps=2561.3, ups=0.25, wpb=10321, bsz=47.5, num_updates=14000, lr=3.01709e-05, gnorm=285.531, loss_scale=2, train_wall=683, gb_free=18, wall=0
[2024-09-18 01:34:23,768][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 01:34:23,768][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:34:23,784][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 93
[2024-09-18 01:34:44,208][valid][INFO] - epoch 450 | valid on 'valid' subset | loss 222.194 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.99 | uer 19.401 | wer 28.423 | raw_wer 28.423 | wps 4671.7 | wpb 1902.9 | bsz 8.5 | num_updates 14032 | best_wer 27.875
[2024-09-18 01:34:44,209][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 450 @ 14032 updates
[2024-09-18 01:34:44,209][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 01:34:47,660][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 01:34:47,730][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 450 @ 14032 updates, score 28.423) (writing took 3.5215127430055873 seconds)
[2024-09-18 01:34:47,731][fairseq_cli.train][INFO] - end of epoch 450 (average epoch stats below)
[2024-09-18 01:34:47,732][train][INFO] - epoch 450 | loss 278.011 | ntokens 10343 | nsentences 47.2727 | nll_loss 1.271 | wps 2613.5 | ups 0.25 | wpb 10343 | bsz 47.3 | num_updates 14032 | lr 2.9883e-05 | gnorm 305.907 | loss_scale 2 | train_wall 150 | gb_free 20.5 | wall 0
[2024-09-18 01:34:47,732][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:34:47,748][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 451
[2024-09-18 01:34:47,763][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 01:34:47,785][fairseq.trainer][INFO] - begin training epoch 451
[2024-09-18 01:34:47,785][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 01:37:18,097][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 01:37:18,097][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:37:18,112][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 94
[2024-09-18 01:37:38,475][valid][INFO] - epoch 451 | valid on 'valid' subset | loss 222.452 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.991 | uer 19.628 | wer 28.594 | raw_wer 28.594 | wps 4658 | wpb 1902.9 | bsz 8.5 | num_updates 14076 | best_wer 27.875
[2024-09-18 01:37:38,476][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 451 @ 14076 updates
[2024-09-18 01:37:38,476][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 01:37:41,860][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 01:37:41,924][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 451 @ 14076 updates, score 28.594) (writing took 3.44836107899755 seconds)
[2024-09-18 01:37:41,925][fairseq_cli.train][INFO] - end of epoch 451 (average epoch stats below)
[2024-09-18 01:37:41,926][train][INFO] - epoch 451 | loss 273.244 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 1.249 | wps 2612.6 | ups 0.25 | wpb 10343.2 | bsz 47.3 | num_updates 14076 | lr 2.94917e-05 | gnorm 283.119 | loss_scale 2 | train_wall 150 | gb_free 24.1 | wall 0
[2024-09-18 01:37:41,926][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:37:41,942][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 452
[2024-09-18 01:37:41,954][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 01:37:41,978][fairseq.trainer][INFO] - begin training epoch 452
[2024-09-18 01:37:41,979][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 01:40:12,030][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 01:40:12,031][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:40:12,047][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 95
[2024-09-18 01:40:32,374][valid][INFO] - epoch 452 | valid on 'valid' subset | loss 220.76 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.984 | uer 19.046 | wer 28.031 | raw_wer 28.031 | wps 4683.1 | wpb 1902.9 | bsz 8.5 | num_updates 14120 | best_wer 27.875
[2024-09-18 01:40:32,375][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 452 @ 14120 updates
[2024-09-18 01:40:32,375][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 01:40:35,787][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 01:40:35,850][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 452 @ 14120 updates, score 28.031) (writing took 3.475093854001898 seconds)
[2024-09-18 01:40:35,850][fairseq_cli.train][INFO] - end of epoch 452 (average epoch stats below)
[2024-09-18 01:40:35,852][train][INFO] - epoch 452 | loss 277.051 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.266 | wps 2616.7 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 14120 | lr 2.91055e-05 | gnorm 287.763 | loss_scale 2 | train_wall 150 | gb_free 21.9 | wall 0
[2024-09-18 01:40:35,852][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:40:35,868][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 453
[2024-09-18 01:40:35,890][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 01:40:35,922][fairseq.trainer][INFO] - begin training epoch 453
[2024-09-18 01:40:35,922][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 01:43:06,577][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 01:43:06,578][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:43:06,593][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 96
[2024-09-18 01:43:26,879][valid][INFO] - epoch 453 | valid on 'valid' subset | loss 222.081 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.99 | uer 18.911 | wer 27.973 | raw_wer 27.973 | wps 4681.5 | wpb 1902.9 | bsz 8.5 | num_updates 14164 | best_wer 27.875
[2024-09-18 01:43:26,880][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 453 @ 14164 updates
[2024-09-18 01:43:26,880][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 01:43:30,277][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 01:43:30,343][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 453 @ 14164 updates, score 27.973) (writing took 3.4632446660034475 seconds)
[2024-09-18 01:43:30,343][fairseq_cli.train][INFO] - end of epoch 453 (average epoch stats below)
[2024-09-18 01:43:30,345][train][INFO] - epoch 453 | loss 267.037 | ntokens 10343 | nsentences 47.2727 | nll_loss 1.22 | wps 2608.1 | ups 0.25 | wpb 10343 | bsz 47.3 | num_updates 14164 | lr 2.87244e-05 | gnorm 278.688 | loss_scale 2 | train_wall 150 | gb_free 18.8 | wall 0
[2024-09-18 01:43:30,346][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:43:30,365][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 454
[2024-09-18 01:43:30,383][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 01:43:30,412][fairseq.trainer][INFO] - begin training epoch 454
[2024-09-18 01:43:30,412][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 01:45:29,572][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2024-09-18 01:45:36,529][train_inner][INFO] - epoch 454:     37 / 44 loss=273.834, ntokens=10358.1, nsentences=47.04, nll_loss=1.244, wps=2649.4, ups=0.26, wpb=10358.1, bsz=47, num_updates=14200, lr=2.84163e-05, gnorm=280.318, loss_scale=1, train_wall=686, gb_free=19.1, wall=0
[2024-09-18 01:45:59,836][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 01:45:59,837][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:45:59,857][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 97
[2024-09-18 01:46:20,096][valid][INFO] - epoch 454 | valid on 'valid' subset | loss 220.17 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.981 | uer 18.939 | wer 27.87 | raw_wer 27.87 | wps 4700.5 | wpb 1902.9 | bsz 8.5 | num_updates 14207 | best_wer 27.87
[2024-09-18 01:46:20,096][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 454 @ 14207 updates
[2024-09-18 01:46:20,097][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 01:46:23,396][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 01:46:25,381][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 454 @ 14207 updates, score 27.87) (writing took 5.285015174005821 seconds)
[2024-09-18 01:46:25,382][fairseq_cli.train][INFO] - end of epoch 454 (average epoch stats below)
[2024-09-18 01:46:25,383][train][INFO] - epoch 454 | loss 267.645 | ntokens 10363.4 | nsentences 47.4419 | nll_loss 1.225 | wps 2545.9 | ups 0.25 | wpb 10363.4 | bsz 47.4 | num_updates 14207 | lr 2.83568e-05 | gnorm 253.989 | loss_scale 1 | train_wall 149 | gb_free 16.7 | wall 0
[2024-09-18 01:46:25,384][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:46:25,400][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 455
[2024-09-18 01:46:25,412][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 01:46:25,434][fairseq.trainer][INFO] - begin training epoch 455
[2024-09-18 01:46:25,434][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 01:48:54,962][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 01:48:54,963][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:48:54,978][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 98
[2024-09-18 01:49:15,351][valid][INFO] - epoch 455 | valid on 'valid' subset | loss 219.797 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.98 | uer 18.631 | wer 27.582 | raw_wer 27.582 | wps 4654.3 | wpb 1902.9 | bsz 8.5 | num_updates 14251 | best_wer 27.582
[2024-09-18 01:49:15,351][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 455 @ 14251 updates
[2024-09-18 01:49:15,352][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 01:49:18,622][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 01:49:20,473][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 455 @ 14251 updates, score 27.582) (writing took 5.121268470000359 seconds)
[2024-09-18 01:49:20,473][fairseq_cli.train][INFO] - end of epoch 455 (average epoch stats below)
[2024-09-18 01:49:20,474][train][INFO] - epoch 455 | loss 266.71 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 1.219 | wps 2599.2 | ups 0.25 | wpb 10343.2 | bsz 47.3 | num_updates 14251 | lr 2.79854e-05 | gnorm 274.432 | loss_scale 1 | train_wall 149 | gb_free 22.5 | wall 0
[2024-09-18 01:49:20,475][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:49:20,491][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 456
[2024-09-18 01:49:20,504][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 01:49:20,526][fairseq.trainer][INFO] - begin training epoch 456
[2024-09-18 01:49:20,526][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 01:51:50,132][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 01:51:50,133][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:51:50,149][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 99
[2024-09-18 01:52:10,476][valid][INFO] - epoch 456 | valid on 'valid' subset | loss 218.759 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.975 | uer 18.622 | wer 27.42 | raw_wer 27.42 | wps 4675.8 | wpb 1902.9 | bsz 8.5 | num_updates 14295 | best_wer 27.42
[2024-09-18 01:52:10,476][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 456 @ 14295 updates
[2024-09-18 01:52:10,477][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 01:52:13,785][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 01:52:15,691][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 456 @ 14295 updates, score 27.42) (writing took 5.214505090996681 seconds)
[2024-09-18 01:52:15,691][fairseq_cli.train][INFO] - end of epoch 456 (average epoch stats below)
[2024-09-18 01:52:15,692][train][INFO] - epoch 456 | loss 270.231 | ntokens 10343.8 | nsentences 47.2727 | nll_loss 1.235 | wps 2597.5 | ups 0.25 | wpb 10343.8 | bsz 47.3 | num_updates 14295 | lr 2.7619e-05 | gnorm 300.983 | loss_scale 1 | train_wall 149 | gb_free 15.7 | wall 0
[2024-09-18 01:52:15,693][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:52:15,709][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 457
[2024-09-18 01:52:15,721][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 01:52:15,742][fairseq.trainer][INFO] - begin training epoch 457
[2024-09-18 01:52:15,742][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 01:54:45,818][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 01:54:45,819][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:54:45,834][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 100
[2024-09-18 01:55:06,079][valid][INFO] - epoch 457 | valid on 'valid' subset | loss 219.087 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.976 | uer 18.81 | wer 27.342 | raw_wer 27.342 | wps 4706.3 | wpb 1902.9 | bsz 8.5 | num_updates 14339 | best_wer 27.342
[2024-09-18 01:55:06,080][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 457 @ 14339 updates
[2024-09-18 01:55:06,081][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 01:55:09,348][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 01:55:11,227][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 457 @ 14339 updates, score 27.342) (writing took 5.147237427001528 seconds)
[2024-09-18 01:55:11,228][fairseq_cli.train][INFO] - end of epoch 457 (average epoch stats below)
[2024-09-18 01:55:11,229][train][INFO] - epoch 457 | loss 262.854 | ntokens 10343 | nsentences 47.2727 | nll_loss 1.201 | wps 2592.6 | ups 0.25 | wpb 10343 | bsz 47.3 | num_updates 14339 | lr 2.72573e-05 | gnorm 282.628 | loss_scale 1 | train_wall 150 | gb_free 18.4 | wall 0
[2024-09-18 01:55:11,230][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:55:11,255][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 458
[2024-09-18 01:55:11,267][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 01:55:11,289][fairseq.trainer][INFO] - begin training epoch 458
[2024-09-18 01:55:11,289][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 01:57:40,794][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 01:57:40,794][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:57:40,809][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 101
[2024-09-18 01:58:01,146][valid][INFO] - epoch 458 | valid on 'valid' subset | loss 218.19 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.972 | uer 18.615 | wer 27.283 | raw_wer 27.283 | wps 4671.1 | wpb 1902.9 | bsz 8.5 | num_updates 14383 | best_wer 27.283
[2024-09-18 01:58:01,147][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 458 @ 14383 updates
[2024-09-18 01:58:01,148][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 01:58:04,451][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 01:58:06,347][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 458 @ 14383 updates, score 27.283) (writing took 5.2001262699996005 seconds)
[2024-09-18 01:58:06,348][fairseq_cli.train][INFO] - end of epoch 458 (average epoch stats below)
[2024-09-18 01:58:06,349][train][INFO] - epoch 458 | loss 268.048 | ntokens 10344 | nsentences 47.2727 | nll_loss 1.225 | wps 2599 | ups 0.25 | wpb 10344 | bsz 47.3 | num_updates 14383 | lr 2.69004e-05 | gnorm 281.015 | loss_scale 1 | train_wall 149 | gb_free 21.2 | wall 0
[2024-09-18 01:58:06,350][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 01:58:06,366][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 459
[2024-09-18 01:58:06,381][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 01:58:06,403][fairseq.trainer][INFO] - begin training epoch 459
[2024-09-18 01:58:06,403][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 01:59:06,437][train_inner][INFO] - epoch 459:     17 / 44 loss=269.692, ntokens=10339.6, nsentences=46.64, nll_loss=1.217, wps=2553.3, ups=0.25, wpb=10339.6, bsz=46.6, num_updates=14400, lr=2.67637e-05, gnorm=286.075, loss_scale=1, train_wall=681, gb_free=16.4, wall=0
[2024-09-18 02:00:37,157][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 02:00:37,157][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:00:37,173][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 102
[2024-09-18 02:00:57,419][valid][INFO] - epoch 459 | valid on 'valid' subset | loss 218.524 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.974 | uer 18.713 | wer 27.117 | raw_wer 27.117 | wps 4685.8 | wpb 1902.9 | bsz 8.5 | num_updates 14427 | best_wer 27.117
[2024-09-18 02:00:57,419][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 459 @ 14427 updates
[2024-09-18 02:00:57,420][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 02:01:00,686][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 02:01:02,586][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 459 @ 14427 updates, score 27.117) (writing took 5.1668815040029585 seconds)
[2024-09-18 02:01:02,587][fairseq_cli.train][INFO] - end of epoch 459 (average epoch stats below)
[2024-09-18 02:01:02,588][train][INFO] - epoch 459 | loss 260.591 | ntokens 10343.3 | nsentences 47.2727 | nll_loss 1.191 | wps 2582.3 | ups 0.25 | wpb 10343.3 | bsz 47.3 | num_updates 14427 | lr 2.65481e-05 | gnorm 298.808 | loss_scale 1 | train_wall 151 | gb_free 16.2 | wall 0
[2024-09-18 02:01:02,589][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:01:02,605][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 460
[2024-09-18 02:01:02,618][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 02:01:02,639][fairseq.trainer][INFO] - begin training epoch 460
[2024-09-18 02:01:02,640][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 02:03:31,203][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 02:03:31,203][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:03:31,219][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 103
[2024-09-18 02:03:51,569][valid][INFO] - epoch 460 | valid on 'valid' subset | loss 216.96 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.967 | uer 18.869 | wer 27.719 | raw_wer 27.719 | wps 4658.2 | wpb 1902.9 | bsz 8.5 | num_updates 14471 | best_wer 27.117
[2024-09-18 02:03:51,569][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 460 @ 14471 updates
[2024-09-18 02:03:51,570][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 02:03:55,040][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 02:03:55,102][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 460 @ 14471 updates, score 27.719) (writing took 3.5329107900033705 seconds)
[2024-09-18 02:03:55,103][fairseq_cli.train][INFO] - end of epoch 460 (average epoch stats below)
[2024-09-18 02:03:55,104][train][INFO] - epoch 460 | loss 270.331 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 1.236 | wps 2638 | ups 0.26 | wpb 10343.2 | bsz 47.3 | num_updates 14471 | lr 2.62005e-05 | gnorm 286.967 | loss_scale 1 | train_wall 148 | gb_free 16.1 | wall 0
[2024-09-18 02:03:55,105][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:03:55,120][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 461
[2024-09-18 02:03:55,132][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 02:03:55,154][fairseq.trainer][INFO] - begin training epoch 461
[2024-09-18 02:03:55,155][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 02:06:25,786][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 02:06:25,786][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:06:25,802][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 104
[2024-09-18 02:06:46,195][valid][INFO] - epoch 461 | valid on 'valid' subset | loss 217.614 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.97 | uer 18.402 | wer 26.931 | raw_wer 26.931 | wps 4660.3 | wpb 1902.9 | bsz 8.5 | num_updates 14515 | best_wer 26.931
[2024-09-18 02:06:46,207][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 461 @ 14515 updates
[2024-09-18 02:06:46,208][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 02:06:49,472][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 02:06:51,427][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 461 @ 14515 updates, score 26.931) (writing took 5.219501959996705 seconds)
[2024-09-18 02:06:51,427][fairseq_cli.train][INFO] - end of epoch 461 (average epoch stats below)
[2024-09-18 02:06:51,428][train][INFO] - epoch 461 | loss 258.144 | ntokens 10343.3 | nsentences 47.2727 | nll_loss 1.18 | wps 2581.1 | ups 0.25 | wpb 10343.3 | bsz 47.3 | num_updates 14515 | lr 2.58574e-05 | gnorm 283.528 | loss_scale 1 | train_wall 150 | gb_free 18 | wall 0
[2024-09-18 02:06:51,429][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:06:51,445][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 462
[2024-09-18 02:06:51,457][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 02:06:51,480][fairseq.trainer][INFO] - begin training epoch 462
[2024-09-18 02:06:51,480][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 02:09:22,194][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 02:09:22,195][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:09:22,210][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 105
[2024-09-18 02:09:42,575][valid][INFO] - epoch 462 | valid on 'valid' subset | loss 214.83 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.957 | uer 18.938 | wer 27.464 | raw_wer 27.464 | wps 4666 | wpb 1902.9 | bsz 8.5 | num_updates 14559 | best_wer 26.931
[2024-09-18 02:09:42,575][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 462 @ 14559 updates
[2024-09-18 02:09:42,576][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 02:09:45,975][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 02:09:46,035][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 462 @ 14559 updates, score 27.464) (writing took 3.4593038730017724 seconds)
[2024-09-18 02:09:46,035][fairseq_cli.train][INFO] - end of epoch 462 (average epoch stats below)
[2024-09-18 02:09:46,036][train][INFO] - epoch 462 | loss 257.837 | ntokens 10343.8 | nsentences 47.2727 | nll_loss 1.178 | wps 2606.6 | ups 0.25 | wpb 10343.8 | bsz 47.3 | num_updates 14559 | lr 2.55188e-05 | gnorm 287.805 | loss_scale 1 | train_wall 151 | gb_free 18.6 | wall 0
[2024-09-18 02:09:46,037][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:09:46,053][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 463
[2024-09-18 02:09:46,073][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 02:09:46,096][fairseq.trainer][INFO] - begin training epoch 463
[2024-09-18 02:09:46,096][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 02:12:06,516][train_inner][INFO] - epoch 463:     41 / 44 loss=260.361, ntokens=10361.1, nsentences=47.88, nll_loss=1.203, wps=2656.4, ups=0.26, wpb=10361.1, bsz=47.9, num_updates=14600, lr=2.52073e-05, gnorm=286.677, loss_scale=1, train_wall=680, gb_free=23, wall=0
[2024-09-18 02:12:15,634][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 02:12:15,635][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:12:15,661][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 106
[2024-09-18 02:12:36,083][valid][INFO] - epoch 463 | valid on 'valid' subset | loss 216.113 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.963 | uer 18.842 | wer 27.499 | raw_wer 27.499 | wps 4667.8 | wpb 1902.9 | bsz 8.5 | num_updates 14603 | best_wer 26.931
[2024-09-18 02:12:36,083][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 463 @ 14603 updates
[2024-09-18 02:12:36,084][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 02:12:39,484][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 02:12:39,548][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 463 @ 14603 updates, score 27.499) (writing took 3.4641570640014834 seconds)
[2024-09-18 02:12:39,548][fairseq_cli.train][INFO] - end of epoch 463 (average epoch stats below)
[2024-09-18 02:12:39,549][train][INFO] - epoch 463 | loss 264.748 | ntokens 10344 | nsentences 47.2727 | nll_loss 1.21 | wps 2623.1 | ups 0.25 | wpb 10344 | bsz 47.3 | num_updates 14603 | lr 2.51846e-05 | gnorm 284.402 | loss_scale 1 | train_wall 149 | gb_free 17.4 | wall 0
[2024-09-18 02:12:39,550][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:12:39,566][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 464
[2024-09-18 02:12:39,578][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 02:12:39,605][fairseq.trainer][INFO] - begin training epoch 464
[2024-09-18 02:12:39,605][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 02:15:09,195][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 02:15:09,196][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:15:09,210][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 107
[2024-09-18 02:15:29,416][valid][INFO] - epoch 464 | valid on 'valid' subset | loss 215.46 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.96 | uer 18.45 | wer 26.755 | raw_wer 26.755 | wps 4706.4 | wpb 1902.9 | bsz 8.5 | num_updates 14647 | best_wer 26.755
[2024-09-18 02:15:29,417][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 464 @ 14647 updates
[2024-09-18 02:15:29,418][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 02:15:32,674][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 02:15:34,652][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 464 @ 14647 updates, score 26.755) (writing took 5.235444935002306 seconds)
[2024-09-18 02:15:34,653][fairseq_cli.train][INFO] - end of epoch 464 (average epoch stats below)
[2024-09-18 02:15:34,654][train][INFO] - epoch 464 | loss 266.43 | ntokens 10343.6 | nsentences 47.2727 | nll_loss 1.218 | wps 2599.1 | ups 0.25 | wpb 10343.6 | bsz 47.3 | num_updates 14647 | lr 2.48549e-05 | gnorm 282.435 | loss_scale 1 | train_wall 149 | gb_free 18.6 | wall 0
[2024-09-18 02:15:34,655][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:15:34,671][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 465
[2024-09-18 02:15:34,684][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 02:15:34,706][fairseq.trainer][INFO] - begin training epoch 465
[2024-09-18 02:15:34,706][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 02:18:04,665][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 02:18:04,666][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:18:04,681][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 108
[2024-09-18 02:18:25,048][valid][INFO] - epoch 465 | valid on 'valid' subset | loss 215.222 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.959 | uer 18.496 | wer 27.088 | raw_wer 27.088 | wps 4677.3 | wpb 1902.9 | bsz 8.5 | num_updates 14691 | best_wer 26.755
[2024-09-18 02:18:25,049][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 465 @ 14691 updates
[2024-09-18 02:18:25,049][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 02:18:28,462][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 02:18:28,518][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 465 @ 14691 updates, score 27.088) (writing took 3.4686737660013023 seconds)
[2024-09-18 02:18:28,518][fairseq_cli.train][INFO] - end of epoch 465 (average epoch stats below)
[2024-09-18 02:18:28,519][train][INFO] - epoch 465 | loss 266.65 | ntokens 10343.3 | nsentences 47.2727 | nll_loss 1.219 | wps 2617.6 | ups 0.25 | wpb 10343.3 | bsz 47.3 | num_updates 14691 | lr 2.45294e-05 | gnorm 284.463 | loss_scale 1 | train_wall 150 | gb_free 19.3 | wall 0
[2024-09-18 02:18:28,520][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:18:28,536][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 466
[2024-09-18 02:18:28,548][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 02:18:28,571][fairseq.trainer][INFO] - begin training epoch 466
[2024-09-18 02:18:28,571][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 02:20:58,105][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 02:20:58,106][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:20:58,120][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 109
[2024-09-18 02:21:18,401][valid][INFO] - epoch 466 | valid on 'valid' subset | loss 213.839 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.953 | uer 18.457 | wer 26.97 | raw_wer 26.97 | wps 4672.6 | wpb 1902.9 | bsz 8.5 | num_updates 14735 | best_wer 26.755
[2024-09-18 02:21:18,402][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 466 @ 14735 updates
[2024-09-18 02:21:18,402][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 02:21:21,821][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 02:21:21,882][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 466 @ 14735 updates, score 26.97) (writing took 3.4798983159998897 seconds)
[2024-09-18 02:21:21,882][fairseq_cli.train][INFO] - end of epoch 466 (average epoch stats below)
[2024-09-18 02:21:21,883][train][INFO] - epoch 466 | loss 267.29 | ntokens 10343.3 | nsentences 47.2727 | nll_loss 1.222 | wps 2625.2 | ups 0.25 | wpb 10343.3 | bsz 47.3 | num_updates 14735 | lr 2.42082e-05 | gnorm 308.279 | loss_scale 1 | train_wall 149 | gb_free 17.9 | wall 0
[2024-09-18 02:21:21,884][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:21:21,899][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 467
[2024-09-18 02:21:21,912][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 02:21:21,935][fairseq.trainer][INFO] - begin training epoch 467
[2024-09-18 02:21:21,936][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 02:23:52,308][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 02:23:52,308][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:23:52,324][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 110
[2024-09-18 02:24:12,592][valid][INFO] - epoch 467 | valid on 'valid' subset | loss 214.758 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.957 | uer 18.143 | wer 26.731 | raw_wer 26.731 | wps 4690.1 | wpb 1902.9 | bsz 8.5 | num_updates 14779 | best_wer 26.731
[2024-09-18 02:24:12,592][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 467 @ 14779 updates
[2024-09-18 02:24:12,593][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 02:24:15,853][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 02:24:17,767][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 467 @ 14779 updates, score 26.731) (writing took 5.174607432993071 seconds)
[2024-09-18 02:24:17,767][fairseq_cli.train][INFO] - end of epoch 467 (average epoch stats below)
[2024-09-18 02:24:17,768][train][INFO] - epoch 467 | loss 260.613 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.191 | wps 2587.6 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 14779 | lr 2.38912e-05 | gnorm 290.977 | loss_scale 1 | train_wall 150 | gb_free 24.5 | wall 0
[2024-09-18 02:24:17,769][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:24:17,785][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 468
[2024-09-18 02:24:17,798][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 02:24:17,821][fairseq.trainer][INFO] - begin training epoch 468
[2024-09-18 02:24:17,821][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 02:25:29,347][train_inner][INFO] - epoch 468:     21 / 44 loss=263.437, ntokens=10335, nsentences=47.44, nll_loss=1.209, wps=2574.7, ups=0.25, wpb=10335, bsz=47.4, num_updates=14800, lr=2.37414e-05, gnorm=289.619, loss_scale=1, train_wall=679, gb_free=17.2, wall=0
[2024-09-18 02:26:48,239][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 02:26:48,240][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:26:48,255][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 111
[2024-09-18 02:27:08,525][valid][INFO] - epoch 468 | valid on 'valid' subset | loss 214.079 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.954 | uer 18.01 | wer 26.941 | raw_wer 26.941 | wps 4689.7 | wpb 1902.9 | bsz 8.5 | num_updates 14823 | best_wer 26.731
[2024-09-18 02:27:08,525][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 468 @ 14823 updates
[2024-09-18 02:27:08,526][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 02:27:11,909][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 02:27:11,965][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 468 @ 14823 updates, score 26.941) (writing took 3.4394237709930167 seconds)
[2024-09-18 02:27:11,965][fairseq_cli.train][INFO] - end of epoch 468 (average epoch stats below)
[2024-09-18 02:27:11,966][train][INFO] - epoch 468 | loss 255.753 | ntokens 10343.3 | nsentences 47.2727 | nll_loss 1.169 | wps 2612.6 | ups 0.25 | wpb 10343.3 | bsz 47.3 | num_updates 14823 | lr 2.35783e-05 | gnorm 275.947 | loss_scale 1 | train_wall 150 | gb_free 17.2 | wall 0
[2024-09-18 02:27:11,967][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:27:11,983][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 469
[2024-09-18 02:27:12,001][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 02:27:12,024][fairseq.trainer][INFO] - begin training epoch 469
[2024-09-18 02:27:12,025][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 02:29:41,990][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 02:29:41,991][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:29:42,006][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 112
[2024-09-18 02:30:02,210][valid][INFO] - epoch 469 | valid on 'valid' subset | loss 212.01 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.945 | uer 18.706 | wer 27.068 | raw_wer 27.068 | wps 4717.9 | wpb 1902.9 | bsz 8.5 | num_updates 14867 | best_wer 26.731
[2024-09-18 02:30:02,211][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 469 @ 14867 updates
[2024-09-18 02:30:02,212][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 02:30:05,587][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 02:30:05,647][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 469 @ 14867 updates, score 27.068) (writing took 3.4356934380048187 seconds)
[2024-09-18 02:30:05,647][fairseq_cli.train][INFO] - end of epoch 469 (average epoch stats below)
[2024-09-18 02:30:05,649][train][INFO] - epoch 469 | loss 259.411 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 1.186 | wps 2620.3 | ups 0.25 | wpb 10343.2 | bsz 47.3 | num_updates 14867 | lr 2.32696e-05 | gnorm 279.763 | loss_scale 1 | train_wall 150 | gb_free 19.6 | wall 0
[2024-09-18 02:30:05,650][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:30:05,670][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 470
[2024-09-18 02:30:05,685][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 02:30:05,715][fairseq.trainer][INFO] - begin training epoch 470
[2024-09-18 02:30:05,715][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 02:32:36,328][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 02:32:36,329][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:32:36,344][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 113
[2024-09-18 02:32:56,572][valid][INFO] - epoch 470 | valid on 'valid' subset | loss 211.155 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.941 | uer 18.728 | wer 27.474 | raw_wer 27.474 | wps 4702.2 | wpb 1902.9 | bsz 8.5 | num_updates 14911 | best_wer 26.731
[2024-09-18 02:32:56,573][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 470 @ 14911 updates
[2024-09-18 02:32:56,573][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 02:32:59,969][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 02:33:00,045][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 470 @ 14911 updates, score 27.474) (writing took 3.471913310997479 seconds)
[2024-09-18 02:33:00,045][fairseq_cli.train][INFO] - end of epoch 470 (average epoch stats below)
[2024-09-18 02:33:00,047][train][INFO] - epoch 470 | loss 254.185 | ntokens 10343.7 | nsentences 47.2727 | nll_loss 1.162 | wps 2609.7 | ups 0.25 | wpb 10343.7 | bsz 47.3 | num_updates 14911 | lr 2.29649e-05 | gnorm 281.025 | loss_scale 1 | train_wall 150 | gb_free 17.4 | wall 0
[2024-09-18 02:33:00,048][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:33:00,066][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 471
[2024-09-18 02:33:00,081][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 02:33:00,109][fairseq.trainer][INFO] - begin training epoch 471
[2024-09-18 02:33:00,109][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 02:35:30,094][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 02:35:30,095][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:35:30,121][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 114
[2024-09-18 02:35:50,407][valid][INFO] - epoch 471 | valid on 'valid' subset | loss 211.431 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.942 | uer 18.067 | wer 26.692 | raw_wer 26.692 | wps 4696.6 | wpb 1902.9 | bsz 8.5 | num_updates 14955 | best_wer 26.692
[2024-09-18 02:35:50,408][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 471 @ 14955 updates
[2024-09-18 02:35:50,409][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 02:35:53,670][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 02:35:55,637][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 471 @ 14955 updates, score 26.692) (writing took 5.228681667998899 seconds)
[2024-09-18 02:35:55,637][fairseq_cli.train][INFO] - end of epoch 471 (average epoch stats below)
[2024-09-18 02:35:55,638][train][INFO] - epoch 471 | loss 256.671 | ntokens 10343.7 | nsentences 47.2727 | nll_loss 1.173 | wps 2591.9 | ups 0.25 | wpb 10343.7 | bsz 47.3 | num_updates 14955 | lr 2.26642e-05 | gnorm 288.984 | loss_scale 1 | train_wall 150 | gb_free 19.6 | wall 0
[2024-09-18 02:35:55,639][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:35:55,654][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 472
[2024-09-18 02:35:55,669][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 02:35:55,693][fairseq.trainer][INFO] - begin training epoch 472
[2024-09-18 02:35:55,693][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 02:38:26,617][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 02:38:26,618][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:38:26,633][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 115
[2024-09-18 02:38:47,180][valid][INFO] - epoch 472 | valid on 'valid' subset | loss 210.26 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.937 | uer 18.234 | wer 26.731 | raw_wer 26.731 | wps 4633.3 | wpb 1902.9 | bsz 8.5 | num_updates 14999 | best_wer 26.692
[2024-09-18 02:38:47,183][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 472 @ 14999 updates
[2024-09-18 02:38:47,183][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 02:38:50,590][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 02:38:50,649][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 472 @ 14999 updates, score 26.731) (writing took 3.4661294129982707 seconds)
[2024-09-18 02:38:50,649][fairseq_cli.train][INFO] - end of epoch 472 (average epoch stats below)
[2024-09-18 02:38:50,650][train][INFO] - epoch 472 | loss 257.825 | ntokens 10343.6 | nsentences 47.2727 | nll_loss 1.178 | wps 2600.5 | ups 0.25 | wpb 10343.6 | bsz 47.3 | num_updates 14999 | lr 2.23674e-05 | gnorm 297.519 | loss_scale 1 | train_wall 151 | gb_free 18.6 | wall 0
[2024-09-18 02:38:50,651][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:38:50,667][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 473
[2024-09-18 02:38:50,679][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 02:38:50,703][fairseq.trainer][INFO] - begin training epoch 473
[2024-09-18 02:38:50,704][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 02:38:53,938][train_inner][INFO] - epoch 473:      1 / 44 loss=257.38, ntokens=10336.5, nsentences=47, nll_loss=1.17, wps=2569.4, ups=0.25, wpb=10336.5, bsz=47, num_updates=15000, lr=2.23607e-05, gnorm=285.683, loss_scale=1, train_wall=683, gb_free=19.6, wall=0
[2024-09-18 02:38:53,938][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 02:38:53,939][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:38:53,954][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 116
[2024-09-18 02:39:14,226][valid][INFO] - epoch 473 | valid on 'valid' subset | loss 210.244 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.937 | uer 18.201 | wer 26.682 | raw_wer 26.682 | wps 4710.2 | wpb 1902.9 | bsz 8.5 | num_updates 15000 | best_wer 26.682
[2024-09-18 02:39:14,227][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 473 @ 15000 updates
[2024-09-18 02:39:14,227][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_473_15000.pt
[2024-09-18 02:39:17,285][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_473_15000.pt
[2024-09-18 02:39:21,388][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_473_15000.pt (epoch 473 @ 15000 updates, score 26.682) (writing took 7.160705304995645 seconds)
[2024-09-18 02:41:47,899][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 02:41:47,900][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:41:47,915][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 117
[2024-09-18 02:42:08,274][valid][INFO] - epoch 473 | valid on 'valid' subset | loss 211.029 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.94 | uer 18.236 | wer 26.604 | raw_wer 26.604 | wps 4691.2 | wpb 1902.9 | bsz 8.5 | num_updates 15043 | best_wer 26.604
[2024-09-18 02:42:08,275][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 473 @ 15043 updates
[2024-09-18 02:42:08,276][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 02:42:11,502][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 02:42:13,375][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 473 @ 15043 updates, score 26.604) (writing took 5.099647820999962 seconds)
[2024-09-18 02:42:13,375][fairseq_cli.train][INFO] - end of epoch 473 (average epoch stats below)
[2024-09-18 02:42:13,376][train][INFO] - epoch 473 | loss 258.429 | ntokens 10343.7 | nsentences 47.2727 | nll_loss 1.181 | wps 2245 | ups 0.22 | wpb 10343.7 | bsz 47.3 | num_updates 15043 | lr 2.20745e-05 | gnorm 273.298 | loss_scale 1 | train_wall 150 | gb_free 16.3 | wall 0
[2024-09-18 02:42:13,377][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:42:13,393][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 474
[2024-09-18 02:42:13,406][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 02:42:13,437][fairseq.trainer][INFO] - begin training epoch 474
[2024-09-18 02:42:13,438][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 02:44:43,965][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 02:44:43,966][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:44:43,991][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 118
[2024-09-18 02:45:04,329][valid][INFO] - epoch 474 | valid on 'valid' subset | loss 210.202 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.937 | uer 17.951 | wer 26.594 | raw_wer 26.594 | wps 4683.7 | wpb 1902.9 | bsz 8.5 | num_updates 15087 | best_wer 26.594
[2024-09-18 02:45:04,329][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 474 @ 15087 updates
[2024-09-18 02:45:04,330][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 02:45:07,645][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 02:45:09,484][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 474 @ 15087 updates, score 26.594) (writing took 5.1546094860023 seconds)
[2024-09-18 02:45:09,484][fairseq_cli.train][INFO] - end of epoch 474 (average epoch stats below)
[2024-09-18 02:45:09,485][train][INFO] - epoch 474 | loss 250.245 | ntokens 10343 | nsentences 47.2727 | nll_loss 1.144 | wps 2584.2 | ups 0.25 | wpb 10343 | bsz 47.3 | num_updates 15087 | lr 2.17854e-05 | gnorm 265.485 | loss_scale 1 | train_wall 150 | gb_free 17.4 | wall 0
[2024-09-18 02:45:09,486][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:45:09,502][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 475
[2024-09-18 02:45:09,514][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 02:45:09,539][fairseq.trainer][INFO] - begin training epoch 475
[2024-09-18 02:45:09,539][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 02:47:38,980][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 02:47:38,981][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:47:38,996][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 119
[2024-09-18 02:47:59,285][valid][INFO] - epoch 475 | valid on 'valid' subset | loss 209.477 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.934 | uer 18.122 | wer 26.662 | raw_wer 26.662 | wps 4687.6 | wpb 1902.9 | bsz 8.5 | num_updates 15131 | best_wer 26.594
[2024-09-18 02:47:59,286][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 475 @ 15131 updates
[2024-09-18 02:47:59,287][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 02:48:02,611][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 02:48:02,691][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 475 @ 15131 updates, score 26.662) (writing took 3.404472057998646 seconds)
[2024-09-18 02:48:02,691][fairseq_cli.train][INFO] - end of epoch 475 (average epoch stats below)
[2024-09-18 02:48:02,692][train][INFO] - epoch 475 | loss 258.537 | ntokens 10343.7 | nsentences 47.2727 | nll_loss 1.182 | wps 2627.6 | ups 0.25 | wpb 10343.7 | bsz 47.3 | num_updates 15131 | lr 2.15002e-05 | gnorm 292.477 | loss_scale 1 | train_wall 149 | gb_free 17.8 | wall 0
[2024-09-18 02:48:02,693][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:48:02,708][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 476
[2024-09-18 02:48:02,721][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 02:48:02,746][fairseq.trainer][INFO] - begin training epoch 476
[2024-09-18 02:48:02,747][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 02:50:32,686][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 02:50:32,687][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:50:32,702][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 120
[2024-09-18 02:50:53,103][valid][INFO] - epoch 476 | valid on 'valid' subset | loss 210.839 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.94 | uer 18.299 | wer 27.151 | raw_wer 27.151 | wps 4664.8 | wpb 1902.9 | bsz 8.5 | num_updates 15175 | best_wer 26.594
[2024-09-18 02:50:53,103][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 476 @ 15175 updates
[2024-09-18 02:50:53,104][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 02:50:56,410][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 02:50:56,466][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 476 @ 15175 updates, score 27.151) (writing took 3.362598314000934 seconds)
[2024-09-18 02:50:56,467][fairseq_cli.train][INFO] - end of epoch 476 (average epoch stats below)
[2024-09-18 02:50:56,468][train][INFO] - epoch 476 | loss 250.196 | ntokens 10343.4 | nsentences 47.2727 | nll_loss 1.143 | wps 2619 | ups 0.25 | wpb 10343.4 | bsz 47.3 | num_updates 15175 | lr 2.12186e-05 | gnorm 276.892 | loss_scale 1 | train_wall 150 | gb_free 18.2 | wall 0
[2024-09-18 02:50:56,468][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:50:56,484][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 477
[2024-09-18 02:50:56,496][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 02:50:56,527][fairseq.trainer][INFO] - begin training epoch 477
[2024-09-18 02:50:56,527][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 02:52:20,787][train_inner][INFO] - epoch 477:     25 / 44 loss=253.764, ntokens=10334.7, nsentences=47.36, nll_loss=1.163, wps=2561.7, ups=0.25, wpb=10334.7, bsz=47.4, num_updates=15200, lr=2.10603e-05, gnorm=276.173, loss_scale=1, train_wall=680, gb_free=23.9, wall=0
[2024-09-18 02:53:27,188][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 02:53:27,189][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:53:27,204][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 121
[2024-09-18 02:53:47,425][valid][INFO] - epoch 477 | valid on 'valid' subset | loss 209.919 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.935 | uer 18.075 | wer 26.975 | raw_wer 26.975 | wps 4693 | wpb 1902.9 | bsz 8.5 | num_updates 15219 | best_wer 26.594
[2024-09-18 02:53:47,426][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 477 @ 15219 updates
[2024-09-18 02:53:47,426][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 02:53:50,748][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 02:53:50,804][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 477 @ 15219 updates, score 26.975) (writing took 3.377877394996176 seconds)
[2024-09-18 02:53:50,804][fairseq_cli.train][INFO] - end of epoch 477 (average epoch stats below)
[2024-09-18 02:53:50,805][train][INFO] - epoch 477 | loss 248.75 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.137 | wps 2610.5 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 15219 | lr 2.09408e-05 | gnorm 265.158 | loss_scale 1 | train_wall 150 | gb_free 22.1 | wall 0
[2024-09-18 02:53:50,806][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:53:50,821][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 478
[2024-09-18 02:53:50,833][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 02:53:50,865][fairseq.trainer][INFO] - begin training epoch 478
[2024-09-18 02:53:50,865][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 02:56:20,685][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 02:56:20,686][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:56:20,702][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 122
[2024-09-18 02:56:41,161][valid][INFO] - epoch 478 | valid on 'valid' subset | loss 208.687 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.93 | uer 17.865 | wer 26.501 | raw_wer 26.501 | wps 4663.7 | wpb 1902.9 | bsz 8.5 | num_updates 15263 | best_wer 26.501
[2024-09-18 02:56:41,162][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 478 @ 15263 updates
[2024-09-18 02:56:41,162][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 02:56:44,450][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 02:56:46,240][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 478 @ 15263 updates, score 26.501) (writing took 5.078822816998581 seconds)
[2024-09-18 02:56:46,241][fairseq_cli.train][INFO] - end of epoch 478 (average epoch stats below)
[2024-09-18 02:56:46,242][train][INFO] - epoch 478 | loss 253.531 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 1.159 | wps 2594.1 | ups 0.25 | wpb 10343.2 | bsz 47.3 | num_updates 15263 | lr 2.06665e-05 | gnorm 281.473 | loss_scale 1 | train_wall 150 | gb_free 18.5 | wall 0
[2024-09-18 02:56:46,243][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:56:46,259][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 479
[2024-09-18 02:56:46,271][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 02:56:46,310][fairseq.trainer][INFO] - begin training epoch 479
[2024-09-18 02:56:46,310][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 02:59:16,536][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 02:59:16,536][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:59:16,551][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 123
[2024-09-18 02:59:36,907][valid][INFO] - epoch 479 | valid on 'valid' subset | loss 208.994 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.931 | uer 17.715 | wer 26.433 | raw_wer 26.433 | wps 4658.5 | wpb 1902.9 | bsz 8.5 | num_updates 15307 | best_wer 26.433
[2024-09-18 02:59:36,908][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 479 @ 15307 updates
[2024-09-18 02:59:36,908][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 02:59:40,264][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 02:59:41,931][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 479 @ 15307 updates, score 26.433) (writing took 5.0235387420034385 seconds)
[2024-09-18 02:59:41,931][fairseq_cli.train][INFO] - end of epoch 479 (average epoch stats below)
[2024-09-18 02:59:41,932][train][INFO] - epoch 479 | loss 250.688 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.146 | wps 2590.4 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 15307 | lr 2.03959e-05 | gnorm 271.115 | loss_scale 1 | train_wall 150 | gb_free 20.8 | wall 0
[2024-09-18 02:59:41,933][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 02:59:41,949][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 480
[2024-09-18 02:59:41,961][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 02:59:41,985][fairseq.trainer][INFO] - begin training epoch 480
[2024-09-18 02:59:41,986][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 03:02:11,820][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 03:02:11,821][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:02:11,836][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 124
[2024-09-18 03:02:32,183][valid][INFO] - epoch 480 | valid on 'valid' subset | loss 208.51 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.929 | uer 18.127 | wer 26.599 | raw_wer 26.599 | wps 4661 | wpb 1902.9 | bsz 8.5 | num_updates 15351 | best_wer 26.433
[2024-09-18 03:02:32,184][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 480 @ 15351 updates
[2024-09-18 03:02:32,185][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:02:35,502][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:02:35,572][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 480 @ 15351 updates, score 26.599) (writing took 3.387590092002938 seconds)
[2024-09-18 03:02:35,572][fairseq_cli.train][INFO] - end of epoch 480 (average epoch stats below)
[2024-09-18 03:02:35,573][train][INFO] - epoch 480 | loss 254.86 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.165 | wps 2621 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 15351 | lr 2.01288e-05 | gnorm 277.156 | loss_scale 1 | train_wall 150 | gb_free 19.3 | wall 0
[2024-09-18 03:02:35,574][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:02:35,590][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 481
[2024-09-18 03:02:35,602][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 03:02:35,628][fairseq.trainer][INFO] - begin training epoch 481
[2024-09-18 03:02:35,628][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 03:05:04,719][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 03:05:04,719][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:05:04,735][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 125
[2024-09-18 03:05:25,102][valid][INFO] - epoch 481 | valid on 'valid' subset | loss 208.043 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.927 | uer 18.058 | wer 26.76 | raw_wer 26.76 | wps 4653.6 | wpb 1902.9 | bsz 8.5 | num_updates 15395 | best_wer 26.433
[2024-09-18 03:05:25,104][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 481 @ 15395 updates
[2024-09-18 03:05:25,105][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:05:28,434][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:05:28,503][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 481 @ 15395 updates, score 26.76) (writing took 3.3986872419991414 seconds)
[2024-09-18 03:05:28,504][fairseq_cli.train][INFO] - end of epoch 481 (average epoch stats below)
[2024-09-18 03:05:28,505][train][INFO] - epoch 481 | loss 261.047 | ntokens 10343.3 | nsentences 47.2727 | nll_loss 1.193 | wps 2631.7 | ups 0.25 | wpb 10343.3 | bsz 47.3 | num_updates 15395 | lr 1.98653e-05 | gnorm 290.905 | loss_scale 1 | train_wall 149 | gb_free 20.3 | wall 0
[2024-09-18 03:05:28,505][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:05:28,521][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 482
[2024-09-18 03:05:28,535][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 03:05:28,560][fairseq.trainer][INFO] - begin training epoch 482
[2024-09-18 03:05:28,560][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 03:05:45,970][train_inner][INFO] - epoch 482:      5 / 44 loss=255.391, ntokens=10353.5, nsentences=47, nll_loss=1.159, wps=2571.7, ups=0.25, wpb=10353.5, bsz=47, num_updates=15400, lr=1.98355e-05, gnorm=278.939, loss_scale=1, train_wall=682, gb_free=20.2, wall=0
[2024-09-18 03:07:58,306][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 03:07:58,306][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:07:58,323][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 126
[2024-09-18 03:08:18,497][valid][INFO] - epoch 482 | valid on 'valid' subset | loss 208.285 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.928 | uer 18.261 | wer 26.76 | raw_wer 26.76 | wps 4716.2 | wpb 1902.9 | bsz 8.5 | num_updates 15439 | best_wer 26.433
[2024-09-18 03:08:18,497][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 482 @ 15439 updates
[2024-09-18 03:08:18,498][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:08:21,847][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:08:21,901][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 482 @ 15439 updates, score 26.76) (writing took 3.403484608999861 seconds)
[2024-09-18 03:08:21,901][fairseq_cli.train][INFO] - end of epoch 482 (average epoch stats below)
[2024-09-18 03:08:21,903][train][INFO] - epoch 482 | loss 257.499 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.177 | wps 2624.7 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 15439 | lr 1.96051e-05 | gnorm 290.659 | loss_scale 1 | train_wall 150 | gb_free 20.6 | wall 0
[2024-09-18 03:08:21,903][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:08:21,920][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 483
[2024-09-18 03:08:21,932][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 03:08:21,963][fairseq.trainer][INFO] - begin training epoch 483
[2024-09-18 03:08:21,963][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 03:10:51,964][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 03:10:51,965][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:10:51,980][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 127
[2024-09-18 03:11:12,394][valid][INFO] - epoch 483 | valid on 'valid' subset | loss 207.583 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.925 | uer 17.925 | wer 26.516 | raw_wer 26.516 | wps 4674 | wpb 1902.9 | bsz 8.5 | num_updates 15483 | best_wer 26.433
[2024-09-18 03:11:12,394][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 483 @ 15483 updates
[2024-09-18 03:11:12,395][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:11:15,724][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:11:15,779][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 483 @ 15483 updates, score 26.516) (writing took 3.3847027640003944 seconds)
[2024-09-18 03:11:15,779][fairseq_cli.train][INFO] - end of epoch 483 (average epoch stats below)
[2024-09-18 03:11:15,780][train][INFO] - epoch 483 | loss 244.39 | ntokens 10343.4 | nsentences 47.2727 | nll_loss 1.117 | wps 2617.4 | ups 0.25 | wpb 10343.4 | bsz 47.3 | num_updates 15483 | lr 1.93484e-05 | gnorm 278.399 | loss_scale 1 | train_wall 150 | gb_free 19 | wall 0
[2024-09-18 03:11:15,781][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:11:15,797][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 484
[2024-09-18 03:11:15,814][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 03:11:15,848][fairseq.trainer][INFO] - begin training epoch 484
[2024-09-18 03:11:15,849][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 03:13:46,018][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 03:13:46,018][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:13:46,033][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 128
[2024-09-18 03:14:06,370][valid][INFO] - epoch 484 | valid on 'valid' subset | loss 207.708 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.926 | uer 17.901 | wer 26.496 | raw_wer 26.496 | wps 4683.1 | wpb 1902.9 | bsz 8.5 | num_updates 15527 | best_wer 26.433
[2024-09-18 03:14:06,370][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 484 @ 15527 updates
[2024-09-18 03:14:06,371][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:14:09,714][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:14:09,784][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 484 @ 15527 updates, score 26.496) (writing took 3.413188236998394 seconds)
[2024-09-18 03:14:09,784][fairseq_cli.train][INFO] - end of epoch 484 (average epoch stats below)
[2024-09-18 03:14:09,786][train][INFO] - epoch 484 | loss 252.836 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 1.156 | wps 2615.5 | ups 0.25 | wpb 10343.2 | bsz 47.3 | num_updates 15527 | lr 1.9095e-05 | gnorm 272.947 | loss_scale 1 | train_wall 150 | gb_free 16.8 | wall 0
[2024-09-18 03:14:09,787][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:14:09,809][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 485
[2024-09-18 03:14:09,826][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 03:14:09,871][fairseq.trainer][INFO] - begin training epoch 485
[2024-09-18 03:14:09,871][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 03:16:39,967][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 03:16:39,968][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:16:39,983][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 129
[2024-09-18 03:17:00,392][valid][INFO] - epoch 485 | valid on 'valid' subset | loss 206.528 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.92 | uer 17.75 | wer 26.076 | raw_wer 26.076 | wps 4658.6 | wpb 1902.9 | bsz 8.5 | num_updates 15571 | best_wer 26.076
[2024-09-18 03:17:00,393][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 485 @ 15571 updates
[2024-09-18 03:17:00,394][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 03:17:03,678][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 03:17:05,475][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 485 @ 15571 updates, score 26.076) (writing took 5.08164099699934 seconds)
[2024-09-18 03:17:05,475][fairseq_cli.train][INFO] - end of epoch 485 (average epoch stats below)
[2024-09-18 03:17:05,476][train][INFO] - epoch 485 | loss 259.45 | ntokens 10344 | nsentences 47.2727 | nll_loss 1.186 | wps 2590.6 | ups 0.25 | wpb 10344 | bsz 47.3 | num_updates 15571 | lr 1.8845e-05 | gnorm 280.267 | loss_scale 1 | train_wall 150 | gb_free 22 | wall 0
[2024-09-18 03:17:05,477][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:17:05,493][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 486
[2024-09-18 03:17:05,505][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 03:17:05,544][fairseq.trainer][INFO] - begin training epoch 486
[2024-09-18 03:17:05,544][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 03:18:43,818][train_inner][INFO] - epoch 486:     29 / 44 loss=251.79, ntokens=10313.6, nsentences=47.68, nll_loss=1.164, wps=2651.8, ups=0.26, wpb=10313.6, bsz=47.7, num_updates=15600, lr=1.8682e-05, gnorm=279.294, loss_scale=1, train_wall=680, gb_free=21.4, wall=0
[2024-09-18 03:19:34,532][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 03:19:34,533][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:19:34,548][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 130
[2024-09-18 03:19:54,902][valid][INFO] - epoch 486 | valid on 'valid' subset | loss 206.181 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.919 | uer 17.829 | wer 26.266 | raw_wer 26.266 | wps 4662 | wpb 1902.9 | bsz 8.5 | num_updates 15615 | best_wer 26.076
[2024-09-18 03:19:54,903][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 486 @ 15615 updates
[2024-09-18 03:19:54,904][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:19:58,246][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:19:58,331][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 486 @ 15615 updates, score 26.266) (writing took 3.42786129200249 seconds)
[2024-09-18 03:19:58,331][fairseq_cli.train][INFO] - end of epoch 486 (average epoch stats below)
[2024-09-18 03:19:58,332][train][INFO] - epoch 486 | loss 258.617 | ntokens 10343.6 | nsentences 47.2727 | nll_loss 1.182 | wps 2632.9 | ups 0.25 | wpb 10343.6 | bsz 47.3 | num_updates 15615 | lr 1.85982e-05 | gnorm 285.992 | loss_scale 1 | train_wall 149 | gb_free 18.5 | wall 0
[2024-09-18 03:19:58,333][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:19:58,349][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 487
[2024-09-18 03:19:58,361][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 03:19:58,388][fairseq.trainer][INFO] - begin training epoch 487
[2024-09-18 03:19:58,388][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 03:22:28,215][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 03:22:28,216][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:22:28,232][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 131
[2024-09-18 03:22:48,647][valid][INFO] - epoch 487 | valid on 'valid' subset | loss 207.676 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.925 | uer 18.248 | wer 26.853 | raw_wer 26.853 | wps 4663.8 | wpb 1902.9 | bsz 8.5 | num_updates 15659 | best_wer 26.076
[2024-09-18 03:22:48,647][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 487 @ 15659 updates
[2024-09-18 03:22:48,648][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:22:51,984][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:22:52,070][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 487 @ 15659 updates, score 26.853) (writing took 3.422359135001898 seconds)
[2024-09-18 03:22:52,070][fairseq_cli.train][INFO] - end of epoch 487 (average epoch stats below)
[2024-09-18 03:22:52,071][train][INFO] - epoch 487 | loss 245.299 | ntokens 10343.4 | nsentences 47.2727 | nll_loss 1.121 | wps 2619.5 | ups 0.25 | wpb 10343.4 | bsz 47.3 | num_updates 15659 | lr 1.83547e-05 | gnorm 267.106 | loss_scale 1 | train_wall 150 | gb_free 18.6 | wall 0
[2024-09-18 03:22:52,072][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:22:52,088][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 488
[2024-09-18 03:22:52,100][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 03:22:52,135][fairseq.trainer][INFO] - begin training epoch 488
[2024-09-18 03:22:52,135][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 03:25:22,058][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 03:25:22,058][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:25:22,074][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 132
[2024-09-18 03:25:42,309][valid][INFO] - epoch 488 | valid on 'valid' subset | loss 206.065 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.918 | uer 18.062 | wer 26.462 | raw_wer 26.462 | wps 4721 | wpb 1902.9 | bsz 8.5 | num_updates 15703 | best_wer 26.076
[2024-09-18 03:25:42,309][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 488 @ 15703 updates
[2024-09-18 03:25:42,310][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:25:45,615][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:25:45,669][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 488 @ 15703 updates, score 26.462) (writing took 3.359714611004165 seconds)
[2024-09-18 03:25:45,670][fairseq_cli.train][INFO] - end of epoch 488 (average epoch stats below)
[2024-09-18 03:25:45,671][train][INFO] - epoch 488 | loss 246.977 | ntokens 10343.4 | nsentences 47.2727 | nll_loss 1.129 | wps 2621.6 | ups 0.25 | wpb 10343.4 | bsz 47.3 | num_updates 15703 | lr 1.81143e-05 | gnorm 281.179 | loss_scale 1 | train_wall 150 | gb_free 19.6 | wall 0
[2024-09-18 03:25:45,672][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:25:45,687][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 489
[2024-09-18 03:25:45,700][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 03:25:45,733][fairseq.trainer][INFO] - begin training epoch 489
[2024-09-18 03:25:45,733][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 03:28:15,117][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 03:28:15,118][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:28:15,132][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 133
[2024-09-18 03:28:35,426][valid][INFO] - epoch 489 | valid on 'valid' subset | loss 206.437 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.92 | uer 18.065 | wer 26.565 | raw_wer 26.565 | wps 4693.7 | wpb 1902.9 | bsz 8.5 | num_updates 15747 | best_wer 26.076
[2024-09-18 03:28:35,437][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 489 @ 15747 updates
[2024-09-18 03:28:35,438][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:28:38,753][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:28:38,807][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 489 @ 15747 updates, score 26.565) (writing took 3.369919534001383 seconds)
[2024-09-18 03:28:38,808][fairseq_cli.train][INFO] - end of epoch 489 (average epoch stats below)
[2024-09-18 03:28:38,809][train][INFO] - epoch 489 | loss 251.663 | ntokens 10343.7 | nsentences 47.2727 | nll_loss 1.15 | wps 2628.7 | ups 0.25 | wpb 10343.7 | bsz 47.3 | num_updates 15747 | lr 1.78771e-05 | gnorm 282.254 | loss_scale 1 | train_wall 149 | gb_free 19.1 | wall 0
[2024-09-18 03:28:38,810][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:28:38,826][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 490
[2024-09-18 03:28:38,839][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 03:28:38,873][fairseq.trainer][INFO] - begin training epoch 490
[2024-09-18 03:28:38,873][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 03:31:09,380][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 03:31:09,381][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:31:09,396][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 134
[2024-09-18 03:31:29,658][valid][INFO] - epoch 490 | valid on 'valid' subset | loss 205.651 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.916 | uer 17.751 | wer 26.428 | raw_wer 26.428 | wps 4691.3 | wpb 1902.9 | bsz 8.5 | num_updates 15791 | best_wer 26.076
[2024-09-18 03:31:29,659][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 490 @ 15791 updates
[2024-09-18 03:31:29,660][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:31:32,968][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:31:33,023][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 490 @ 15791 updates, score 26.428) (writing took 3.363967034005327 seconds)
[2024-09-18 03:31:33,024][fairseq_cli.train][INFO] - end of epoch 490 (average epoch stats below)
[2024-09-18 03:31:33,025][train][INFO] - epoch 490 | loss 247.268 | ntokens 10343.3 | nsentences 47.2727 | nll_loss 1.13 | wps 2612.3 | ups 0.25 | wpb 10343.3 | bsz 47.3 | num_updates 15791 | lr 1.7643e-05 | gnorm 291.168 | loss_scale 1 | train_wall 150 | gb_free 18.3 | wall 0
[2024-09-18 03:31:33,025][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:31:33,041][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 491
[2024-09-18 03:31:33,053][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 03:31:33,085][fairseq.trainer][INFO] - begin training epoch 491
[2024-09-18 03:31:33,085][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 03:32:03,754][train_inner][INFO] - epoch 491:      9 / 44 loss=248.592, ntokens=10375, nsentences=47.48, nll_loss=1.138, wps=2594, ups=0.25, wpb=10375, bsz=47.5, num_updates=15800, lr=1.75955e-05, gnorm=282.054, loss_scale=1, train_wall=680, gb_free=17.4, wall=0
[2024-09-18 03:34:02,774][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 03:34:02,775][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:34:02,800][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 135
[2024-09-18 03:34:23,362][valid][INFO] - epoch 491 | valid on 'valid' subset | loss 205.149 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.914 | uer 17.84 | wer 26.354 | raw_wer 26.354 | wps 4633.8 | wpb 1902.9 | bsz 8.5 | num_updates 15835 | best_wer 26.076
[2024-09-18 03:34:23,363][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 491 @ 15835 updates
[2024-09-18 03:34:23,364][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:34:26,679][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:34:26,732][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 491 @ 15835 updates, score 26.354) (writing took 3.3686944369983394 seconds)
[2024-09-18 03:34:26,732][fairseq_cli.train][INFO] - end of epoch 491 (average epoch stats below)
[2024-09-18 03:34:26,733][train][INFO] - epoch 491 | loss 248.731 | ntokens 10343.8 | nsentences 47.2727 | nll_loss 1.137 | wps 2620.1 | ups 0.25 | wpb 10343.8 | bsz 47.3 | num_updates 15835 | lr 1.7412e-05 | gnorm 290.677 | loss_scale 1 | train_wall 150 | gb_free 18.1 | wall 0
[2024-09-18 03:34:26,734][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:34:26,750][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 492
[2024-09-18 03:34:26,762][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 03:34:26,807][fairseq.trainer][INFO] - begin training epoch 492
[2024-09-18 03:34:26,807][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 03:36:57,748][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 03:36:57,748][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:36:57,763][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 136
[2024-09-18 03:37:18,169][valid][INFO] - epoch 492 | valid on 'valid' subset | loss 205.158 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.914 | uer 18.133 | wer 26.702 | raw_wer 26.702 | wps 4664 | wpb 1902.9 | bsz 8.5 | num_updates 15879 | best_wer 26.076
[2024-09-18 03:37:18,170][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 492 @ 15879 updates
[2024-09-18 03:37:18,171][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:37:21,503][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:37:21,569][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 492 @ 15879 updates, score 26.702) (writing took 3.3988994999963325 seconds)
[2024-09-18 03:37:21,570][fairseq_cli.train][INFO] - end of epoch 492 (average epoch stats below)
[2024-09-18 03:37:21,571][train][INFO] - epoch 492 | loss 243.311 | ntokens 10343.3 | nsentences 47.2727 | nll_loss 1.112 | wps 2603 | ups 0.25 | wpb 10343.3 | bsz 47.3 | num_updates 15879 | lr 1.7184e-05 | gnorm 264.338 | loss_scale 1 | train_wall 151 | gb_free 18.6 | wall 0
[2024-09-18 03:37:21,572][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:37:21,594][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 493
[2024-09-18 03:37:21,611][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 03:37:21,656][fairseq.trainer][INFO] - begin training epoch 493
[2024-09-18 03:37:21,657][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 03:39:51,691][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 03:39:51,692][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:39:51,708][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 137
[2024-09-18 03:40:12,084][valid][INFO] - epoch 493 | valid on 'valid' subset | loss 203.086 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.905 | uer 18.051 | wer 26.154 | raw_wer 26.154 | wps 4680.9 | wpb 1902.9 | bsz 8.5 | num_updates 15923 | best_wer 26.076
[2024-09-18 03:40:12,084][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 493 @ 15923 updates
[2024-09-18 03:40:12,085][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:40:15,378][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:40:15,432][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 493 @ 15923 updates, score 26.154) (writing took 3.347738347998529 seconds)
[2024-09-18 03:40:15,433][fairseq_cli.train][INFO] - end of epoch 493 (average epoch stats below)
[2024-09-18 03:40:15,434][train][INFO] - epoch 493 | loss 246.849 | ntokens 10343.4 | nsentences 47.2727 | nll_loss 1.128 | wps 2617.7 | ups 0.25 | wpb 10343.4 | bsz 47.3 | num_updates 15923 | lr 1.6959e-05 | gnorm 280.957 | loss_scale 1 | train_wall 150 | gb_free 17.5 | wall 0
[2024-09-18 03:40:15,435][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:40:15,451][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 494
[2024-09-18 03:40:15,463][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 03:40:15,498][fairseq.trainer][INFO] - begin training epoch 494
[2024-09-18 03:40:15,499][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 03:42:45,248][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 03:42:45,248][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:42:45,263][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 138
[2024-09-18 03:43:05,600][valid][INFO] - epoch 494 | valid on 'valid' subset | loss 204.174 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.91 | uer 17.645 | wer 26.002 | raw_wer 26.002 | wps 4696.9 | wpb 1902.9 | bsz 8.5 | num_updates 15967 | best_wer 26.002
[2024-09-18 03:43:05,601][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 494 @ 15967 updates
[2024-09-18 03:43:05,602][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 03:43:08,906][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 03:43:10,776][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 494 @ 15967 updates, score 26.002) (writing took 5.17485803400632 seconds)
[2024-09-18 03:43:10,776][fairseq_cli.train][INFO] - end of epoch 494 (average epoch stats below)
[2024-09-18 03:43:10,777][train][INFO] - epoch 494 | loss 245.786 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.123 | wps 2595.6 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 15967 | lr 1.67369e-05 | gnorm 271.631 | loss_scale 1 | train_wall 150 | gb_free 20.2 | wall 0
[2024-09-18 03:43:10,778][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:43:10,794][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 495
[2024-09-18 03:43:10,810][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 03:43:10,854][fairseq.trainer][INFO] - begin training epoch 495
[2024-09-18 03:43:10,854][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 03:45:03,873][train_inner][INFO] - epoch 495:     33 / 44 loss=246.972, ntokens=10351.6, nsentences=46.88, nll_loss=1.118, wps=2653.9, ups=0.26, wpb=10351.6, bsz=46.9, num_updates=16000, lr=1.65723e-05, gnorm=277.649, loss_scale=1, train_wall=682, gb_free=20.1, wall=0
[2024-09-18 03:45:41,215][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 03:45:41,216][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:45:41,231][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 139
[2024-09-18 03:46:01,581][valid][INFO] - epoch 495 | valid on 'valid' subset | loss 202.67 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.903 | uer 17.615 | wer 26.11 | raw_wer 26.11 | wps 4686 | wpb 1902.9 | bsz 8.5 | num_updates 16011 | best_wer 26.002
[2024-09-18 03:46:01,582][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 495 @ 16011 updates
[2024-09-18 03:46:01,582][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:46:04,926][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:46:05,004][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 495 @ 16011 updates, score 26.11) (writing took 3.4226157360026264 seconds)
[2024-09-18 03:46:05,005][fairseq_cli.train][INFO] - end of epoch 495 (average epoch stats below)
[2024-09-18 03:46:05,006][train][INFO] - epoch 495 | loss 243.362 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.112 | wps 2612.2 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 16011 | lr 1.65177e-05 | gnorm 276.278 | loss_scale 1 | train_wall 150 | gb_free 16.3 | wall 0
[2024-09-18 03:46:05,007][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:46:05,022][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 496
[2024-09-18 03:46:05,036][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 03:46:05,063][fairseq.trainer][INFO] - begin training epoch 496
[2024-09-18 03:46:05,064][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 03:48:34,863][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 03:48:34,863][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:48:34,878][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 140
[2024-09-18 03:48:55,258][valid][INFO] - epoch 496 | valid on 'valid' subset | loss 202.2 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.901 | uer 17.471 | wer 25.733 | raw_wer 25.733 | wps 4655.8 | wpb 1902.9 | bsz 8.5 | num_updates 16055 | best_wer 25.733
[2024-09-18 03:48:55,259][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 496 @ 16055 updates
[2024-09-18 03:48:55,260][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 03:48:58,567][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 03:49:00,468][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 496 @ 16055 updates, score 25.733) (writing took 5.209355327999219 seconds)
[2024-09-18 03:49:00,469][fairseq_cli.train][INFO] - end of epoch 496 (average epoch stats below)
[2024-09-18 03:49:00,470][train][INFO] - epoch 496 | loss 249.483 | ntokens 10343.6 | nsentences 47.2727 | nll_loss 1.14 | wps 2593.8 | ups 0.25 | wpb 10343.6 | bsz 47.3 | num_updates 16055 | lr 1.63015e-05 | gnorm 265.42 | loss_scale 1 | train_wall 150 | gb_free 16.9 | wall 0
[2024-09-18 03:49:00,471][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:49:00,487][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 497
[2024-09-18 03:49:00,507][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 03:49:00,552][fairseq.trainer][INFO] - begin training epoch 497
[2024-09-18 03:49:00,552][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 03:51:29,638][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 03:51:29,639][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:51:29,654][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 141
[2024-09-18 03:51:49,986][valid][INFO] - epoch 497 | valid on 'valid' subset | loss 202.819 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.904 | uer 17.697 | wer 26.115 | raw_wer 26.115 | wps 4691.4 | wpb 1902.9 | bsz 8.5 | num_updates 16099 | best_wer 25.733
[2024-09-18 03:51:49,986][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 497 @ 16099 updates
[2024-09-18 03:51:49,987][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:51:53,292][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:51:53,367][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 497 @ 16099 updates, score 26.115) (writing took 3.3810635960035142 seconds)
[2024-09-18 03:51:53,368][fairseq_cli.train][INFO] - end of epoch 497 (average epoch stats below)
[2024-09-18 03:51:53,369][train][INFO] - epoch 497 | loss 256.176 | ntokens 10343.7 | nsentences 47.2727 | nll_loss 1.171 | wps 2632.3 | ups 0.25 | wpb 10343.7 | bsz 47.3 | num_updates 16099 | lr 1.6088e-05 | gnorm 295.901 | loss_scale 1 | train_wall 149 | gb_free 15.6 | wall 0
[2024-09-18 03:51:53,370][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:51:53,385][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 498
[2024-09-18 03:51:53,402][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 03:51:53,431][fairseq.trainer][INFO] - begin training epoch 498
[2024-09-18 03:51:53,431][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 03:54:23,425][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 03:54:23,425][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:54:23,440][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 142
[2024-09-18 03:54:43,774][valid][INFO] - epoch 498 | valid on 'valid' subset | loss 202.294 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.902 | uer 17.733 | wer 26.022 | raw_wer 26.022 | wps 4684 | wpb 1902.9 | bsz 8.5 | num_updates 16143 | best_wer 25.733
[2024-09-18 03:54:43,774][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 498 @ 16143 updates
[2024-09-18 03:54:43,775][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:54:47,101][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:54:47,177][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 498 @ 16143 updates, score 26.022) (writing took 3.4022940739960177 seconds)
[2024-09-18 03:54:47,177][fairseq_cli.train][INFO] - end of epoch 498 (average epoch stats below)
[2024-09-18 03:54:47,178][train][INFO] - epoch 498 | loss 245.91 | ntokens 10343.4 | nsentences 47.2727 | nll_loss 1.124 | wps 2618.5 | ups 0.25 | wpb 10343.4 | bsz 47.3 | num_updates 16143 | lr 1.58773e-05 | gnorm 268.873 | loss_scale 1 | train_wall 150 | gb_free 18.4 | wall 0
[2024-09-18 03:54:47,179][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:54:47,194][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 499
[2024-09-18 03:54:47,206][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 03:54:47,235][fairseq.trainer][INFO] - begin training epoch 499
[2024-09-18 03:54:47,235][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 03:57:16,431][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 03:57:16,431][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:57:16,446][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 143
[2024-09-18 03:57:36,801][valid][INFO] - epoch 499 | valid on 'valid' subset | loss 203.042 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.905 | uer 17.577 | wer 26.129 | raw_wer 26.129 | wps 4676 | wpb 1902.9 | bsz 8.5 | num_updates 16187 | best_wer 25.733
[2024-09-18 03:57:36,802][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 499 @ 16187 updates
[2024-09-18 03:57:36,802][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:57:40,103][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 03:57:40,156][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 499 @ 16187 updates, score 26.129) (writing took 3.3546045719995163 seconds)
[2024-09-18 03:57:40,157][fairseq_cli.train][INFO] - end of epoch 499 (average epoch stats below)
[2024-09-18 03:57:40,158][train][INFO] - epoch 499 | loss 251.147 | ntokens 10343.6 | nsentences 47.2727 | nll_loss 1.148 | wps 2631.1 | ups 0.25 | wpb 10343.6 | bsz 47.3 | num_updates 16187 | lr 1.56694e-05 | gnorm 281.938 | loss_scale 1 | train_wall 149 | gb_free 18.6 | wall 0
[2024-09-18 03:57:40,158][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 03:57:40,175][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 500
[2024-09-18 03:57:40,187][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 03:57:40,232][fairseq.trainer][INFO] - begin training epoch 500
[2024-09-18 03:57:40,232][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 03:58:24,251][train_inner][INFO] - epoch 500:     13 / 44 loss=250.35, ntokens=10314.9, nsentences=47.12, nll_loss=1.144, wps=2577.5, ups=0.25, wpb=10314.9, bsz=47.1, num_updates=16200, lr=1.56085e-05, gnorm=276.701, loss_scale=1, train_wall=679, gb_free=16.4, wall=0
[2024-09-18 04:00:09,899][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 04:00:09,900][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:00:09,915][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 144
[2024-09-18 04:00:30,341][valid][INFO] - epoch 500 | valid on 'valid' subset | loss 202.592 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.903 | uer 17.816 | wer 26.129 | raw_wer 26.129 | wps 4667.3 | wpb 1902.9 | bsz 8.5 | num_updates 16231 | best_wer 25.733
[2024-09-18 04:00:30,341][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 500 @ 16231 updates
[2024-09-18 04:00:30,342][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 04:00:33,656][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 04:00:33,709][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 500 @ 16231 updates, score 26.129) (writing took 3.367767288000323 seconds)
[2024-09-18 04:00:33,710][fairseq_cli.train][INFO] - end of epoch 500 (average epoch stats below)
[2024-09-18 04:00:33,711][train][INFO] - epoch 500 | loss 249.708 | ntokens 10343.6 | nsentences 47.2727 | nll_loss 1.141 | wps 2622.4 | ups 0.25 | wpb 10343.6 | bsz 47.3 | num_updates 16231 | lr 1.54642e-05 | gnorm 280.528 | loss_scale 1 | train_wall 150 | gb_free 21.9 | wall 0
[2024-09-18 04:00:33,712][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:00:33,728][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 501
[2024-09-18 04:00:33,740][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 04:00:33,776][fairseq.trainer][INFO] - begin training epoch 501
[2024-09-18 04:00:33,777][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 04:03:04,602][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 04:03:04,603][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:03:04,618][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 145
[2024-09-18 04:03:24,931][valid][INFO] - epoch 501 | valid on 'valid' subset | loss 201.208 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.897 | uer 17.254 | wer 25.685 | raw_wer 25.685 | wps 4683.6 | wpb 1902.9 | bsz 8.5 | num_updates 16275 | best_wer 25.685
[2024-09-18 04:03:24,932][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 501 @ 16275 updates
[2024-09-18 04:03:24,933][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 04:03:28,110][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 04:03:30,002][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 501 @ 16275 updates, score 25.685) (writing took 5.06989897099993 seconds)
[2024-09-18 04:03:30,002][fairseq_cli.train][INFO] - end of epoch 501 (average epoch stats below)
[2024-09-18 04:03:30,003][train][INFO] - epoch 501 | loss 240.443 | ntokens 10343.6 | nsentences 47.2727 | nll_loss 1.099 | wps 2581.6 | ups 0.25 | wpb 10343.6 | bsz 47.3 | num_updates 16275 | lr 1.52617e-05 | gnorm 282.919 | loss_scale 1 | train_wall 151 | gb_free 19.5 | wall 0
[2024-09-18 04:03:30,004][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:03:30,020][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 502
[2024-09-18 04:03:30,032][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 04:03:30,076][fairseq.trainer][INFO] - begin training epoch 502
[2024-09-18 04:03:30,076][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 04:06:00,605][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 04:06:00,605][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:06:00,632][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 146
[2024-09-18 04:06:20,943][valid][INFO] - epoch 502 | valid on 'valid' subset | loss 201.778 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.899 | uer 17.55 | wer 25.631 | raw_wer 25.631 | wps 4675 | wpb 1902.9 | bsz 8.5 | num_updates 16319 | best_wer 25.631
[2024-09-18 04:06:20,944][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 502 @ 16319 updates
[2024-09-18 04:06:20,945][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 04:06:24,208][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 04:06:26,116][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 502 @ 16319 updates, score 25.631) (writing took 5.171597509004641 seconds)
[2024-09-18 04:06:26,116][fairseq_cli.train][INFO] - end of epoch 502 (average epoch stats below)
[2024-09-18 04:06:26,117][train][INFO] - epoch 502 | loss 243.781 | ntokens 10343.3 | nsentences 47.2727 | nll_loss 1.114 | wps 2584.2 | ups 0.25 | wpb 10343.3 | bsz 47.3 | num_updates 16319 | lr 1.50619e-05 | gnorm 291.991 | loss_scale 1 | train_wall 150 | gb_free 18.6 | wall 0
[2024-09-18 04:06:26,118][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:06:26,134][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 503
[2024-09-18 04:06:26,146][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 04:06:26,192][fairseq.trainer][INFO] - begin training epoch 503
[2024-09-18 04:06:26,192][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 04:08:57,163][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 04:08:57,164][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:08:57,179][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 147
[2024-09-18 04:09:17,541][valid][INFO] - epoch 503 | valid on 'valid' subset | loss 200.682 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.894 | uer 17.395 | wer 25.836 | raw_wer 25.836 | wps 4673.3 | wpb 1902.9 | bsz 8.5 | num_updates 16363 | best_wer 25.631
[2024-09-18 04:09:17,542][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 503 @ 16363 updates
[2024-09-18 04:09:17,543][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 04:09:20,837][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 04:09:20,906][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 503 @ 16363 updates, score 25.836) (writing took 3.364192422006454 seconds)
[2024-09-18 04:09:20,907][fairseq_cli.train][INFO] - end of epoch 503 (average epoch stats below)
[2024-09-18 04:09:20,908][train][INFO] - epoch 503 | loss 240.435 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 1.099 | wps 2603.7 | ups 0.25 | wpb 10343.2 | bsz 47.3 | num_updates 16363 | lr 1.48646e-05 | gnorm 279.406 | loss_scale 1 | train_wall 151 | gb_free 19 | wall 0
[2024-09-18 04:09:20,909][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:09:20,924][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 504
[2024-09-18 04:09:20,937][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 04:09:20,965][fairseq.trainer][INFO] - begin training epoch 504
[2024-09-18 04:09:20,965][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 04:11:27,866][train_inner][INFO] - epoch 504:     37 / 44 loss=242.749, ntokens=10385.4, nsentences=47.48, nll_loss=1.11, wps=2650.6, ups=0.26, wpb=10385.4, bsz=47.5, num_updates=16400, lr=1.47008e-05, gnorm=283.264, loss_scale=1, train_wall=684, gb_free=19.3, wall=0
[2024-09-18 04:11:50,707][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 04:11:50,708][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:11:50,723][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 148
[2024-09-18 04:12:11,056][valid][INFO] - epoch 504 | valid on 'valid' subset | loss 200.744 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.895 | uer 17.512 | wer 25.645 | raw_wer 25.645 | wps 4679.4 | wpb 1902.9 | bsz 8.5 | num_updates 16407 | best_wer 25.631
[2024-09-18 04:12:11,057][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 504 @ 16407 updates
[2024-09-18 04:12:11,057][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 04:12:14,355][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 04:12:14,428][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 504 @ 16407 updates, score 25.645) (writing took 3.371563098000479 seconds)
[2024-09-18 04:12:14,429][fairseq_cli.train][INFO] - end of epoch 504 (average epoch stats below)
[2024-09-18 04:12:14,430][train][INFO] - epoch 504 | loss 243.729 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 1.114 | wps 2622.7 | ups 0.25 | wpb 10343.2 | bsz 47.3 | num_updates 16407 | lr 1.467e-05 | gnorm 279.57 | loss_scale 1 | train_wall 150 | gb_free 19.1 | wall 0
[2024-09-18 04:12:14,431][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:12:14,447][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 505
[2024-09-18 04:12:14,465][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 04:12:14,494][fairseq.trainer][INFO] - begin training epoch 505
[2024-09-18 04:12:14,494][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 04:14:44,599][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 04:14:44,600][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:14:44,615][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 149
[2024-09-18 04:15:04,942][valid][INFO] - epoch 505 | valid on 'valid' subset | loss 201.036 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.896 | uer 17.439 | wer 26.144 | raw_wer 26.144 | wps 4700.7 | wpb 1902.9 | bsz 8.5 | num_updates 16451 | best_wer 25.631
[2024-09-18 04:15:04,943][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 505 @ 16451 updates
[2024-09-18 04:15:04,944][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 04:15:08,254][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 04:15:08,307][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 505 @ 16451 updates, score 26.144) (writing took 3.364014448998205 seconds)
[2024-09-18 04:15:08,307][fairseq_cli.train][INFO] - end of epoch 505 (average epoch stats below)
[2024-09-18 04:15:08,308][train][INFO] - epoch 505 | loss 249.029 | ntokens 10344 | nsentences 47.2727 | nll_loss 1.138 | wps 2617.6 | ups 0.25 | wpb 10344 | bsz 47.3 | num_updates 16451 | lr 1.44779e-05 | gnorm 300.681 | loss_scale 1 | train_wall 150 | gb_free 19.6 | wall 0
[2024-09-18 04:15:08,309][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:15:08,325][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 506
[2024-09-18 04:15:08,342][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 04:15:08,379][fairseq.trainer][INFO] - begin training epoch 506
[2024-09-18 04:15:08,379][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 04:17:38,128][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 04:17:38,129][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:17:38,143][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 150
[2024-09-18 04:17:58,644][valid][INFO] - epoch 506 | valid on 'valid' subset | loss 200.254 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.892 | uer 17.229 | wer 25.685 | raw_wer 25.685 | wps 4653.1 | wpb 1902.9 | bsz 8.5 | num_updates 16495 | best_wer 25.631
[2024-09-18 04:17:58,645][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 506 @ 16495 updates
[2024-09-18 04:17:58,645][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 04:18:02,000][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 04:18:02,055][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 506 @ 16495 updates, score 25.685) (writing took 3.4105827000021236 seconds)
[2024-09-18 04:18:02,056][fairseq_cli.train][INFO] - end of epoch 506 (average epoch stats below)
[2024-09-18 04:18:02,057][train][INFO] - epoch 506 | loss 242.372 | ntokens 10343.6 | nsentences 47.2727 | nll_loss 1.108 | wps 2619.4 | ups 0.25 | wpb 10343.6 | bsz 47.3 | num_updates 16495 | lr 1.42883e-05 | gnorm 270.977 | loss_scale 1 | train_wall 150 | gb_free 19.4 | wall 0
[2024-09-18 04:18:02,058][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:18:02,073][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 507
[2024-09-18 04:18:02,085][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 04:18:02,122][fairseq.trainer][INFO] - begin training epoch 507
[2024-09-18 04:18:02,122][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 04:20:32,410][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 04:20:32,411][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:20:32,425][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 151
[2024-09-18 04:20:52,743][valid][INFO] - epoch 507 | valid on 'valid' subset | loss 199.703 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.89 | uer 17.317 | wer 25.616 | raw_wer 25.616 | wps 4678.8 | wpb 1902.9 | bsz 8.5 | num_updates 16539 | best_wer 25.616
[2024-09-18 04:20:52,744][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 507 @ 16539 updates
[2024-09-18 04:20:52,745][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 04:20:56,037][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 04:20:57,863][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 507 @ 16539 updates, score 25.616) (writing took 5.119235477999609 seconds)
[2024-09-18 04:20:57,864][fairseq_cli.train][INFO] - end of epoch 507 (average epoch stats below)
[2024-09-18 04:20:57,865][train][INFO] - epoch 507 | loss 240.669 | ntokens 10343.3 | nsentences 47.2727 | nll_loss 1.1 | wps 2588.7 | ups 0.25 | wpb 10343.3 | bsz 47.3 | num_updates 16539 | lr 1.41012e-05 | gnorm 276.745 | loss_scale 1 | train_wall 150 | gb_free 19.7 | wall 0
[2024-09-18 04:20:57,866][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:20:57,882][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 508
[2024-09-18 04:20:57,895][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 04:20:57,946][fairseq.trainer][INFO] - begin training epoch 508
[2024-09-18 04:20:57,946][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 04:23:28,451][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 04:23:28,451][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:23:28,477][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 152
[2024-09-18 04:23:48,801][valid][INFO] - epoch 508 | valid on 'valid' subset | loss 200.572 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.894 | uer 17.48 | wer 25.978 | raw_wer 25.978 | wps 4690.2 | wpb 1902.9 | bsz 8.5 | num_updates 16583 | best_wer 25.616
[2024-09-18 04:23:48,802][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 508 @ 16583 updates
[2024-09-18 04:23:48,803][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 04:23:52,139][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 04:23:52,220][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 508 @ 16583 updates, score 25.978) (writing took 3.417933124997944 seconds)
[2024-09-18 04:23:52,220][fairseq_cli.train][INFO] - end of epoch 508 (average epoch stats below)
[2024-09-18 04:23:52,221][train][INFO] - epoch 508 | loss 236.108 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 1.079 | wps 2610.2 | ups 0.25 | wpb 10343.2 | bsz 47.3 | num_updates 16583 | lr 1.39166e-05 | gnorm 274.437 | loss_scale 1 | train_wall 150 | gb_free 17.3 | wall 0
[2024-09-18 04:23:52,222][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:23:52,238][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 509
[2024-09-18 04:23:52,252][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 04:23:52,281][fairseq.trainer][INFO] - begin training epoch 509
[2024-09-18 04:23:52,281][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 04:24:50,524][train_inner][INFO] - epoch 509:     17 / 44 loss=242.602, ntokens=10317.4, nsentences=47.12, nll_loss=1.108, wps=2570.8, ups=0.25, wpb=10317.4, bsz=47.1, num_updates=16600, lr=1.38459e-05, gnorm=281.654, loss_scale=1, train_wall=681, gb_free=16.1, wall=0
[2024-09-18 04:26:22,115][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 04:26:22,116][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:26:22,131][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 153
[2024-09-18 04:26:42,448][valid][INFO] - epoch 509 | valid on 'valid' subset | loss 200.555 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.894 | uer 17.516 | wer 26.193 | raw_wer 26.193 | wps 4689.1 | wpb 1902.9 | bsz 8.5 | num_updates 16627 | best_wer 25.616
[2024-09-18 04:26:42,449][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 509 @ 16627 updates
[2024-09-18 04:26:42,449][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 04:26:45,760][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 04:26:45,842][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 509 @ 16627 updates, score 26.193) (writing took 3.392952685004275 seconds)
[2024-09-18 04:26:45,842][fairseq_cli.train][INFO] - end of epoch 509 (average epoch stats below)
[2024-09-18 04:26:45,843][train][INFO] - epoch 509 | loss 242.95 | ntokens 10343.4 | nsentences 47.2727 | nll_loss 1.11 | wps 2621.3 | ups 0.25 | wpb 10343.4 | bsz 47.3 | num_updates 16627 | lr 1.37343e-05 | gnorm 277.508 | loss_scale 1 | train_wall 150 | gb_free 18 | wall 0
[2024-09-18 04:26:45,844][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:26:45,859][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 510
[2024-09-18 04:26:45,871][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 04:26:45,902][fairseq.trainer][INFO] - begin training epoch 510
[2024-09-18 04:26:45,902][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 04:29:16,898][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 04:29:16,899][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:29:16,914][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 154
[2024-09-18 04:29:37,271][valid][INFO] - epoch 510 | valid on 'valid' subset | loss 199.257 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.888 | uer 17.199 | wer 25.465 | raw_wer 25.465 | wps 4663.6 | wpb 1902.9 | bsz 8.5 | num_updates 16671 | best_wer 25.465
[2024-09-18 04:29:37,271][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 510 @ 16671 updates
[2024-09-18 04:29:37,272][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 04:29:40,564][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 04:29:42,482][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 510 @ 16671 updates, score 25.465) (writing took 5.210792306999792 seconds)
[2024-09-18 04:29:42,483][fairseq_cli.train][INFO] - end of epoch 510 (average epoch stats below)
[2024-09-18 04:29:42,484][train][INFO] - epoch 510 | loss 233.569 | ntokens 10343.6 | nsentences 47.2727 | nll_loss 1.067 | wps 2576.5 | ups 0.25 | wpb 10343.6 | bsz 47.3 | num_updates 16671 | lr 1.35545e-05 | gnorm 260.748 | loss_scale 1 | train_wall 151 | gb_free 16.4 | wall 0
[2024-09-18 04:29:42,485][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:29:42,501][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 511
[2024-09-18 04:29:42,514][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 04:29:42,560][fairseq.trainer][INFO] - begin training epoch 511
[2024-09-18 04:29:42,560][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 04:32:12,936][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 04:32:12,936][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:32:12,952][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 155
[2024-09-18 04:32:33,251][valid][INFO] - epoch 511 | valid on 'valid' subset | loss 201.356 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.897 | uer 17.073 | wer 25.562 | raw_wer 25.562 | wps 4683.3 | wpb 1902.9 | bsz 8.5 | num_updates 16715 | best_wer 25.465
[2024-09-18 04:32:33,251][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 511 @ 16715 updates
[2024-09-18 04:32:33,252][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 04:32:36,638][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 04:32:36,720][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 511 @ 16715 updates, score 25.562) (writing took 3.468168728002638 seconds)
[2024-09-18 04:32:36,720][fairseq_cli.train][INFO] - end of epoch 511 (average epoch stats below)
[2024-09-18 04:32:36,722][train][INFO] - epoch 511 | loss 242.516 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.108 | wps 2612 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 16715 | lr 1.3377e-05 | gnorm 279.173 | loss_scale 1 | train_wall 150 | gb_free 24.6 | wall 0
[2024-09-18 04:32:36,723][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:32:36,744][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 512
[2024-09-18 04:32:36,764][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 04:32:36,807][fairseq.trainer][INFO] - begin training epoch 512
[2024-09-18 04:32:36,808][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 04:35:06,841][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 04:35:06,842][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:35:06,857][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 156
[2024-09-18 04:35:27,205][valid][INFO] - epoch 512 | valid on 'valid' subset | loss 199.199 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.888 | uer 17.211 | wer 25.401 | raw_wer 25.401 | wps 4684.7 | wpb 1902.9 | bsz 8.5 | num_updates 16759 | best_wer 25.401
[2024-09-18 04:35:27,206][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 512 @ 16759 updates
[2024-09-18 04:35:27,206][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 04:35:30,484][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 04:35:32,457][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 512 @ 16759 updates, score 25.401) (writing took 5.251584439996805 seconds)
[2024-09-18 04:35:32,458][fairseq_cli.train][INFO] - end of epoch 512 (average epoch stats below)
[2024-09-18 04:35:32,459][train][INFO] - epoch 512 | loss 244.095 | ntokens 10343.6 | nsentences 47.2727 | nll_loss 1.116 | wps 2589.8 | ups 0.25 | wpb 10343.6 | bsz 47.3 | num_updates 16759 | lr 1.32018e-05 | gnorm 292.384 | loss_scale 1 | train_wall 150 | gb_free 18.7 | wall 0
[2024-09-18 04:35:32,460][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:35:32,475][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 513
[2024-09-18 04:35:32,488][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 04:35:32,517][fairseq.trainer][INFO] - begin training epoch 513
[2024-09-18 04:35:32,517][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 04:37:52,140][train_inner][INFO] - epoch 513:     41 / 44 loss=243.252, ntokens=10363, nsentences=47.24, nll_loss=1.109, wps=2651.7, ups=0.26, wpb=10363, bsz=47.2, num_updates=16800, lr=1.30407e-05, gnorm=277.878, loss_scale=1, train_wall=682, gb_free=19, wall=0
[2024-09-18 04:38:01,424][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 04:38:01,425][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:38:01,440][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 157
[2024-09-18 04:38:21,849][valid][INFO] - epoch 513 | valid on 'valid' subset | loss 199.805 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.89 | uer 17.379 | wer 25.631 | raw_wer 25.631 | wps 4672.3 | wpb 1902.9 | bsz 8.5 | num_updates 16803 | best_wer 25.401
[2024-09-18 04:38:21,850][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 513 @ 16803 updates
[2024-09-18 04:38:21,851][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 04:38:25,218][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 04:38:25,309][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 513 @ 16803 updates, score 25.631) (writing took 3.4584463149949443 seconds)
[2024-09-18 04:38:25,309][fairseq_cli.train][INFO] - end of epoch 513 (average epoch stats below)
[2024-09-18 04:38:25,310][train][INFO] - epoch 513 | loss 249.722 | ntokens 10343.7 | nsentences 47.2727 | nll_loss 1.141 | wps 2633 | ups 0.25 | wpb 10343.7 | bsz 47.3 | num_updates 16803 | lr 1.30289e-05 | gnorm 278.814 | loss_scale 1 | train_wall 149 | gb_free 18.8 | wall 0
[2024-09-18 04:38:25,311][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:38:25,330][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 514
[2024-09-18 04:38:25,345][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 04:38:25,390][fairseq.trainer][INFO] - begin training epoch 514
[2024-09-18 04:38:25,390][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 04:40:55,621][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 04:40:55,621][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:40:55,636][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 158
[2024-09-18 04:41:15,964][valid][INFO] - epoch 514 | valid on 'valid' subset | loss 199.143 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.887 | uer 17.346 | wer 25.328 | raw_wer 25.328 | wps 4676.5 | wpb 1902.9 | bsz 8.5 | num_updates 16847 | best_wer 25.328
[2024-09-18 04:41:15,964][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 514 @ 16847 updates
[2024-09-18 04:41:15,965][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 04:41:19,263][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 04:41:21,226][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 514 @ 16847 updates, score 25.328) (writing took 5.261129931001051 seconds)
[2024-09-18 04:41:21,226][fairseq_cli.train][INFO] - end of epoch 514 (average epoch stats below)
[2024-09-18 04:41:21,227][train][INFO] - epoch 514 | loss 243.438 | ntokens 10343.6 | nsentences 47.2727 | nll_loss 1.113 | wps 2587.1 | ups 0.25 | wpb 10343.6 | bsz 47.3 | num_updates 16847 | lr 1.28583e-05 | gnorm 277.488 | loss_scale 1 | train_wall 150 | gb_free 19.5 | wall 0
[2024-09-18 04:41:21,228][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:41:21,243][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 515
[2024-09-18 04:41:21,256][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 04:41:21,286][fairseq.trainer][INFO] - begin training epoch 515
[2024-09-18 04:41:21,286][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 04:43:51,705][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 04:43:51,705][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:43:51,721][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 159
[2024-09-18 04:44:12,343][valid][INFO] - epoch 515 | valid on 'valid' subset | loss 199.211 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.888 | uer 17.207 | wer 25.504 | raw_wer 25.504 | wps 4618.9 | wpb 1902.9 | bsz 8.5 | num_updates 16891 | best_wer 25.328
[2024-09-18 04:44:12,344][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 515 @ 16891 updates
[2024-09-18 04:44:12,344][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 04:44:15,740][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 04:44:15,813][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 515 @ 16891 updates, score 25.504) (writing took 3.4696407569936127 seconds)
[2024-09-18 04:44:15,814][fairseq_cli.train][INFO] - end of epoch 515 (average epoch stats below)
[2024-09-18 04:44:15,815][train][INFO] - epoch 515 | loss 240.044 | ntokens 10343.8 | nsentences 47.2727 | nll_loss 1.097 | wps 2606.9 | ups 0.25 | wpb 10343.8 | bsz 47.3 | num_updates 16891 | lr 1.269e-05 | gnorm 269.672 | loss_scale 1 | train_wall 150 | gb_free 17.2 | wall 0
[2024-09-18 04:44:15,816][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:44:15,831][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 516
[2024-09-18 04:44:15,846][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 04:44:15,876][fairseq.trainer][INFO] - begin training epoch 516
[2024-09-18 04:44:15,876][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 04:46:46,292][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 04:46:46,293][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:46:46,307][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 160
[2024-09-18 04:47:06,629][valid][INFO] - epoch 516 | valid on 'valid' subset | loss 199.099 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.887 | uer 17.057 | wer 25.244 | raw_wer 25.244 | wps 4691.2 | wpb 1902.9 | bsz 8.5 | num_updates 16935 | best_wer 25.244
[2024-09-18 04:47:06,629][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 516 @ 16935 updates
[2024-09-18 04:47:06,630][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 04:47:09,929][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 04:47:11,891][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 516 @ 16935 updates, score 25.244) (writing took 5.261762606998673 seconds)
[2024-09-18 04:47:11,892][fairseq_cli.train][INFO] - end of epoch 516 (average epoch stats below)
[2024-09-18 04:47:11,893][train][INFO] - epoch 516 | loss 240.828 | ntokens 10343.4 | nsentences 47.2727 | nll_loss 1.101 | wps 2584.7 | ups 0.25 | wpb 10343.4 | bsz 47.3 | num_updates 16935 | lr 1.25238e-05 | gnorm 270.682 | loss_scale 1 | train_wall 150 | gb_free 16.9 | wall 0
[2024-09-18 04:47:11,894][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:47:11,909][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 517
[2024-09-18 04:47:11,921][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 04:47:11,952][fairseq.trainer][INFO] - begin training epoch 517
[2024-09-18 04:47:11,952][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 04:49:41,294][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 04:49:41,295][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:49:41,310][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 161
[2024-09-18 04:50:01,672][valid][INFO] - epoch 517 | valid on 'valid' subset | loss 198.574 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.885 | uer 17.314 | wer 25.533 | raw_wer 25.533 | wps 4657.3 | wpb 1902.9 | bsz 8.5 | num_updates 16979 | best_wer 25.244
[2024-09-18 04:50:01,673][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 517 @ 16979 updates
[2024-09-18 04:50:01,674][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 04:50:05,007][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 04:50:05,079][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 517 @ 16979 updates, score 25.533) (writing took 3.4056898729977547 seconds)
[2024-09-18 04:50:05,079][fairseq_cli.train][INFO] - end of epoch 517 (average epoch stats below)
[2024-09-18 04:50:05,080][train][INFO] - epoch 517 | loss 243.544 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.113 | wps 2627.9 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 16979 | lr 1.23598e-05 | gnorm 275.579 | loss_scale 1 | train_wall 149 | gb_free 18.5 | wall 0
[2024-09-18 04:50:05,081][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:50:05,097][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 518
[2024-09-18 04:50:05,113][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 04:50:05,144][fairseq.trainer][INFO] - begin training epoch 518
[2024-09-18 04:50:05,145][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 04:51:16,086][train_inner][INFO] - epoch 518:     21 / 44 loss=241.058, ntokens=10329.6, nsentences=47.24, nll_loss=1.102, wps=2569.7, ups=0.25, wpb=10329.6, bsz=47.2, num_updates=17000, lr=1.22823e-05, gnorm=272.893, loss_scale=1, train_wall=680, gb_free=19.6, wall=0
[2024-09-18 04:52:35,059][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 04:52:35,060][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:52:35,076][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 162
[2024-09-18 04:52:55,397][valid][INFO] - epoch 518 | valid on 'valid' subset | loss 198.729 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.886 | uer 17.223 | wer 25.66 | raw_wer 25.66 | wps 4691.3 | wpb 1902.9 | bsz 8.5 | num_updates 17023 | best_wer 25.244
[2024-09-18 04:52:55,398][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 518 @ 17023 updates
[2024-09-18 04:52:55,399][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 04:52:58,817][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 04:52:58,891][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 518 @ 17023 updates, score 25.66) (writing took 3.493145979999099 seconds)
[2024-09-18 04:52:58,892][fairseq_cli.train][INFO] - end of epoch 518 (average epoch stats below)
[2024-09-18 04:52:58,893][train][INFO] - epoch 518 | loss 237.807 | ntokens 10343.6 | nsentences 47.2727 | nll_loss 1.087 | wps 2618.5 | ups 0.25 | wpb 10343.6 | bsz 47.3 | num_updates 17023 | lr 1.21979e-05 | gnorm 271.103 | loss_scale 1 | train_wall 150 | gb_free 20.2 | wall 0
[2024-09-18 04:52:58,894][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:52:58,909][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 519
[2024-09-18 04:52:58,922][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 04:52:58,954][fairseq.trainer][INFO] - begin training epoch 519
[2024-09-18 04:52:58,954][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 04:55:28,790][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 04:55:28,791][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:55:28,805][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 163
[2024-09-18 04:55:49,149][valid][INFO] - epoch 519 | valid on 'valid' subset | loss 198.317 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.884 | uer 17.367 | wer 25.557 | raw_wer 25.557 | wps 4668.2 | wpb 1902.9 | bsz 8.5 | num_updates 17067 | best_wer 25.244
[2024-09-18 04:55:49,150][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 519 @ 17067 updates
[2024-09-18 04:55:49,151][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 04:55:52,549][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 04:55:52,614][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 519 @ 17067 updates, score 25.557) (writing took 3.464359826000873 seconds)
[2024-09-18 04:55:52,615][fairseq_cli.train][INFO] - end of epoch 519 (average epoch stats below)
[2024-09-18 04:55:52,616][train][INFO] - epoch 519 | loss 245.057 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.12 | wps 2619.8 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 17067 | lr 1.20382e-05 | gnorm 286.144 | loss_scale 1 | train_wall 150 | gb_free 22.7 | wall 0
[2024-09-18 04:55:52,617][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:55:52,637][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 520
[2024-09-18 04:55:52,653][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 04:55:52,706][fairseq.trainer][INFO] - begin training epoch 520
[2024-09-18 04:55:52,706][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 04:58:22,642][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 04:58:22,642][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:58:22,658][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 164
[2024-09-18 04:58:42,969][valid][INFO] - epoch 520 | valid on 'valid' subset | loss 198.328 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.884 | uer 17.146 | wer 25.44 | raw_wer 25.44 | wps 4671 | wpb 1902.9 | bsz 8.5 | num_updates 17111 | best_wer 25.244
[2024-09-18 04:58:42,969][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 520 @ 17111 updates
[2024-09-18 04:58:42,970][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 04:58:46,340][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 04:58:46,412][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 520 @ 17111 updates, score 25.44) (writing took 3.4427491379974526 seconds)
[2024-09-18 04:58:46,413][fairseq_cli.train][INFO] - end of epoch 520 (average epoch stats below)
[2024-09-18 04:58:46,414][train][INFO] - epoch 520 | loss 238.155 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.088 | wps 2618.7 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 17111 | lr 1.18806e-05 | gnorm 276.611 | loss_scale 1 | train_wall 150 | gb_free 18.5 | wall 0
[2024-09-18 04:58:46,416][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 04:58:46,435][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 521
[2024-09-18 04:58:46,450][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 04:58:46,489][fairseq.trainer][INFO] - begin training epoch 521
[2024-09-18 04:58:46,490][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 05:01:15,928][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 05:01:15,928][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:01:15,944][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 165
[2024-09-18 05:01:36,361][valid][INFO] - epoch 521 | valid on 'valid' subset | loss 198.527 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.885 | uer 17.277 | wer 25.46 | raw_wer 25.46 | wps 4651.9 | wpb 1902.9 | bsz 8.5 | num_updates 17155 | best_wer 25.244
[2024-09-18 05:01:36,361][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 521 @ 17155 updates
[2024-09-18 05:01:36,362][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:01:39,760][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:01:39,817][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 521 @ 17155 updates, score 25.46) (writing took 3.4550782019941835 seconds)
[2024-09-18 05:01:39,817][fairseq_cli.train][INFO] - end of epoch 521 (average epoch stats below)
[2024-09-18 05:01:39,818][train][INFO] - epoch 521 | loss 243.117 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.111 | wps 2624.6 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 17155 | lr 1.1725e-05 | gnorm 280.001 | loss_scale 1 | train_wall 149 | gb_free 21.9 | wall 0
[2024-09-18 05:01:39,819][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:01:39,834][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 522
[2024-09-18 05:01:39,846][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 05:01:39,877][fairseq.trainer][INFO] - begin training epoch 522
[2024-09-18 05:01:39,878][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 05:04:09,682][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 05:04:09,683][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:04:09,698][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 166
[2024-09-18 05:04:29,993][valid][INFO] - epoch 522 | valid on 'valid' subset | loss 196.981 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.878 | uer 17.023 | wer 25.088 | raw_wer 25.088 | wps 4673.1 | wpb 1902.9 | bsz 8.5 | num_updates 17199 | best_wer 25.088
[2024-09-18 05:04:29,994][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 522 @ 17199 updates
[2024-09-18 05:04:29,995][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 05:04:33,290][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 05:04:35,234][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 522 @ 17199 updates, score 25.088) (writing took 5.2394462789961835 seconds)
[2024-09-18 05:04:35,234][fairseq_cli.train][INFO] - end of epoch 522 (average epoch stats below)
[2024-09-18 05:04:35,235][train][INFO] - epoch 522 | loss 240.768 | ntokens 10343.4 | nsentences 47.2727 | nll_loss 1.1 | wps 2594.5 | ups 0.25 | wpb 10343.4 | bsz 47.3 | num_updates 17199 | lr 1.15715e-05 | gnorm 284.773 | loss_scale 1 | train_wall 150 | gb_free 18.3 | wall 0
[2024-09-18 05:04:35,236][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:04:35,252][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 523
[2024-09-18 05:04:35,265][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 05:04:35,300][fairseq.trainer][INFO] - begin training epoch 523
[2024-09-18 05:04:35,300][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 05:04:38,999][train_inner][INFO] - epoch 523:      1 / 44 loss=241.391, ntokens=10349.4, nsentences=47.36, nll_loss=1.105, wps=2578, ups=0.25, wpb=10349.4, bsz=47.4, num_updates=17200, lr=1.1568e-05, gnorm=280.186, loss_scale=1, train_wall=681, gb_free=17, wall=0
[2024-09-18 05:07:04,806][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 05:07:04,807][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:07:04,822][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 167
[2024-09-18 05:07:25,181][valid][INFO] - epoch 523 | valid on 'valid' subset | loss 197.334 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.879 | uer 17.039 | wer 25.23 | raw_wer 25.23 | wps 4674.1 | wpb 1902.9 | bsz 8.5 | num_updates 17243 | best_wer 25.088
[2024-09-18 05:07:25,182][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 523 @ 17243 updates
[2024-09-18 05:07:25,183][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:07:28,575][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:07:28,651][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 523 @ 17243 updates, score 25.23) (writing took 3.4688337779953144 seconds)
[2024-09-18 05:07:28,651][fairseq_cli.train][INFO] - end of epoch 523 (average epoch stats below)
[2024-09-18 05:07:28,653][train][INFO] - epoch 523 | loss 237.553 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 1.086 | wps 2624.3 | ups 0.25 | wpb 10343.2 | bsz 47.3 | num_updates 17243 | lr 1.14199e-05 | gnorm 275.076 | loss_scale 1 | train_wall 149 | gb_free 18.2 | wall 0
[2024-09-18 05:07:28,654][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:07:28,670][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 524
[2024-09-18 05:07:28,682][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 05:07:28,713][fairseq.trainer][INFO] - begin training epoch 524
[2024-09-18 05:07:28,714][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 05:09:58,862][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 05:09:58,862][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:09:58,878][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 168
[2024-09-18 05:10:19,172][valid][INFO] - epoch 524 | valid on 'valid' subset | loss 197.163 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.879 | uer 16.944 | wer 25.044 | raw_wer 25.044 | wps 4690.8 | wpb 1902.9 | bsz 8.5 | num_updates 17287 | best_wer 25.044
[2024-09-18 05:10:19,173][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 524 @ 17287 updates
[2024-09-18 05:10:19,174][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 05:10:22,468][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 05:10:24,448][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 524 @ 17287 updates, score 25.044) (writing took 5.274390391998168 seconds)
[2024-09-18 05:10:24,448][fairseq_cli.train][INFO] - end of epoch 524 (average epoch stats below)
[2024-09-18 05:10:24,449][train][INFO] - epoch 524 | loss 237.455 | ntokens 10343.3 | nsentences 47.2727 | nll_loss 1.085 | wps 2588.8 | ups 0.25 | wpb 10343.3 | bsz 47.3 | num_updates 17287 | lr 1.12704e-05 | gnorm 269.44 | loss_scale 1 | train_wall 150 | gb_free 20.3 | wall 0
[2024-09-18 05:10:24,450][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:10:24,466][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 525
[2024-09-18 05:10:24,479][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 05:10:24,511][fairseq.trainer][INFO] - begin training epoch 525
[2024-09-18 05:10:24,511][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 05:12:53,381][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 05:12:53,382][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:12:53,396][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 169
[2024-09-18 05:13:13,728][valid][INFO] - epoch 525 | valid on 'valid' subset | loss 197.203 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.879 | uer 17.192 | wer 25.337 | raw_wer 25.337 | wps 4684 | wpb 1902.9 | bsz 8.5 | num_updates 17331 | best_wer 25.044
[2024-09-18 05:13:13,729][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 525 @ 17331 updates
[2024-09-18 05:13:13,730][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:13:17,090][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:13:17,159][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 525 @ 17331 updates, score 25.337) (writing took 3.4298164790016017 seconds)
[2024-09-18 05:13:17,159][fairseq_cli.train][INFO] - end of epoch 525 (average epoch stats below)
[2024-09-18 05:13:17,160][train][INFO] - epoch 525 | loss 238.803 | ntokens 10343.7 | nsentences 47.2727 | nll_loss 1.091 | wps 2635.2 | ups 0.25 | wpb 10343.7 | bsz 47.3 | num_updates 17331 | lr 1.11228e-05 | gnorm 269.544 | loss_scale 1 | train_wall 149 | gb_free 21.1 | wall 0
[2024-09-18 05:13:17,161][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:13:17,177][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 526
[2024-09-18 05:13:17,189][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 05:13:17,220][fairseq.trainer][INFO] - begin training epoch 526
[2024-09-18 05:13:17,220][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 05:15:47,499][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 05:15:47,499][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:15:47,514][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 170
[2024-09-18 05:16:07,676][valid][INFO] - epoch 526 | valid on 'valid' subset | loss 196.567 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.876 | uer 16.851 | wer 25.108 | raw_wer 25.108 | wps 4722.8 | wpb 1902.9 | bsz 8.5 | num_updates 17375 | best_wer 25.044
[2024-09-18 05:16:07,677][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 526 @ 17375 updates
[2024-09-18 05:16:07,678][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:16:11,087][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:16:11,159][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 526 @ 17375 updates, score 25.108) (writing took 3.482188269998005 seconds)
[2024-09-18 05:16:11,160][fairseq_cli.train][INFO] - end of epoch 526 (average epoch stats below)
[2024-09-18 05:16:11,161][train][INFO] - epoch 526 | loss 236.448 | ntokens 10343.7 | nsentences 47.2727 | nll_loss 1.081 | wps 2615.6 | ups 0.25 | wpb 10343.7 | bsz 47.3 | num_updates 17375 | lr 1.09772e-05 | gnorm 272.037 | loss_scale 1 | train_wall 150 | gb_free 18.1 | wall 0
[2024-09-18 05:16:11,162][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:16:11,177][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 527
[2024-09-18 05:16:11,189][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 05:16:11,222][fairseq.trainer][INFO] - begin training epoch 527
[2024-09-18 05:16:11,222][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 05:17:37,449][train_inner][INFO] - epoch 527:     25 / 44 loss=236.548, ntokens=10347.6, nsentences=47.48, nll_loss=1.085, wps=2658.5, ups=0.26, wpb=10347.6, bsz=47.5, num_updates=17400, lr=1.08953e-05, gnorm=270.015, loss_scale=1, train_wall=681, gb_free=18.5, wall=0
[2024-09-18 05:18:40,487][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 05:18:40,488][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:18:40,503][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 171
[2024-09-18 05:19:00,821][valid][INFO] - epoch 527 | valid on 'valid' subset | loss 197.318 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.879 | uer 16.93 | wer 25.196 | raw_wer 25.196 | wps 4690.3 | wpb 1902.9 | bsz 8.5 | num_updates 17419 | best_wer 25.044
[2024-09-18 05:19:00,822][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 527 @ 17419 updates
[2024-09-18 05:19:00,822][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:19:04,149][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:19:04,216][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 527 @ 17419 updates, score 25.196) (writing took 3.394019984996703 seconds)
[2024-09-18 05:19:04,216][fairseq_cli.train][INFO] - end of epoch 527 (average epoch stats below)
[2024-09-18 05:19:04,218][train][INFO] - epoch 527 | loss 241.877 | ntokens 10343.3 | nsentences 47.2727 | nll_loss 1.105 | wps 2629.8 | ups 0.25 | wpb 10343.3 | bsz 47.3 | num_updates 17419 | lr 1.08334e-05 | gnorm 270.559 | loss_scale 1 | train_wall 149 | gb_free 16.4 | wall 0
[2024-09-18 05:19:04,219][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:19:04,239][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 528
[2024-09-18 05:19:04,254][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 05:19:04,309][fairseq.trainer][INFO] - begin training epoch 528
[2024-09-18 05:19:04,309][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 05:21:33,912][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 05:21:33,913][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:21:33,928][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 172
[2024-09-18 05:21:54,150][valid][INFO] - epoch 528 | valid on 'valid' subset | loss 196.656 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.876 | uer 17.022 | wer 25.274 | raw_wer 25.274 | wps 4702.3 | wpb 1902.9 | bsz 8.5 | num_updates 17463 | best_wer 25.044
[2024-09-18 05:21:54,151][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 528 @ 17463 updates
[2024-09-18 05:21:54,152][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:21:57,517][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:21:57,574][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 528 @ 17463 updates, score 25.274) (writing took 3.4224619029992027 seconds)
[2024-09-18 05:21:57,574][fairseq_cli.train][INFO] - end of epoch 528 (average epoch stats below)
[2024-09-18 05:21:57,575][train][INFO] - epoch 528 | loss 241.964 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.106 | wps 2625.3 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 17463 | lr 1.06916e-05 | gnorm 279.715 | loss_scale 1 | train_wall 149 | gb_free 15.6 | wall 0
[2024-09-18 05:21:57,576][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:21:57,592][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 529
[2024-09-18 05:21:57,604][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 05:21:57,636][fairseq.trainer][INFO] - begin training epoch 529
[2024-09-18 05:21:57,637][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 05:24:27,450][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 05:24:27,451][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:24:27,466][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 173
[2024-09-18 05:24:47,822][valid][INFO] - epoch 529 | valid on 'valid' subset | loss 196.085 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.874 | uer 17.145 | wer 25.274 | raw_wer 25.274 | wps 4691.1 | wpb 1902.9 | bsz 8.5 | num_updates 17507 | best_wer 25.044
[2024-09-18 05:24:47,823][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 529 @ 17507 updates
[2024-09-18 05:24:47,824][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:24:51,110][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:24:51,180][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 529 @ 17507 updates, score 25.274) (writing took 3.3569981399996323 seconds)
[2024-09-18 05:24:51,181][fairseq_cli.train][INFO] - end of epoch 529 (average epoch stats below)
[2024-09-18 05:24:51,182][train][INFO] - epoch 529 | loss 241.357 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.103 | wps 2621.5 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 17507 | lr 1.05516e-05 | gnorm 283.594 | loss_scale 2 | train_wall 150 | gb_free 18.6 | wall 0
[2024-09-18 05:24:51,184][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:24:51,203][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 530
[2024-09-18 05:24:51,218][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 05:24:51,280][fairseq.trainer][INFO] - begin training epoch 530
[2024-09-18 05:24:51,281][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 05:27:21,862][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 05:27:21,863][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:27:21,889][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 174
[2024-09-18 05:27:42,248][valid][INFO] - epoch 530 | valid on 'valid' subset | loss 196.405 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.875 | uer 17.218 | wer 25.455 | raw_wer 25.455 | wps 4682.9 | wpb 1902.9 | bsz 8.5 | num_updates 17551 | best_wer 25.044
[2024-09-18 05:27:42,248][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 530 @ 17551 updates
[2024-09-18 05:27:42,249][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:27:45,582][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:27:45,640][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 530 @ 17551 updates, score 25.455) (writing took 3.3914771839990863 seconds)
[2024-09-18 05:27:45,640][fairseq_cli.train][INFO] - end of epoch 530 (average epoch stats below)
[2024-09-18 05:27:45,642][train][INFO] - epoch 530 | loss 240.216 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 1.098 | wps 2608.6 | ups 0.25 | wpb 10343.2 | bsz 47.3 | num_updates 17551 | lr 1.04134e-05 | gnorm 272.266 | loss_scale 2 | train_wall 150 | gb_free 17.2 | wall 0
[2024-09-18 05:27:45,642][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:27:45,658][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 531
[2024-09-18 05:27:45,674][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 05:27:45,707][fairseq.trainer][INFO] - begin training epoch 531
[2024-09-18 05:27:45,707][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 05:30:15,685][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 05:30:15,688][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:30:15,703][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 175
[2024-09-18 05:30:36,029][valid][INFO] - epoch 531 | valid on 'valid' subset | loss 196.548 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.876 | uer 17.014 | wer 25.332 | raw_wer 25.332 | wps 4679.3 | wpb 1902.9 | bsz 8.5 | num_updates 17595 | best_wer 25.044
[2024-09-18 05:30:36,030][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 531 @ 17595 updates
[2024-09-18 05:30:36,031][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:30:39,290][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:30:39,347][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 531 @ 17595 updates, score 25.332) (writing took 3.3169718150020344 seconds)
[2024-09-18 05:30:39,348][fairseq_cli.train][INFO] - end of epoch 531 (average epoch stats below)
[2024-09-18 05:30:39,349][train][INFO] - epoch 531 | loss 232.599 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.063 | wps 2620 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 17595 | lr 1.0277e-05 | gnorm 262.844 | loss_scale 2 | train_wall 150 | gb_free 18.9 | wall 0
[2024-09-18 05:30:39,349][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:30:39,365][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 532
[2024-09-18 05:30:39,377][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 05:30:39,415][fairseq.trainer][INFO] - begin training epoch 532
[2024-09-18 05:30:39,415][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 05:30:57,085][train_inner][INFO] - epoch 532:      5 / 44 loss=241.117, ntokens=10327.2, nsentences=46.92, nll_loss=1.095, wps=2583, ups=0.25, wpb=10327.2, bsz=46.9, num_updates=17600, lr=1.02617e-05, gnorm=275.29, loss_scale=2, train_wall=680, gb_free=19.4, wall=0
[2024-09-18 05:33:09,646][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 05:33:09,647][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:33:09,662][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 176
[2024-09-18 05:33:30,005][valid][INFO] - epoch 532 | valid on 'valid' subset | loss 195.785 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.872 | uer 16.964 | wer 25.098 | raw_wer 25.098 | wps 4672.1 | wpb 1902.9 | bsz 8.5 | num_updates 17639 | best_wer 25.044
[2024-09-18 05:33:30,017][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 532 @ 17639 updates
[2024-09-18 05:33:30,018][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:33:33,381][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:33:33,451][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 532 @ 17639 updates, score 25.098) (writing took 3.4333414680004353 seconds)
[2024-09-18 05:33:33,451][fairseq_cli.train][INFO] - end of epoch 532 (average epoch stats below)
[2024-09-18 05:33:33,453][train][INFO] - epoch 532 | loss 233.603 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 1.068 | wps 2614 | ups 0.25 | wpb 10343.2 | bsz 47.3 | num_updates 17639 | lr 1.01425e-05 | gnorm 266.93 | loss_scale 2 | train_wall 150 | gb_free 18.7 | wall 0
[2024-09-18 05:33:33,454][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:33:33,474][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 533
[2024-09-18 05:33:33,495][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 05:33:33,541][fairseq.trainer][INFO] - begin training epoch 533
[2024-09-18 05:33:33,542][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 05:36:01,939][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 05:36:01,940][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:36:01,955][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 177
[2024-09-18 05:36:22,246][valid][INFO] - epoch 533 | valid on 'valid' subset | loss 196.001 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.873 | uer 17.1 | wer 25.636 | raw_wer 25.636 | wps 4689.6 | wpb 1902.9 | bsz 8.5 | num_updates 17683 | best_wer 25.044
[2024-09-18 05:36:22,247][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 533 @ 17683 updates
[2024-09-18 05:36:22,247][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:36:25,623][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:36:25,692][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 533 @ 17683 updates, score 25.636) (writing took 3.445082067999465 seconds)
[2024-09-18 05:36:25,693][fairseq_cli.train][INFO] - end of epoch 533 (average epoch stats below)
[2024-09-18 05:36:25,694][train][INFO] - epoch 533 | loss 241.798 | ntokens 10343.3 | nsentences 47.2727 | nll_loss 1.105 | wps 2642.3 | ups 0.26 | wpb 10343.3 | bsz 47.3 | num_updates 17683 | lr 1.00096e-05 | gnorm 275.666 | loss_scale 2 | train_wall 148 | gb_free 21.1 | wall 0
[2024-09-18 05:36:25,695][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:36:25,715][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 534
[2024-09-18 05:36:25,730][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 05:36:25,772][fairseq.trainer][INFO] - begin training epoch 534
[2024-09-18 05:36:25,773][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 05:38:56,450][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 05:38:56,450][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:38:56,465][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 178
[2024-09-18 05:39:16,793][valid][INFO] - epoch 534 | valid on 'valid' subset | loss 195.941 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.873 | uer 16.975 | wer 25.147 | raw_wer 25.147 | wps 4683.7 | wpb 1902.9 | bsz 8.5 | num_updates 17727 | best_wer 25.044
[2024-09-18 05:39:16,793][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 534 @ 17727 updates
[2024-09-18 05:39:16,794][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:39:20,159][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:39:20,216][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 534 @ 17727 updates, score 25.147) (writing took 3.4220902910019504 seconds)
[2024-09-18 05:39:20,216][fairseq_cli.train][INFO] - end of epoch 534 (average epoch stats below)
[2024-09-18 05:39:20,217][train][INFO] - epoch 534 | loss 238.681 | ntokens 10343.6 | nsentences 47.2727 | nll_loss 1.091 | wps 2607.8 | ups 0.25 | wpb 10343.6 | bsz 47.3 | num_updates 17727 | lr 9.87857e-06 | gnorm 284.997 | loss_scale 2 | train_wall 151 | gb_free 18.2 | wall 0
[2024-09-18 05:39:20,218][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:39:20,233][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 535
[2024-09-18 05:39:20,250][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 05:39:20,291][fairseq.trainer][INFO] - begin training epoch 535
[2024-09-18 05:39:20,292][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 05:41:50,456][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 05:41:50,457][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:41:50,472][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 179
[2024-09-18 05:42:10,879][valid][INFO] - epoch 535 | valid on 'valid' subset | loss 194.849 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.868 | uer 17.025 | wer 25.288 | raw_wer 25.288 | wps 4659.3 | wpb 1902.9 | bsz 8.5 | num_updates 17771 | best_wer 25.044
[2024-09-18 05:42:10,880][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 535 @ 17771 updates
[2024-09-18 05:42:10,881][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:42:14,265][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:42:14,324][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 535 @ 17771 updates, score 25.288) (writing took 3.44425400000182 seconds)
[2024-09-18 05:42:14,325][fairseq_cli.train][INFO] - end of epoch 535 (average epoch stats below)
[2024-09-18 05:42:14,326][train][INFO] - epoch 535 | loss 239.916 | ntokens 10343.8 | nsentences 47.2727 | nll_loss 1.096 | wps 2614 | ups 0.25 | wpb 10343.8 | bsz 47.3 | num_updates 17771 | lr 9.74921e-06 | gnorm 278.619 | loss_scale 2 | train_wall 150 | gb_free 17.5 | wall 0
[2024-09-18 05:42:14,327][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:42:14,343][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 536
[2024-09-18 05:42:14,355][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 05:42:14,390][fairseq.trainer][INFO] - begin training epoch 536
[2024-09-18 05:42:14,390][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 05:43:53,696][train_inner][INFO] - epoch 536:     29 / 44 loss=237.727, ntokens=10351.1, nsentences=47.56, nll_loss=1.092, wps=2665.7, ups=0.26, wpb=10351.1, bsz=47.6, num_updates=17800, lr=9.66488e-06, gnorm=277.095, loss_scale=2, train_wall=680, gb_free=18, wall=0
[2024-09-18 05:44:44,684][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 05:44:44,685][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:44:44,701][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 180
[2024-09-18 05:45:05,022][valid][INFO] - epoch 536 | valid on 'valid' subset | loss 195.014 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.869 | uer 16.866 | wer 25.337 | raw_wer 25.337 | wps 4704.3 | wpb 1902.9 | bsz 8.5 | num_updates 17815 | best_wer 25.044
[2024-09-18 05:45:05,022][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 536 @ 17815 updates
[2024-09-18 05:45:05,023][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:45:08,389][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:45:08,445][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 536 @ 17815 updates, score 25.337) (writing took 3.422155281994492 seconds)
[2024-09-18 05:45:08,445][fairseq_cli.train][INFO] - end of epoch 536 (average epoch stats below)
[2024-09-18 05:45:08,446][train][INFO] - epoch 536 | loss 240.773 | ntokens 10343.8 | nsentences 47.2727 | nll_loss 1.1 | wps 2613.9 | ups 0.25 | wpb 10343.8 | bsz 47.3 | num_updates 17815 | lr 9.62155e-06 | gnorm 273.286 | loss_scale 2 | train_wall 150 | gb_free 21 | wall 0
[2024-09-18 05:45:08,447][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:45:08,462][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 537
[2024-09-18 05:45:08,474][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 05:45:08,511][fairseq.trainer][INFO] - begin training epoch 537
[2024-09-18 05:45:08,511][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 05:47:39,158][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 05:47:39,158][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:47:39,173][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 181
[2024-09-18 05:47:59,519][valid][INFO] - epoch 537 | valid on 'valid' subset | loss 195.186 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.87 | uer 16.722 | wer 25.02 | raw_wer 25.02 | wps 4685.2 | wpb 1902.9 | bsz 8.5 | num_updates 17859 | best_wer 25.02
[2024-09-18 05:47:59,520][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 537 @ 17859 updates
[2024-09-18 05:47:59,520][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 05:48:02,819][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 05:48:04,755][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 537 @ 17859 updates, score 25.02) (writing took 5.235686524996709 seconds)
[2024-09-18 05:48:04,756][fairseq_cli.train][INFO] - end of epoch 537 (average epoch stats below)
[2024-09-18 05:48:04,757][train][INFO] - epoch 537 | loss 232.439 | ntokens 10343 | nsentences 47.2727 | nll_loss 1.062 | wps 2581.2 | ups 0.25 | wpb 10343 | bsz 47.3 | num_updates 17859 | lr 9.49556e-06 | gnorm 271.402 | loss_scale 2 | train_wall 150 | gb_free 20.6 | wall 0
[2024-09-18 05:48:04,758][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:48:04,773][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 538
[2024-09-18 05:48:04,785][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 05:48:04,819][fairseq.trainer][INFO] - begin training epoch 538
[2024-09-18 05:48:04,819][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 05:50:34,521][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 05:50:34,522][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:50:34,537][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 182
[2024-09-18 05:50:54,798][valid][INFO] - epoch 538 | valid on 'valid' subset | loss 194.578 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.867 | uer 16.789 | wer 25.078 | raw_wer 25.078 | wps 4685.9 | wpb 1902.9 | bsz 8.5 | num_updates 17903 | best_wer 25.02
[2024-09-18 05:50:54,799][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 538 @ 17903 updates
[2024-09-18 05:50:54,800][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:50:58,182][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:50:58,258][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 538 @ 17903 updates, score 25.078) (writing took 3.4585056389987585 seconds)
[2024-09-18 05:50:58,258][fairseq_cli.train][INFO] - end of epoch 538 (average epoch stats below)
[2024-09-18 05:50:58,259][train][INFO] - epoch 538 | loss 239.872 | ntokens 10343.3 | nsentences 47.2727 | nll_loss 1.096 | wps 2623.1 | ups 0.25 | wpb 10343.3 | bsz 47.3 | num_updates 17903 | lr 9.37122e-06 | gnorm 269.992 | loss_scale 2 | train_wall 150 | gb_free 20 | wall 0
[2024-09-18 05:50:58,260][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:50:58,276][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 539
[2024-09-18 05:50:58,291][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 05:50:58,324][fairseq.trainer][INFO] - begin training epoch 539
[2024-09-18 05:50:58,325][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 05:53:27,558][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 05:53:27,559][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:53:27,574][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 183
[2024-09-18 05:53:47,900][valid][INFO] - epoch 539 | valid on 'valid' subset | loss 195.018 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.869 | uer 16.893 | wer 24.956 | raw_wer 24.956 | wps 4688.4 | wpb 1902.9 | bsz 8.5 | num_updates 17947 | best_wer 24.956
[2024-09-18 05:53:47,900][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 539 @ 17947 updates
[2024-09-18 05:53:47,901][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 05:53:51,126][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 05:53:53,104][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 539 @ 17947 updates, score 24.956) (writing took 5.203274671002873 seconds)
[2024-09-18 05:53:53,104][fairseq_cli.train][INFO] - end of epoch 539 (average epoch stats below)
[2024-09-18 05:53:53,105][train][INFO] - epoch 539 | loss 238.95 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.092 | wps 2603 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 17947 | lr 9.2485e-06 | gnorm 273.7 | loss_scale 2 | train_wall 149 | gb_free 18.5 | wall 0
[2024-09-18 05:53:53,106][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:53:53,121][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 540
[2024-09-18 05:53:53,134][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 05:53:53,168][fairseq.trainer][INFO] - begin training epoch 540
[2024-09-18 05:53:53,168][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 05:56:23,433][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 05:56:23,433][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:56:23,448][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 184
[2024-09-18 05:56:43,879][valid][INFO] - epoch 540 | valid on 'valid' subset | loss 194.705 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.868 | uer 16.733 | wer 25.015 | raw_wer 25.015 | wps 4652.9 | wpb 1902.9 | bsz 8.5 | num_updates 17991 | best_wer 24.956
[2024-09-18 05:56:43,880][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 540 @ 17991 updates
[2024-09-18 05:56:43,881][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:56:47,154][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:56:47,228][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 540 @ 17991 updates, score 25.015) (writing took 3.347645780995663 seconds)
[2024-09-18 05:56:47,228][fairseq_cli.train][INFO] - end of epoch 540 (average epoch stats below)
[2024-09-18 05:56:47,229][train][INFO] - epoch 540 | loss 240.36 | ntokens 10343.8 | nsentences 47.2727 | nll_loss 1.098 | wps 2613.8 | ups 0.25 | wpb 10343.8 | bsz 47.3 | num_updates 17991 | lr 9.1274e-06 | gnorm 272.714 | loss_scale 2 | train_wall 150 | gb_free 20.7 | wall 0
[2024-09-18 05:56:47,230][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:56:47,246][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 541
[2024-09-18 05:56:47,258][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 05:56:47,292][fairseq.trainer][INFO] - begin training epoch 541
[2024-09-18 05:56:47,292][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 05:57:18,407][train_inner][INFO] - epoch 541:      9 / 44 loss=237.944, ntokens=10346.4, nsentences=47.36, nll_loss=1.089, wps=2571.5, ups=0.25, wpb=10346.4, bsz=47.4, num_updates=18000, lr=9.10282e-06, gnorm=271.092, loss_scale=2, train_wall=681, gb_free=18.9, wall=0
[2024-09-18 05:59:17,035][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 05:59:17,036][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:59:17,051][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 185
[2024-09-18 05:59:37,483][valid][INFO] - epoch 541 | valid on 'valid' subset | loss 193.812 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.864 | uer 16.96 | wer 25.044 | raw_wer 25.044 | wps 4642.6 | wpb 1902.9 | bsz 8.5 | num_updates 18035 | best_wer 24.956
[2024-09-18 05:59:37,484][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 541 @ 18035 updates
[2024-09-18 05:59:37,484][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:59:40,867][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 05:59:40,943][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 541 @ 18035 updates, score 25.044) (writing took 3.4588551519991597 seconds)
[2024-09-18 05:59:40,943][fairseq_cli.train][INFO] - end of epoch 541 (average epoch stats below)
[2024-09-18 05:59:40,945][train][INFO] - epoch 541 | loss 240.379 | ntokens 10343.3 | nsentences 47.2727 | nll_loss 1.099 | wps 2619.8 | ups 0.25 | wpb 10343.3 | bsz 47.3 | num_updates 18035 | lr 9.00788e-06 | gnorm 276.903 | loss_scale 2 | train_wall 150 | gb_free 16.4 | wall 0
[2024-09-18 05:59:40,945][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 05:59:40,961][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 542
[2024-09-18 05:59:40,974][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 05:59:41,008][fairseq.trainer][INFO] - begin training epoch 542
[2024-09-18 05:59:41,009][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 06:02:09,946][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 06:02:09,946][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:02:09,962][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 186
[2024-09-18 06:02:30,320][valid][INFO] - epoch 542 | valid on 'valid' subset | loss 194.028 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.865 | uer 16.833 | wer 25.01 | raw_wer 25.01 | wps 4676.4 | wpb 1902.9 | bsz 8.5 | num_updates 18079 | best_wer 24.956
[2024-09-18 06:02:30,320][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 542 @ 18079 updates
[2024-09-18 06:02:30,321][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:02:33,676][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:02:33,732][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 542 @ 18079 updates, score 25.01) (writing took 3.4117009460023837 seconds)
[2024-09-18 06:02:33,733][fairseq_cli.train][INFO] - end of epoch 542 (average epoch stats below)
[2024-09-18 06:02:33,734][train][INFO] - epoch 542 | loss 243.321 | ntokens 10343.8 | nsentences 47.2727 | nll_loss 1.112 | wps 2634 | ups 0.25 | wpb 10343.8 | bsz 47.3 | num_updates 18079 | lr 8.88992e-06 | gnorm 282.715 | loss_scale 2 | train_wall 149 | gb_free 19 | wall 0
[2024-09-18 06:02:33,734][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:02:33,750][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 543
[2024-09-18 06:02:33,763][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 06:02:33,813][fairseq.trainer][INFO] - begin training epoch 543
[2024-09-18 06:02:33,813][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 06:05:04,136][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 06:05:04,136][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:05:04,152][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 187
[2024-09-18 06:05:24,428][valid][INFO] - epoch 543 | valid on 'valid' subset | loss 194.021 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.865 | uer 16.844 | wer 24.927 | raw_wer 24.927 | wps 4706.3 | wpb 1902.9 | bsz 8.5 | num_updates 18123 | best_wer 24.927
[2024-09-18 06:05:24,429][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 543 @ 18123 updates
[2024-09-18 06:05:24,430][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 06:05:27,729][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 06:05:29,657][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 543 @ 18123 updates, score 24.927) (writing took 5.227954571004375 seconds)
[2024-09-18 06:05:29,658][fairseq_cli.train][INFO] - end of epoch 543 (average epoch stats below)
[2024-09-18 06:05:29,659][train][INFO] - epoch 543 | loss 231.023 | ntokens 10344 | nsentences 47.2727 | nll_loss 1.056 | wps 2587.1 | ups 0.25 | wpb 10344 | bsz 47.3 | num_updates 18123 | lr 8.77351e-06 | gnorm 255.051 | loss_scale 2 | train_wall 150 | gb_free 19.1 | wall 0
[2024-09-18 06:05:29,660][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:05:29,675][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 544
[2024-09-18 06:05:29,687][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 06:05:29,728][fairseq.trainer][INFO] - begin training epoch 544
[2024-09-18 06:05:29,728][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 06:08:00,341][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 06:08:00,341][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:08:00,356][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 188
[2024-09-18 06:08:20,595][valid][INFO] - epoch 544 | valid on 'valid' subset | loss 194.412 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.866 | uer 16.781 | wer 24.985 | raw_wer 24.985 | wps 4704.1 | wpb 1902.9 | bsz 8.5 | num_updates 18167 | best_wer 24.927
[2024-09-18 06:08:20,596][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 544 @ 18167 updates
[2024-09-18 06:08:20,596][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:08:23,975][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:08:24,049][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 544 @ 18167 updates, score 24.985) (writing took 3.453385654000158 seconds)
[2024-09-18 06:08:24,049][fairseq_cli.train][INFO] - end of epoch 544 (average epoch stats below)
[2024-09-18 06:08:24,051][train][INFO] - epoch 544 | loss 234.837 | ntokens 10343.3 | nsentences 47.2727 | nll_loss 1.073 | wps 2609.7 | ups 0.25 | wpb 10343.3 | bsz 47.3 | num_updates 18167 | lr 8.65862e-06 | gnorm 257.838 | loss_scale 2 | train_wall 150 | gb_free 18.5 | wall 0
[2024-09-18 06:08:24,052][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:08:24,067][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 545
[2024-09-18 06:08:24,082][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 06:08:24,117][fairseq.trainer][INFO] - begin training epoch 545
[2024-09-18 06:08:24,117][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 06:10:16,497][train_inner][INFO] - epoch 545:     33 / 44 loss=236.571, ntokens=10342.7, nsentences=47.48, nll_loss=1.086, wps=2658.5, ups=0.26, wpb=10342.7, bsz=47.5, num_updates=18200, lr=8.57345e-06, gnorm=265.994, loss_scale=2, train_wall=680, gb_free=18.7, wall=0
[2024-09-18 06:10:54,475][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 06:10:54,476][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:10:54,491][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 189
[2024-09-18 06:11:14,801][valid][INFO] - epoch 545 | valid on 'valid' subset | loss 194.351 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.866 | uer 16.774 | wer 24.99 | raw_wer 24.99 | wps 4672.2 | wpb 1902.9 | bsz 8.5 | num_updates 18211 | best_wer 24.927
[2024-09-18 06:11:14,802][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 545 @ 18211 updates
[2024-09-18 06:11:14,802][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:11:18,160][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:11:18,230][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 545 @ 18211 updates, score 24.99) (writing took 3.428677275995142 seconds)
[2024-09-18 06:11:18,231][fairseq_cli.train][INFO] - end of epoch 545 (average epoch stats below)
[2024-09-18 06:11:18,232][train][INFO] - epoch 545 | loss 234.915 | ntokens 10343.4 | nsentences 47.2727 | nll_loss 1.074 | wps 2612.9 | ups 0.25 | wpb 10343.4 | bsz 47.3 | num_updates 18211 | lr 8.54524e-06 | gnorm 257.798 | loss_scale 2 | train_wall 150 | gb_free 17.6 | wall 0
[2024-09-18 06:11:18,233][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:11:18,248][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 546
[2024-09-18 06:11:18,260][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 06:11:18,296][fairseq.trainer][INFO] - begin training epoch 546
[2024-09-18 06:11:18,296][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 06:13:48,522][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 06:13:48,523][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:13:48,539][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 190
[2024-09-18 06:14:08,890][valid][INFO] - epoch 546 | valid on 'valid' subset | loss 193.899 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.864 | uer 16.717 | wer 24.99 | raw_wer 24.99 | wps 4666.2 | wpb 1902.9 | bsz 8.5 | num_updates 18255 | best_wer 24.927
[2024-09-18 06:14:08,891][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 546 @ 18255 updates
[2024-09-18 06:14:08,892][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:14:12,117][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:14:12,175][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 546 @ 18255 updates, score 24.99) (writing took 3.283578763999685 seconds)
[2024-09-18 06:14:12,175][fairseq_cli.train][INFO] - end of epoch 546 (average epoch stats below)
[2024-09-18 06:14:12,176][train][INFO] - epoch 546 | loss 232.284 | ntokens 10343.6 | nsentences 47.2727 | nll_loss 1.062 | wps 2616.5 | ups 0.25 | wpb 10343.6 | bsz 47.3 | num_updates 18255 | lr 8.43334e-06 | gnorm 264.287 | loss_scale 2 | train_wall 150 | gb_free 18.9 | wall 0
[2024-09-18 06:14:12,177][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:14:12,193][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 547
[2024-09-18 06:14:12,205][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 06:14:12,249][fairseq.trainer][INFO] - begin training epoch 547
[2024-09-18 06:14:12,249][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 06:16:41,684][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 06:16:41,685][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:16:41,701][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 191
[2024-09-18 06:17:02,142][valid][INFO] - epoch 547 | valid on 'valid' subset | loss 193.593 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.863 | uer 16.851 | wer 24.917 | raw_wer 24.917 | wps 4654.6 | wpb 1902.9 | bsz 8.5 | num_updates 18299 | best_wer 24.917
[2024-09-18 06:17:02,143][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 547 @ 18299 updates
[2024-09-18 06:17:02,143][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 06:17:05,313][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 06:17:07,200][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 547 @ 18299 updates, score 24.917) (writing took 5.057266896001238 seconds)
[2024-09-18 06:17:07,201][fairseq_cli.train][INFO] - end of epoch 547 (average epoch stats below)
[2024-09-18 06:17:07,202][train][INFO] - epoch 547 | loss 239.14 | ntokens 10343.8 | nsentences 47.2727 | nll_loss 1.093 | wps 2600.4 | ups 0.25 | wpb 10343.8 | bsz 47.3 | num_updates 18299 | lr 8.32291e-06 | gnorm 283.6 | loss_scale 2 | train_wall 149 | gb_free 21.1 | wall 0
[2024-09-18 06:17:07,202][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:17:07,218][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 548
[2024-09-18 06:17:07,231][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 06:17:07,278][fairseq.trainer][INFO] - begin training epoch 548
[2024-09-18 06:17:07,278][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 06:19:38,119][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 06:19:38,120][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:19:38,135][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 192
[2024-09-18 06:19:58,444][valid][INFO] - epoch 548 | valid on 'valid' subset | loss 193.923 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.864 | uer 16.789 | wer 25 | raw_wer 25 | wps 4687.6 | wpb 1902.9 | bsz 8.5 | num_updates 18343 | best_wer 24.917
[2024-09-18 06:19:58,445][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 548 @ 18343 updates
[2024-09-18 06:19:58,445][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:20:01,842][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:20:01,915][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 548 @ 18343 updates, score 25.0) (writing took 3.469915488996776 seconds)
[2024-09-18 06:20:01,915][fairseq_cli.train][INFO] - end of epoch 548 (average epoch stats below)
[2024-09-18 06:20:01,916][train][INFO] - epoch 548 | loss 232.131 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.061 | wps 2604.9 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 18343 | lr 8.21392e-06 | gnorm 269.109 | loss_scale 2 | train_wall 151 | gb_free 16.9 | wall 0
[2024-09-18 06:20:01,917][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:20:01,932][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 549
[2024-09-18 06:20:01,945][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 06:20:01,980][fairseq.trainer][INFO] - begin training epoch 549
[2024-09-18 06:20:01,981][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 06:22:31,953][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 06:22:31,954][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:22:31,969][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 193
[2024-09-18 06:22:52,297][valid][INFO] - epoch 549 | valid on 'valid' subset | loss 193.733 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.863 | uer 16.736 | wer 24.941 | raw_wer 24.941 | wps 4684.6 | wpb 1902.9 | bsz 8.5 | num_updates 18387 | best_wer 24.917
[2024-09-18 06:22:52,298][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 549 @ 18387 updates
[2024-09-18 06:22:52,298][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:22:55,646][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:22:55,716][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 549 @ 18387 updates, score 24.941) (writing took 3.4185240299993893 seconds)
[2024-09-18 06:22:55,717][fairseq_cli.train][INFO] - end of epoch 549 (average epoch stats below)
[2024-09-18 06:22:55,718][train][INFO] - epoch 549 | loss 234.016 | ntokens 10343.7 | nsentences 47.2727 | nll_loss 1.07 | wps 2618.6 | ups 0.25 | wpb 10343.7 | bsz 47.3 | num_updates 18387 | lr 8.10636e-06 | gnorm 267.074 | loss_scale 2 | train_wall 150 | gb_free 19.5 | wall 0
[2024-09-18 06:22:55,719][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:22:55,735][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 550
[2024-09-18 06:22:55,747][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 06:22:55,783][fairseq.trainer][INFO] - begin training epoch 550
[2024-09-18 06:22:55,783][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 06:23:39,953][train_inner][INFO] - epoch 550:     13 / 44 loss=234.301, ntokens=10321.1, nsentences=47.16, nll_loss=1.071, wps=2569.2, ups=0.25, wpb=10321.1, bsz=47.2, num_updates=18400, lr=8.07486e-06, gnorm=268.56, loss_scale=2, train_wall=682, gb_free=21.2, wall=0
[2024-09-18 06:25:25,270][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 06:25:25,271][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:25:25,286][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 194
[2024-09-18 06:25:45,725][valid][INFO] - epoch 550 | valid on 'valid' subset | loss 193.283 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.861 | uer 16.802 | wer 25.059 | raw_wer 25.059 | wps 4661.8 | wpb 1902.9 | bsz 8.5 | num_updates 18431 | best_wer 24.917
[2024-09-18 06:25:45,725][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 550 @ 18431 updates
[2024-09-18 06:25:45,726][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:25:49,022][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:25:49,078][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 550 @ 18431 updates, score 25.059) (writing took 3.352344801001891 seconds)
[2024-09-18 06:25:49,078][fairseq_cli.train][INFO] - end of epoch 550 (average epoch stats below)
[2024-09-18 06:25:49,079][train][INFO] - epoch 550 | loss 240.634 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.1 | wps 2625.2 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 18431 | lr 8.00021e-06 | gnorm 273.806 | loss_scale 2 | train_wall 149 | gb_free 23.1 | wall 0
[2024-09-18 06:25:49,080][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:25:49,096][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 551
[2024-09-18 06:25:49,113][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 06:25:49,169][fairseq.trainer][INFO] - begin training epoch 551
[2024-09-18 06:25:49,169][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 06:28:17,977][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 06:28:17,978][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:28:17,992][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 195
[2024-09-18 06:28:38,378][valid][INFO] - epoch 551 | valid on 'valid' subset | loss 193.876 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.864 | uer 16.833 | wer 25.156 | raw_wer 25.156 | wps 4667.3 | wpb 1902.9 | bsz 8.5 | num_updates 18475 | best_wer 24.917
[2024-09-18 06:28:38,378][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 551 @ 18475 updates
[2024-09-18 06:28:38,379][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:28:41,772][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:28:41,829][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 551 @ 18475 updates, score 25.156) (writing took 3.4501028320009937 seconds)
[2024-09-18 06:28:41,829][fairseq_cli.train][INFO] - end of epoch 551 (average epoch stats below)
[2024-09-18 06:28:41,830][train][INFO] - epoch 551 | loss 237.784 | ntokens 10343.8 | nsentences 47.2727 | nll_loss 1.087 | wps 2634.6 | ups 0.25 | wpb 10343.8 | bsz 47.3 | num_updates 18475 | lr 7.89545e-06 | gnorm 273.837 | loss_scale 2 | train_wall 149 | gb_free 17.5 | wall 0
[2024-09-18 06:28:41,831][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:28:41,850][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 552
[2024-09-18 06:28:41,866][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 06:28:41,924][fairseq.trainer][INFO] - begin training epoch 552
[2024-09-18 06:28:41,924][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 06:31:11,406][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 06:31:11,406][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:31:11,421][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 196
[2024-09-18 06:31:31,866][valid][INFO] - epoch 552 | valid on 'valid' subset | loss 193.147 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.861 | uer 16.869 | wer 24.951 | raw_wer 24.951 | wps 4653.8 | wpb 1902.9 | bsz 8.5 | num_updates 18519 | best_wer 24.917
[2024-09-18 06:31:31,867][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 552 @ 18519 updates
[2024-09-18 06:31:31,867][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:31:35,225][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:31:35,282][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 552 @ 18519 updates, score 24.951) (writing took 3.4156183110026177 seconds)
[2024-09-18 06:31:35,283][fairseq_cli.train][INFO] - end of epoch 552 (average epoch stats below)
[2024-09-18 06:31:35,284][train][INFO] - epoch 552 | loss 229.093 | ntokens 10343.3 | nsentences 47.2727 | nll_loss 1.047 | wps 2623.8 | ups 0.25 | wpb 10343.3 | bsz 47.3 | num_updates 18519 | lr 7.79206e-06 | gnorm 259.747 | loss_scale 2 | train_wall 149 | gb_free 18.6 | wall 0
[2024-09-18 06:31:35,285][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:31:35,300][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 553
[2024-09-18 06:31:35,313][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 06:31:35,355][fairseq.trainer][INFO] - begin training epoch 553
[2024-09-18 06:31:35,356][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 06:34:05,550][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 06:34:05,551][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:34:05,566][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 197
[2024-09-18 06:34:25,948][valid][INFO] - epoch 553 | valid on 'valid' subset | loss 193.324 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.862 | uer 16.749 | wer 25.039 | raw_wer 25.039 | wps 4667.1 | wpb 1902.9 | bsz 8.5 | num_updates 18563 | best_wer 24.917
[2024-09-18 06:34:25,949][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 553 @ 18563 updates
[2024-09-18 06:34:25,949][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:34:29,327][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:34:29,397][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 553 @ 18563 updates, score 25.039) (writing took 3.447941793005157 seconds)
[2024-09-18 06:34:29,398][fairseq_cli.train][INFO] - end of epoch 553 (average epoch stats below)
[2024-09-18 06:34:29,399][train][INFO] - epoch 553 | loss 229.331 | ntokens 10344 | nsentences 47.2727 | nll_loss 1.048 | wps 2614 | ups 0.25 | wpb 10344 | bsz 47.3 | num_updates 18563 | lr 7.69003e-06 | gnorm 263.997 | loss_scale 2 | train_wall 150 | gb_free 20.1 | wall 0
[2024-09-18 06:34:29,400][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:34:29,419][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 554
[2024-09-18 06:34:29,434][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 06:34:29,485][fairseq.trainer][INFO] - begin training epoch 554
[2024-09-18 06:34:29,485][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 06:36:35,450][train_inner][INFO] - epoch 554:     37 / 44 loss=235.458, ntokens=10354.3, nsentences=47.04, nll_loss=1.07, wps=2670.4, ups=0.26, wpb=10354.3, bsz=47, num_updates=18600, lr=7.60526e-06, gnorm=269.103, loss_scale=2, train_wall=679, gb_free=20.3, wall=0
[2024-09-18 06:36:58,885][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 06:36:58,886][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:36:58,901][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 198
[2024-09-18 06:37:19,159][valid][INFO] - epoch 554 | valid on 'valid' subset | loss 193.008 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.86 | uer 16.779 | wer 24.99 | raw_wer 24.99 | wps 4698 | wpb 1902.9 | bsz 8.5 | num_updates 18607 | best_wer 24.917
[2024-09-18 06:37:19,159][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 554 @ 18607 updates
[2024-09-18 06:37:19,160][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:37:22,540][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:37:22,597][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 554 @ 18607 updates, score 24.99) (writing took 3.437888729997212 seconds)
[2024-09-18 06:37:22,598][fairseq_cli.train][INFO] - end of epoch 554 (average epoch stats below)
[2024-09-18 06:37:22,599][train][INFO] - epoch 554 | loss 233.937 | ntokens 10343.4 | nsentences 47.2727 | nll_loss 1.069 | wps 2627.7 | ups 0.25 | wpb 10343.4 | bsz 47.3 | num_updates 18607 | lr 7.58933e-06 | gnorm 263.066 | loss_scale 2 | train_wall 149 | gb_free 19.2 | wall 0
[2024-09-18 06:37:22,600][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:37:22,622][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 555
[2024-09-18 06:37:22,637][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 06:37:22,686][fairseq.trainer][INFO] - begin training epoch 555
[2024-09-18 06:37:22,687][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 06:39:52,112][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 06:39:52,112][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:39:52,128][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 199
[2024-09-18 06:40:12,503][valid][INFO] - epoch 555 | valid on 'valid' subset | loss 192.733 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.859 | uer 16.718 | wer 24.985 | raw_wer 24.985 | wps 4672.3 | wpb 1902.9 | bsz 8.5 | num_updates 18651 | best_wer 24.917
[2024-09-18 06:40:12,504][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 555 @ 18651 updates
[2024-09-18 06:40:12,504][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:40:15,822][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:40:15,891][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 555 @ 18651 updates, score 24.985) (writing took 3.3877572839992354 seconds)
[2024-09-18 06:40:15,892][fairseq_cli.train][INFO] - end of epoch 555 (average epoch stats below)
[2024-09-18 06:40:15,894][train][INFO] - epoch 555 | loss 234.399 | ntokens 10344 | nsentences 47.2727 | nll_loss 1.071 | wps 2626.4 | ups 0.25 | wpb 10344 | bsz 47.3 | num_updates 18651 | lr 7.48995e-06 | gnorm 253.564 | loss_scale 2 | train_wall 149 | gb_free 20.9 | wall 0
[2024-09-18 06:40:15,895][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:40:15,914][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 556
[2024-09-18 06:40:15,931][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 06:40:15,977][fairseq.trainer][INFO] - begin training epoch 556
[2024-09-18 06:40:15,978][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 06:42:45,629][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 06:42:45,629][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:42:45,644][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 200
[2024-09-18 06:43:05,982][valid][INFO] - epoch 556 | valid on 'valid' subset | loss 192.447 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.858 | uer 16.603 | wer 24.702 | raw_wer 24.702 | wps 4677.9 | wpb 1902.9 | bsz 8.5 | num_updates 18695 | best_wer 24.702
[2024-09-18 06:43:05,982][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 556 @ 18695 updates
[2024-09-18 06:43:05,983][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 06:43:09,241][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 06:43:11,168][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 556 @ 18695 updates, score 24.702) (writing took 5.186006545001874 seconds)
[2024-09-18 06:43:11,169][fairseq_cli.train][INFO] - end of epoch 556 (average epoch stats below)
[2024-09-18 06:43:11,170][train][INFO] - epoch 556 | loss 235.125 | ntokens 10344 | nsentences 47.2727 | nll_loss 1.075 | wps 2596.7 | ups 0.25 | wpb 10344 | bsz 47.3 | num_updates 18695 | lr 7.39187e-06 | gnorm 263.467 | loss_scale 2 | train_wall 149 | gb_free 16.3 | wall 0
[2024-09-18 06:43:11,171][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:43:11,186][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 557
[2024-09-18 06:43:11,199][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 06:43:11,236][fairseq.trainer][INFO] - begin training epoch 557
[2024-09-18 06:43:11,236][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 06:45:42,513][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 06:45:42,513][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:45:42,529][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 201
[2024-09-18 06:46:02,928][valid][INFO] - epoch 557 | valid on 'valid' subset | loss 192.432 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.858 | uer 16.692 | wer 24.912 | raw_wer 24.912 | wps 4674.6 | wpb 1902.9 | bsz 8.5 | num_updates 18739 | best_wer 24.702
[2024-09-18 06:46:02,929][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 557 @ 18739 updates
[2024-09-18 06:46:02,930][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:46:06,318][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:46:06,401][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 557 @ 18739 updates, score 24.912) (writing took 3.4713698190025752 seconds)
[2024-09-18 06:46:06,401][fairseq_cli.train][INFO] - end of epoch 557 (average epoch stats below)
[2024-09-18 06:46:06,402][train][INFO] - epoch 557 | loss 226.382 | ntokens 10343 | nsentences 47.2727 | nll_loss 1.035 | wps 2597.1 | ups 0.25 | wpb 10343 | bsz 47.3 | num_updates 18739 | lr 7.29508e-06 | gnorm 249.22 | loss_scale 2 | train_wall 151 | gb_free 19.9 | wall 0
[2024-09-18 06:46:06,403][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:46:06,418][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 558
[2024-09-18 06:46:06,431][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 06:46:06,468][fairseq.trainer][INFO] - begin training epoch 558
[2024-09-18 06:46:06,468][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 06:48:36,481][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 06:48:36,482][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:48:36,497][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 202
[2024-09-18 06:48:56,935][valid][INFO] - epoch 558 | valid on 'valid' subset | loss 192.206 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.857 | uer 16.729 | wer 24.858 | raw_wer 24.858 | wps 4657.3 | wpb 1902.9 | bsz 8.5 | num_updates 18783 | best_wer 24.702
[2024-09-18 06:48:56,936][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 558 @ 18783 updates
[2024-09-18 06:48:56,936][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:49:00,279][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:49:00,355][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 558 @ 18783 updates, score 24.858) (writing took 3.419553633997566 seconds)
[2024-09-18 06:49:00,356][fairseq_cli.train][INFO] - end of epoch 558 (average epoch stats below)
[2024-09-18 06:49:00,357][train][INFO] - epoch 558 | loss 236.104 | ntokens 10343.8 | nsentences 47.2727 | nll_loss 1.079 | wps 2616.4 | ups 0.25 | wpb 10343.8 | bsz 47.3 | num_updates 18783 | lr 7.19955e-06 | gnorm 270.96 | loss_scale 2 | train_wall 150 | gb_free 19.7 | wall 0
[2024-09-18 06:49:00,357][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:49:00,373][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 559
[2024-09-18 06:49:00,386][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 06:49:00,423][fairseq.trainer][INFO] - begin training epoch 559
[2024-09-18 06:49:00,423][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 06:49:57,507][train_inner][INFO] - epoch 559:     17 / 44 loss=232.811, ntokens=10353, nsentences=47.48, nll_loss=1.068, wps=2581.6, ups=0.25, wpb=10353, bsz=47.5, num_updates=18800, lr=7.16298e-06, gnorm=257.686, loss_scale=2, train_wall=680, gb_free=18.5, wall=0
[2024-09-18 06:51:29,966][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 06:51:29,967][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:51:29,981][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 203
[2024-09-18 06:51:50,368][valid][INFO] - epoch 559 | valid on 'valid' subset | loss 192.131 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.856 | uer 16.683 | wer 24.848 | raw_wer 24.848 | wps 4676.7 | wpb 1902.9 | bsz 8.5 | num_updates 18827 | best_wer 24.702
[2024-09-18 06:51:50,368][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 559 @ 18827 updates
[2024-09-18 06:51:50,369][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:51:53,771][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:51:53,840][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 559 @ 18827 updates, score 24.848) (writing took 3.4711689809992095 seconds)
[2024-09-18 06:51:53,840][fairseq_cli.train][INFO] - end of epoch 559 (average epoch stats below)
[2024-09-18 06:51:53,842][train][INFO] - epoch 559 | loss 240.963 | ntokens 10343.8 | nsentences 47.2727 | nll_loss 1.101 | wps 2623.4 | ups 0.25 | wpb 10343.8 | bsz 47.3 | num_updates 18827 | lr 7.10527e-06 | gnorm 263.772 | loss_scale 2 | train_wall 149 | gb_free 18.3 | wall 0
[2024-09-18 06:51:53,843][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:51:53,862][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 560
[2024-09-18 06:51:53,884][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 06:51:53,957][fairseq.trainer][INFO] - begin training epoch 560
[2024-09-18 06:51:53,958][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 06:54:23,629][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 06:54:23,630][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:54:23,644][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 204
[2024-09-18 06:54:44,009][valid][INFO] - epoch 560 | valid on 'valid' subset | loss 192.252 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.857 | uer 16.751 | wer 24.878 | raw_wer 24.878 | wps 4664.1 | wpb 1902.9 | bsz 8.5 | num_updates 18871 | best_wer 24.702
[2024-09-18 06:54:44,010][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 560 @ 18871 updates
[2024-09-18 06:54:44,011][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:54:47,330][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:54:47,388][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 560 @ 18871 updates, score 24.878) (writing took 3.378191310999682 seconds)
[2024-09-18 06:54:47,389][fairseq_cli.train][INFO] - end of epoch 560 (average epoch stats below)
[2024-09-18 06:54:47,390][train][INFO] - epoch 560 | loss 230.236 | ntokens 10344 | nsentences 47.2727 | nll_loss 1.052 | wps 2622.5 | ups 0.25 | wpb 10344 | bsz 47.3 | num_updates 18871 | lr 7.01223e-06 | gnorm 263.688 | loss_scale 2 | train_wall 150 | gb_free 19.6 | wall 0
[2024-09-18 06:54:47,390][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:54:47,406][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 561
[2024-09-18 06:54:47,418][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 06:54:47,459][fairseq.trainer][INFO] - begin training epoch 561
[2024-09-18 06:54:47,460][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 06:57:16,079][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 06:57:16,079][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:57:16,095][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 205
[2024-09-18 06:57:36,389][valid][INFO] - epoch 561 | valid on 'valid' subset | loss 192.17 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.856 | uer 16.777 | wer 24.98 | raw_wer 24.98 | wps 4713.8 | wpb 1902.9 | bsz 8.5 | num_updates 18915 | best_wer 24.702
[2024-09-18 06:57:36,389][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 561 @ 18915 updates
[2024-09-18 06:57:36,390][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:57:39,736][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 06:57:39,802][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 561 @ 18915 updates, score 24.98) (writing took 3.412881723997998 seconds)
[2024-09-18 06:57:39,803][fairseq_cli.train][INFO] - end of epoch 561 (average epoch stats below)
[2024-09-18 06:57:39,804][train][INFO] - epoch 561 | loss 231.885 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 1.06 | wps 2639.6 | ups 0.26 | wpb 10343.2 | bsz 47.3 | num_updates 18915 | lr 6.92041e-06 | gnorm 259.562 | loss_scale 2 | train_wall 148 | gb_free 19.2 | wall 0
[2024-09-18 06:57:39,805][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 06:57:39,825][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 562
[2024-09-18 06:57:39,848][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 06:57:39,914][fairseq.trainer][INFO] - begin training epoch 562
[2024-09-18 06:57:39,915][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 07:00:09,341][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 07:00:09,341][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:00:09,356][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 206
[2024-09-18 07:00:29,674][valid][INFO] - epoch 562 | valid on 'valid' subset | loss 192.013 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.856 | uer 16.73 | wer 24.858 | raw_wer 24.858 | wps 4680.6 | wpb 1902.9 | bsz 8.5 | num_updates 18959 | best_wer 24.702
[2024-09-18 07:00:29,675][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 562 @ 18959 updates
[2024-09-18 07:00:29,675][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:00:33,003][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:00:33,070][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 562 @ 18959 updates, score 24.858) (writing took 3.395666099000664 seconds)
[2024-09-18 07:00:33,071][fairseq_cli.train][INFO] - end of epoch 562 (average epoch stats below)
[2024-09-18 07:00:33,072][train][INFO] - epoch 562 | loss 237.651 | ntokens 10343.6 | nsentences 47.2727 | nll_loss 1.086 | wps 2626.7 | ups 0.25 | wpb 10343.6 | bsz 47.3 | num_updates 18959 | lr 6.82979e-06 | gnorm 268.052 | loss_scale 2 | train_wall 149 | gb_free 21.2 | wall 0
[2024-09-18 07:00:33,073][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:00:33,093][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 563
[2024-09-18 07:00:33,109][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 07:00:33,156][fairseq.trainer][INFO] - begin training epoch 563
[2024-09-18 07:00:33,157][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 07:02:53,547][train_inner][INFO] - epoch 563:     41 / 44 loss=234.894, ntokens=10347.8, nsentences=47.08, nll_loss=1.069, wps=2666.8, ups=0.26, wpb=10347.8, bsz=47.1, num_updates=19000, lr=6.74641e-06, gnorm=265.672, loss_scale=2, train_wall=680, gb_free=17, wall=0
[2024-09-18 07:03:03,110][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 07:03:03,111][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:03:03,126][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 207
[2024-09-18 07:03:23,448][valid][INFO] - epoch 563 | valid on 'valid' subset | loss 192.019 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.856 | uer 16.632 | wer 24.853 | raw_wer 24.853 | wps 4676.5 | wpb 1902.9 | bsz 8.5 | num_updates 19003 | best_wer 24.702
[2024-09-18 07:03:23,449][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 563 @ 19003 updates
[2024-09-18 07:03:23,449][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:03:26,824][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:03:26,881][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 563 @ 19003 updates, score 24.853) (writing took 3.4316865820001112 seconds)
[2024-09-18 07:03:26,881][fairseq_cli.train][INFO] - end of epoch 563 (average epoch stats below)
[2024-09-18 07:03:26,882][train][INFO] - epoch 563 | loss 233.298 | ntokens 10343.8 | nsentences 47.2727 | nll_loss 1.066 | wps 2618.5 | ups 0.25 | wpb 10343.8 | bsz 47.3 | num_updates 19003 | lr 6.74035e-06 | gnorm 264.144 | loss_scale 2 | train_wall 150 | gb_free 19.4 | wall 0
[2024-09-18 07:03:26,883][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:03:26,898][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 564
[2024-09-18 07:03:26,910][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 07:03:26,947][fairseq.trainer][INFO] - begin training epoch 564
[2024-09-18 07:03:26,948][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 07:05:56,794][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 07:05:56,795][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:05:56,810][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 208
[2024-09-18 07:06:17,194][valid][INFO] - epoch 564 | valid on 'valid' subset | loss 192.02 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.856 | uer 16.529 | wer 24.716 | raw_wer 24.716 | wps 4649.3 | wpb 1902.9 | bsz 8.5 | num_updates 19047 | best_wer 24.702
[2024-09-18 07:06:17,194][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 564 @ 19047 updates
[2024-09-18 07:06:17,195][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:06:20,568][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:06:20,623][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 564 @ 19047 updates, score 24.716) (writing took 3.4285300920018926 seconds)
[2024-09-18 07:06:20,623][fairseq_cli.train][INFO] - end of epoch 564 (average epoch stats below)
[2024-09-18 07:06:20,625][train][INFO] - epoch 564 | loss 238.422 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 1.09 | wps 2619.4 | ups 0.25 | wpb 10343.2 | bsz 47.3 | num_updates 19047 | lr 6.65209e-06 | gnorm 270.737 | loss_scale 2 | train_wall 150 | gb_free 17.4 | wall 0
[2024-09-18 07:06:20,625][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:06:20,641][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 565
[2024-09-18 07:06:20,653][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 07:06:20,694][fairseq.trainer][INFO] - begin training epoch 565
[2024-09-18 07:06:20,695][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 07:08:50,856][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 07:08:50,856][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:08:50,873][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 209
[2024-09-18 07:09:11,269][valid][INFO] - epoch 565 | valid on 'valid' subset | loss 191.443 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.853 | uer 16.735 | wer 24.976 | raw_wer 24.976 | wps 4679.4 | wpb 1902.9 | bsz 8.5 | num_updates 19091 | best_wer 24.702
[2024-09-18 07:09:11,270][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 565 @ 19091 updates
[2024-09-18 07:09:11,271][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:09:14,669][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:09:14,726][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 565 @ 19091 updates, score 24.976) (writing took 3.4560837399985758 seconds)
[2024-09-18 07:09:14,727][fairseq_cli.train][INFO] - end of epoch 565 (average epoch stats below)
[2024-09-18 07:09:14,728][train][INFO] - epoch 565 | loss 234.036 | ntokens 10343.7 | nsentences 47.2727 | nll_loss 1.07 | wps 2614.1 | ups 0.25 | wpb 10343.7 | bsz 47.3 | num_updates 19091 | lr 6.56498e-06 | gnorm 267.563 | loss_scale 2 | train_wall 150 | gb_free 18.4 | wall 0
[2024-09-18 07:09:14,729][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:09:14,744][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 566
[2024-09-18 07:09:14,756][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 07:09:14,798][fairseq.trainer][INFO] - begin training epoch 566
[2024-09-18 07:09:14,799][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 07:11:45,488][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 07:11:45,488][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:11:45,503][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 210
[2024-09-18 07:12:05,846][valid][INFO] - epoch 566 | valid on 'valid' subset | loss 191.668 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.854 | uer 16.82 | wer 24.868 | raw_wer 24.868 | wps 4689.9 | wpb 1902.9 | bsz 8.5 | num_updates 19135 | best_wer 24.702
[2024-09-18 07:12:05,847][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 566 @ 19135 updates
[2024-09-18 07:12:05,847][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:12:09,149][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:12:09,204][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 566 @ 19135 updates, score 24.868) (writing took 3.3578162060002796 seconds)
[2024-09-18 07:12:09,205][fairseq_cli.train][INFO] - end of epoch 566 (average epoch stats below)
[2024-09-18 07:12:09,206][train][INFO] - epoch 566 | loss 224.956 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 1.028 | wps 2608.4 | ups 0.25 | wpb 10343.2 | bsz 47.3 | num_updates 19135 | lr 6.47902e-06 | gnorm 266.824 | loss_scale 2 | train_wall 151 | gb_free 18.9 | wall 0
[2024-09-18 07:12:09,208][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:12:09,229][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 567
[2024-09-18 07:12:09,250][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 07:12:09,310][fairseq.trainer][INFO] - begin training epoch 567
[2024-09-18 07:12:09,310][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 07:14:39,148][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 07:14:39,148][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:14:39,163][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 211
[2024-09-18 07:14:59,494][valid][INFO] - epoch 567 | valid on 'valid' subset | loss 192.057 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.856 | uer 16.425 | wer 24.746 | raw_wer 24.746 | wps 4683.8 | wpb 1902.9 | bsz 8.5 | num_updates 19179 | best_wer 24.702
[2024-09-18 07:14:59,495][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 567 @ 19179 updates
[2024-09-18 07:14:59,495][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:15:02,837][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:15:02,894][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 567 @ 19179 updates, score 24.746) (writing took 3.399735116996453 seconds)
[2024-09-18 07:15:02,895][fairseq_cli.train][INFO] - end of epoch 567 (average epoch stats below)
[2024-09-18 07:15:02,898][train][INFO] - epoch 567 | loss 230.31 | ntokens 10343.6 | nsentences 47.2727 | nll_loss 1.053 | wps 2620.3 | ups 0.25 | wpb 10343.6 | bsz 47.3 | num_updates 19179 | lr 6.39418e-06 | gnorm 264.993 | loss_scale 2 | train_wall 150 | gb_free 18.2 | wall 0
[2024-09-18 07:15:02,900][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:15:02,920][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 568
[2024-09-18 07:15:02,935][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 07:15:02,986][fairseq.trainer][INFO] - begin training epoch 568
[2024-09-18 07:15:02,986][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 07:16:14,379][train_inner][INFO] - epoch 568:     21 / 44 loss=232.463, ntokens=10346.6, nsentences=47.28, nll_loss=1.062, wps=2584, ups=0.25, wpb=10346.6, bsz=47.3, num_updates=19200, lr=6.35408e-06, gnorm=267.286, loss_scale=2, train_wall=681, gb_free=19.5, wall=0
[2024-09-18 07:17:33,118][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 07:17:33,118][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:17:33,133][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 212
[2024-09-18 07:17:53,441][valid][INFO] - epoch 568 | valid on 'valid' subset | loss 191.44 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.853 | uer 16.614 | wer 24.77 | raw_wer 24.77 | wps 4698.9 | wpb 1902.9 | bsz 8.5 | num_updates 19223 | best_wer 24.702
[2024-09-18 07:17:53,442][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 568 @ 19223 updates
[2024-09-18 07:17:53,442][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:17:56,811][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:17:56,868][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 568 @ 19223 updates, score 24.77) (writing took 3.4266695140031516 seconds)
[2024-09-18 07:17:56,869][fairseq_cli.train][INFO] - end of epoch 568 (average epoch stats below)
[2024-09-18 07:17:56,870][train][INFO] - epoch 568 | loss 226.175 | ntokens 10343 | nsentences 47.2727 | nll_loss 1.034 | wps 2615.9 | ups 0.25 | wpb 10343 | bsz 47.3 | num_updates 19223 | lr 6.31045e-06 | gnorm 259.915 | loss_scale 2 | train_wall 150 | gb_free 17.2 | wall 0
[2024-09-18 07:17:56,871][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:17:56,886][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 569
[2024-09-18 07:17:56,899][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 07:17:56,937][fairseq.trainer][INFO] - begin training epoch 569
[2024-09-18 07:17:56,938][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 07:20:25,916][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 07:20:25,916][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:20:25,931][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 213
[2024-09-18 07:20:46,283][valid][INFO] - epoch 569 | valid on 'valid' subset | loss 191.552 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.854 | uer 16.568 | wer 24.809 | raw_wer 24.809 | wps 4666.4 | wpb 1902.9 | bsz 8.5 | num_updates 19267 | best_wer 24.702
[2024-09-18 07:20:46,284][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 569 @ 19267 updates
[2024-09-18 07:20:46,284][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:20:49,635][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:20:49,690][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 569 @ 19267 updates, score 24.809) (writing took 3.4056493660027627 seconds)
[2024-09-18 07:20:49,690][fairseq_cli.train][INFO] - end of epoch 569 (average epoch stats below)
[2024-09-18 07:20:49,691][train][INFO] - epoch 569 | loss 240.759 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 1.1 | wps 2633.4 | ups 0.25 | wpb 10343.2 | bsz 47.3 | num_updates 19267 | lr 6.22781e-06 | gnorm 285.192 | loss_scale 2 | train_wall 149 | gb_free 19.4 | wall 0
[2024-09-18 07:20:49,692][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:20:49,708][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 570
[2024-09-18 07:20:49,721][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 07:20:49,775][fairseq.trainer][INFO] - begin training epoch 570
[2024-09-18 07:20:49,776][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 07:23:20,489][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 07:23:20,489][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:23:20,505][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 214
[2024-09-18 07:23:40,839][valid][INFO] - epoch 570 | valid on 'valid' subset | loss 190.971 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.851 | uer 16.535 | wer 24.702 | raw_wer 24.702 | wps 4677 | wpb 1902.9 | bsz 8.5 | num_updates 19311 | best_wer 24.702
[2024-09-18 07:23:40,840][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 570 @ 19311 updates
[2024-09-18 07:23:40,841][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 07:23:44,149][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 07:23:46,041][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 570 @ 19311 updates, score 24.702) (writing took 5.200579242002277 seconds)
[2024-09-18 07:23:46,041][fairseq_cli.train][INFO] - end of epoch 570 (average epoch stats below)
[2024-09-18 07:23:46,042][train][INFO] - epoch 570 | loss 224.141 | ntokens 10343.6 | nsentences 47.2727 | nll_loss 1.024 | wps 2580.8 | ups 0.25 | wpb 10343.6 | bsz 47.3 | num_updates 19311 | lr 6.14626e-06 | gnorm 258.637 | loss_scale 2 | train_wall 151 | gb_free 16.1 | wall 0
[2024-09-18 07:23:46,043][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:23:46,059][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 571
[2024-09-18 07:23:46,079][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 07:23:46,123][fairseq.trainer][INFO] - begin training epoch 571
[2024-09-18 07:23:46,124][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 07:26:15,373][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 07:26:15,373][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:26:15,389][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 215
[2024-09-18 07:26:35,807][valid][INFO] - epoch 571 | valid on 'valid' subset | loss 190.622 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.849 | uer 16.55 | wer 24.633 | raw_wer 24.633 | wps 4687.2 | wpb 1902.9 | bsz 8.5 | num_updates 19355 | best_wer 24.633
[2024-09-18 07:26:35,808][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 571 @ 19355 updates
[2024-09-18 07:26:35,809][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 07:26:39,137][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 07:26:41,000][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 571 @ 19355 updates, score 24.633) (writing took 5.192215158000181 seconds)
[2024-09-18 07:26:41,001][fairseq_cli.train][INFO] - end of epoch 571 (average epoch stats below)
[2024-09-18 07:26:41,002][train][INFO] - epoch 571 | loss 230.795 | ntokens 10343.6 | nsentences 47.2727 | nll_loss 1.055 | wps 2601.3 | ups 0.25 | wpb 10343.6 | bsz 47.3 | num_updates 19355 | lr 6.06578e-06 | gnorm 259.868 | loss_scale 2 | train_wall 149 | gb_free 23.4 | wall 0
[2024-09-18 07:26:41,004][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:26:41,019][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 572
[2024-09-18 07:26:41,031][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 07:26:41,074][fairseq.trainer][INFO] - begin training epoch 572
[2024-09-18 07:26:41,074][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 07:29:12,136][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 07:29:12,137][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:29:12,151][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 216
[2024-09-18 07:29:32,700][valid][INFO] - epoch 572 | valid on 'valid' subset | loss 190.448 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.849 | uer 16.438 | wer 24.535 | raw_wer 24.535 | wps 4641.9 | wpb 1902.9 | bsz 8.5 | num_updates 19399 | best_wer 24.535
[2024-09-18 07:29:32,701][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 572 @ 19399 updates
[2024-09-18 07:29:32,701][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 07:29:36,013][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_best.pt
[2024-09-18 07:29:37,887][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_best.pt (epoch 572 @ 19399 updates, score 24.535) (writing took 5.186363950000668 seconds)
[2024-09-18 07:29:37,887][fairseq_cli.train][INFO] - end of epoch 572 (average epoch stats below)
[2024-09-18 07:29:37,888][train][INFO] - epoch 572 | loss 225.628 | ntokens 10343.8 | nsentences 47.2727 | nll_loss 1.031 | wps 2573 | ups 0.25 | wpb 10343.8 | bsz 47.3 | num_updates 19399 | lr 5.98635e-06 | gnorm 258.711 | loss_scale 2 | train_wall 151 | gb_free 18.4 | wall 0
[2024-09-18 07:29:37,889][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:29:37,905][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 573
[2024-09-18 07:29:37,921][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 07:29:37,959][fairseq.trainer][INFO] - begin training epoch 573
[2024-09-18 07:29:37,959][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 07:29:40,979][train_inner][INFO] - epoch 573:      1 / 44 loss=229.069, ntokens=10327.1, nsentences=47.16, nll_loss=1.046, wps=2560.7, ups=0.25, wpb=10327.1, bsz=47.2, num_updates=19400, lr=5.98455e-06, gnorm=264.477, loss_scale=2, train_wall=681, gb_free=26.4, wall=0
[2024-09-18 07:32:08,172][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 07:32:08,173][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:32:08,188][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 217
[2024-09-18 07:32:28,614][valid][INFO] - epoch 573 | valid on 'valid' subset | loss 190.516 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.849 | uer 16.471 | wer 24.672 | raw_wer 24.672 | wps 4655.4 | wpb 1902.9 | bsz 8.5 | num_updates 19443 | best_wer 24.535
[2024-09-18 07:32:28,615][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 573 @ 19443 updates
[2024-09-18 07:32:28,615][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:32:32,560][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:32:32,630][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 573 @ 19443 updates, score 24.672) (writing took 4.0155337960022734 seconds)
[2024-09-18 07:32:32,631][fairseq_cli.train][INFO] - end of epoch 573 (average epoch stats below)
[2024-09-18 07:32:32,632][train][INFO] - epoch 573 | loss 228.713 | ntokens 10343.3 | nsentences 47.2727 | nll_loss 1.045 | wps 2604.4 | ups 0.25 | wpb 10343.3 | bsz 47.3 | num_updates 19443 | lr 5.90796e-06 | gnorm 272.541 | loss_scale 2 | train_wall 150 | gb_free 19.7 | wall 0
[2024-09-18 07:32:32,633][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:32:32,648][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 574
[2024-09-18 07:32:32,661][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 07:32:32,701][fairseq.trainer][INFO] - begin training epoch 574
[2024-09-18 07:32:32,701][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 07:35:02,585][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 07:35:02,585][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:35:02,601][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 218
[2024-09-18 07:35:22,915][valid][INFO] - epoch 574 | valid on 'valid' subset | loss 190.273 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.848 | uer 16.611 | wer 24.702 | raw_wer 24.702 | wps 4671.5 | wpb 1902.9 | bsz 8.5 | num_updates 19487 | best_wer 24.535
[2024-09-18 07:35:22,916][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 574 @ 19487 updates
[2024-09-18 07:35:22,916][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:35:26,632][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:35:26,699][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 574 @ 19487 updates, score 24.702) (writing took 3.7832809570027166 seconds)
[2024-09-18 07:35:26,699][fairseq_cli.train][INFO] - end of epoch 574 (average epoch stats below)
[2024-09-18 07:35:26,700][train][INFO] - epoch 574 | loss 231.489 | ntokens 10343.6 | nsentences 47.2727 | nll_loss 1.058 | wps 2614.6 | ups 0.25 | wpb 10343.6 | bsz 47.3 | num_updates 19487 | lr 5.83059e-06 | gnorm 270.42 | loss_scale 2 | train_wall 150 | gb_free 22.7 | wall 0
[2024-09-18 07:35:26,701][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:35:26,717][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 575
[2024-09-18 07:35:26,729][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 07:35:26,769][fairseq.trainer][INFO] - begin training epoch 575
[2024-09-18 07:35:26,769][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 07:37:57,036][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 07:37:57,037][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:37:57,052][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 219
[2024-09-18 07:38:17,325][valid][INFO] - epoch 575 | valid on 'valid' subset | loss 190.013 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.847 | uer 16.566 | wer 24.663 | raw_wer 24.663 | wps 4690.7 | wpb 1902.9 | bsz 8.5 | num_updates 19531 | best_wer 24.535
[2024-09-18 07:38:17,326][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 575 @ 19531 updates
[2024-09-18 07:38:17,326][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:38:20,910][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:38:20,964][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 575 @ 19531 updates, score 24.663) (writing took 3.6382361559954006 seconds)
[2024-09-18 07:38:20,965][fairseq_cli.train][INFO] - end of epoch 575 (average epoch stats below)
[2024-09-18 07:38:20,966][train][INFO] - epoch 575 | loss 225.867 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 1.032 | wps 2611.6 | ups 0.25 | wpb 10343.2 | bsz 47.3 | num_updates 19531 | lr 5.75424e-06 | gnorm 262.206 | loss_scale 2 | train_wall 150 | gb_free 16.2 | wall 0
[2024-09-18 07:38:20,966][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:38:20,982][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 576
[2024-09-18 07:38:20,994][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 07:38:21,046][fairseq.trainer][INFO] - begin training epoch 576
[2024-09-18 07:38:21,047][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 07:40:50,253][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 07:40:50,253][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:40:50,269][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 220
[2024-09-18 07:41:10,574][valid][INFO] - epoch 576 | valid on 'valid' subset | loss 190.732 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.85 | uer 16.41 | wer 24.658 | raw_wer 24.658 | wps 4698 | wpb 1902.9 | bsz 8.5 | num_updates 19575 | best_wer 24.535
[2024-09-18 07:41:10,574][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 576 @ 19575 updates
[2024-09-18 07:41:10,575][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:41:13,934][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:41:14,005][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 576 @ 19575 updates, score 24.658) (writing took 3.4311373550008284 seconds)
[2024-09-18 07:41:14,006][fairseq_cli.train][INFO] - end of epoch 576 (average epoch stats below)
[2024-09-18 07:41:14,008][train][INFO] - epoch 576 | loss 231.575 | ntokens 10343.8 | nsentences 47.2727 | nll_loss 1.058 | wps 2630.2 | ups 0.25 | wpb 10343.8 | bsz 47.3 | num_updates 19575 | lr 5.67889e-06 | gnorm 266.371 | loss_scale 2 | train_wall 149 | gb_free 20.8 | wall 0
[2024-09-18 07:41:14,009][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:41:14,028][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 577
[2024-09-18 07:41:14,043][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 07:41:14,096][fairseq.trainer][INFO] - begin training epoch 577
[2024-09-18 07:41:14,096][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 07:42:39,870][train_inner][INFO] - epoch 577:     25 / 44 loss=229.446, ntokens=10374.3, nsentences=47.52, nll_loss=1.051, wps=2663.9, ups=0.26, wpb=10374.3, bsz=47.5, num_updates=19600, lr=5.63652e-06, gnorm=266.106, loss_scale=2, train_wall=682, gb_free=20.7, wall=0
[2024-09-18 07:43:44,497][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 07:43:44,497][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:43:44,512][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 221
[2024-09-18 07:44:04,835][valid][INFO] - epoch 577 | valid on 'valid' subset | loss 189.964 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.847 | uer 16.496 | wer 24.643 | raw_wer 24.643 | wps 4669.9 | wpb 1902.9 | bsz 8.5 | num_updates 19619 | best_wer 24.535
[2024-09-18 07:44:04,836][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 577 @ 19619 updates
[2024-09-18 07:44:04,836][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:44:08,138][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:44:08,209][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 577 @ 19619 updates, score 24.643) (writing took 3.3736096049979096 seconds)
[2024-09-18 07:44:08,210][fairseq_cli.train][INFO] - end of epoch 577 (average epoch stats below)
[2024-09-18 07:44:08,211][train][INFO] - epoch 577 | loss 229.058 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.047 | wps 2612.6 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 19619 | lr 5.60453e-06 | gnorm 252.878 | loss_scale 2 | train_wall 150 | gb_free 22.7 | wall 0
[2024-09-18 07:44:08,212][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:44:08,231][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 578
[2024-09-18 07:44:08,247][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 07:44:08,299][fairseq.trainer][INFO] - begin training epoch 578
[2024-09-18 07:44:08,300][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 07:46:37,376][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 07:46:37,377][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:46:37,392][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 222
[2024-09-18 07:46:57,762][valid][INFO] - epoch 578 | valid on 'valid' subset | loss 190.31 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.848 | uer 16.572 | wer 24.604 | raw_wer 24.604 | wps 4667.1 | wpb 1902.9 | bsz 8.5 | num_updates 19663 | best_wer 24.535
[2024-09-18 07:46:57,762][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 578 @ 19663 updates
[2024-09-18 07:46:57,763][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:47:01,080][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:47:01,141][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 578 @ 19663 updates, score 24.604) (writing took 3.378859493001073 seconds)
[2024-09-18 07:47:01,142][fairseq_cli.train][INFO] - end of epoch 578 (average epoch stats below)
[2024-09-18 07:47:01,143][train][INFO] - epoch 578 | loss 225.472 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.03 | wps 2631.8 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 19663 | lr 5.53114e-06 | gnorm 251.476 | loss_scale 2 | train_wall 149 | gb_free 18.6 | wall 0
[2024-09-18 07:47:01,144][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:47:01,160][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 579
[2024-09-18 07:47:01,172][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 07:47:01,212][fairseq.trainer][INFO] - begin training epoch 579
[2024-09-18 07:47:01,212][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 07:49:30,583][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 07:49:30,584][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:49:30,599][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 223
[2024-09-18 07:49:51,011][valid][INFO] - epoch 579 | valid on 'valid' subset | loss 190.098 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.847 | uer 16.568 | wer 24.726 | raw_wer 24.726 | wps 4670.8 | wpb 1902.9 | bsz 8.5 | num_updates 19707 | best_wer 24.535
[2024-09-18 07:49:51,012][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 579 @ 19707 updates
[2024-09-18 07:49:51,013][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:49:54,334][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:49:54,387][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 579 @ 19707 updates, score 24.726) (writing took 3.375288132003334 seconds)
[2024-09-18 07:49:54,388][fairseq_cli.train][INFO] - end of epoch 579 (average epoch stats below)
[2024-09-18 07:49:54,389][train][INFO] - epoch 579 | loss 232.381 | ntokens 10343.3 | nsentences 47.2727 | nll_loss 1.062 | wps 2626.9 | ups 0.25 | wpb 10343.3 | bsz 47.3 | num_updates 19707 | lr 5.45871e-06 | gnorm 259.853 | loss_scale 2 | train_wall 149 | gb_free 20.7 | wall 0
[2024-09-18 07:49:54,390][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:49:54,405][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 580
[2024-09-18 07:49:54,417][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 07:49:54,473][fairseq.trainer][INFO] - begin training epoch 580
[2024-09-18 07:49:54,474][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 07:52:24,073][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 07:52:24,073][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:52:24,088][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 224
[2024-09-18 07:52:44,348][valid][INFO] - epoch 580 | valid on 'valid' subset | loss 189.962 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.847 | uer 16.569 | wer 24.79 | raw_wer 24.79 | wps 4703.8 | wpb 1902.9 | bsz 8.5 | num_updates 19751 | best_wer 24.535
[2024-09-18 07:52:44,349][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 580 @ 19751 updates
[2024-09-18 07:52:44,349][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:52:47,715][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:52:47,771][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 580 @ 19751 updates, score 24.79) (writing took 3.4222866609998164 seconds)
[2024-09-18 07:52:47,771][fairseq_cli.train][INFO] - end of epoch 580 (average epoch stats below)
[2024-09-18 07:52:47,772][train][INFO] - epoch 580 | loss 235.878 | ntokens 10343.2 | nsentences 47.2727 | nll_loss 1.078 | wps 2624.8 | ups 0.25 | wpb 10343.2 | bsz 47.3 | num_updates 19751 | lr 5.38723e-06 | gnorm 271.741 | loss_scale 2 | train_wall 149 | gb_free 19.1 | wall 0
[2024-09-18 07:52:47,773][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:52:47,788][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 581
[2024-09-18 07:52:47,801][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 07:52:47,844][fairseq.trainer][INFO] - begin training epoch 581
[2024-09-18 07:52:47,845][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 07:55:17,579][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 07:55:17,579][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:55:17,594][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 225
[2024-09-18 07:55:38,090][valid][INFO] - epoch 581 | valid on 'valid' subset | loss 189.345 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.844 | uer 16.501 | wer 24.658 | raw_wer 24.658 | wps 4651.7 | wpb 1902.9 | bsz 8.5 | num_updates 19795 | best_wer 24.535
[2024-09-18 07:55:38,091][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 581 @ 19795 updates
[2024-09-18 07:55:38,091][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:55:41,449][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:55:41,504][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 581 @ 19795 updates, score 24.658) (writing took 3.4133582010035752 seconds)
[2024-09-18 07:55:41,504][fairseq_cli.train][INFO] - end of epoch 581 (average epoch stats below)
[2024-09-18 07:55:41,506][train][INFO] - epoch 581 | loss 231.507 | ntokens 10344 | nsentences 47.2727 | nll_loss 1.058 | wps 2619.8 | ups 0.25 | wpb 10344 | bsz 47.3 | num_updates 19795 | lr 5.31669e-06 | gnorm 274.755 | loss_scale 2 | train_wall 150 | gb_free 18.2 | wall 0
[2024-09-18 07:55:41,506][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:55:41,522][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 582
[2024-09-18 07:55:41,535][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 07:55:41,581][fairseq.trainer][INFO] - begin training epoch 582
[2024-09-18 07:55:41,581][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 07:55:59,468][train_inner][INFO] - epoch 582:      5 / 44 loss=230.587, ntokens=10343.2, nsentences=47.2, nll_loss=1.052, wps=2587.1, ups=0.25, wpb=10343.2, bsz=47.2, num_updates=19800, lr=5.30873e-06, gnorm=262.797, loss_scale=2, train_wall=679, gb_free=17.7, wall=0
[2024-09-18 07:58:11,591][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 07:58:11,592][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:58:11,607][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 226
[2024-09-18 07:58:31,995][valid][INFO] - epoch 582 | valid on 'valid' subset | loss 190.145 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.847 | uer 16.623 | wer 24.858 | raw_wer 24.858 | wps 4659.7 | wpb 1902.9 | bsz 8.5 | num_updates 19839 | best_wer 24.535
[2024-09-18 07:58:31,996][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 582 @ 19839 updates
[2024-09-18 07:58:31,996][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:58:35,332][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 07:58:35,389][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 582 @ 19839 updates, score 24.858) (writing took 3.3928505450021476 seconds)
[2024-09-18 07:58:35,389][fairseq_cli.train][INFO] - end of epoch 582 (average epoch stats below)
[2024-09-18 07:58:35,390][train][INFO] - epoch 582 | loss 226.079 | ntokens 10343.8 | nsentences 47.2727 | nll_loss 1.033 | wps 2617.4 | ups 0.25 | wpb 10343.8 | bsz 47.3 | num_updates 19839 | lr 5.24707e-06 | gnorm 258.555 | loss_scale 2 | train_wall 150 | gb_free 19.2 | wall 0
[2024-09-18 07:58:35,391][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 07:58:35,409][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 583
[2024-09-18 07:58:35,425][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 07:58:35,478][fairseq.trainer][INFO] - begin training epoch 583
[2024-09-18 07:58:35,479][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 08:01:04,710][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 08:01:04,710][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 08:01:04,726][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 227
[2024-09-18 08:01:25,049][valid][INFO] - epoch 583 | valid on 'valid' subset | loss 190.179 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.848 | uer 16.348 | wer 24.731 | raw_wer 24.731 | wps 4674.3 | wpb 1902.9 | bsz 8.5 | num_updates 19883 | best_wer 24.535
[2024-09-18 08:01:25,049][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 583 @ 19883 updates
[2024-09-18 08:01:25,050][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 08:01:28,396][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 08:01:28,451][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 583 @ 19883 updates, score 24.731) (writing took 3.4013164680000045 seconds)
[2024-09-18 08:01:28,451][fairseq_cli.train][INFO] - end of epoch 583 (average epoch stats below)
[2024-09-18 08:01:28,452][train][INFO] - epoch 583 | loss 231.974 | ntokens 10343.4 | nsentences 47.2727 | nll_loss 1.06 | wps 2629.8 | ups 0.25 | wpb 10343.4 | bsz 47.3 | num_updates 19883 | lr 5.17836e-06 | gnorm 261.047 | loss_scale 2 | train_wall 149 | gb_free 18.7 | wall 0
[2024-09-18 08:01:28,453][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 08:01:28,469][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 584
[2024-09-18 08:01:28,481][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 08:01:28,530][fairseq.trainer][INFO] - begin training epoch 584
[2024-09-18 08:01:28,531][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 08:03:58,096][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 08:03:58,097][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 08:03:58,112][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 228
[2024-09-18 08:04:18,311][valid][INFO] - epoch 584 | valid on 'valid' subset | loss 190.15 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.847 | uer 16.501 | wer 24.716 | raw_wer 24.716 | wps 4714.2 | wpb 1902.9 | bsz 8.5 | num_updates 19927 | best_wer 24.535
[2024-09-18 08:04:18,311][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 584 @ 19927 updates
[2024-09-18 08:04:18,312][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 08:04:21,588][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 08:04:21,644][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 584 @ 19927 updates, score 24.716) (writing took 3.3326118499971926 seconds)
[2024-09-18 08:04:21,645][fairseq_cli.train][INFO] - end of epoch 584 (average epoch stats below)
[2024-09-18 08:04:21,646][train][INFO] - epoch 584 | loss 234.581 | ntokens 10343.4 | nsentences 47.2727 | nll_loss 1.072 | wps 2627.8 | ups 0.25 | wpb 10343.4 | bsz 47.3 | num_updates 19927 | lr 5.11055e-06 | gnorm 268.096 | loss_scale 2 | train_wall 149 | gb_free 18.5 | wall 0
[2024-09-18 08:04:21,646][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 08:04:21,662][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 585
[2024-09-18 08:04:21,674][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 08:04:21,719][fairseq.trainer][INFO] - begin training epoch 585
[2024-09-18 08:04:21,720][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 08:06:52,196][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 08:06:52,197][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 08:06:52,212][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 229
[2024-09-18 08:07:12,513][valid][INFO] - epoch 585 | valid on 'valid' subset | loss 189.571 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.845 | uer 16.416 | wer 24.638 | raw_wer 24.638 | wps 4704.7 | wpb 1902.9 | bsz 8.5 | num_updates 19971 | best_wer 24.535
[2024-09-18 08:07:12,514][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 585 @ 19971 updates
[2024-09-18 08:07:12,515][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 08:07:15,868][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_last.pt
[2024-09-18 08:07:15,929][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 585 @ 19971 updates, score 24.638) (writing took 3.414464469999075 seconds)
[2024-09-18 08:07:15,929][fairseq_cli.train][INFO] - end of epoch 585 (average epoch stats below)
[2024-09-18 08:07:15,931][train][INFO] - epoch 585 | loss 229.34 | ntokens 10343.5 | nsentences 47.2727 | nll_loss 1.048 | wps 2611.3 | ups 0.25 | wpb 10343.5 | bsz 47.3 | num_updates 19971 | lr 5.04363e-06 | gnorm 258.64 | loss_scale 2 | train_wall 150 | gb_free 18.3 | wall 0
[2024-09-18 08:07:15,932][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 08:07:15,952][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 586
[2024-09-18 08:07:15,968][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 08:07:16,024][fairseq.trainer][INFO] - begin training epoch 586
[2024-09-18 08:07:16,024][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 08:08:54,458][train_inner][INFO] - epoch 586:     29 / 44 loss=230.951, ntokens=10332.6, nsentences=47.16, nll_loss=1.054, wps=2666.5, ups=0.26, wpb=10332.6, bsz=47.2, num_updates=20000, lr=5e-06, gnorm=263.558, loss_scale=2, train_wall=679, gb_free=20.5, wall=0
[2024-09-18 08:08:54,459][fairseq_cli.train][INFO] - Stopping training due to num_updates: 20000 >= max_update: 20000
[2024-09-18 08:08:54,459][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-18 08:08:54,459][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 08:08:54,474][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 230
[2024-09-18 08:09:14,780][valid][INFO] - epoch 586 | valid on 'valid' subset | loss 189.768 | ntokens 1902.88 | nsentences 8.48 | nll_loss 0.846 | uer 16.262 | wer 24.535 | raw_wer 24.535 | wps 4706.4 | wpb 1902.9 | bsz 8.5 | num_updates 20000 | best_wer 24.535
[2024-09-18 08:09:14,780][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 586 @ 20000 updates
[2024-09-18 08:09:14,781][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_586_20000.pt
[2024-09-18 08:09:17,853][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-17/21-07-10/ckpts/checkpoint_586_20000.pt
[2024-09-18 08:09:22,497][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_586_20000.pt (epoch 586 @ 20000 updates, score 24.535) (writing took 7.716832639998756 seconds)
[2024-09-18 08:09:22,614][fairseq_cli.train][INFO] - end of epoch 586 (average epoch stats below)
[2024-09-18 08:09:22,616][train][INFO] - epoch 586 | loss 231.682 | ntokens 10445.2 | nsentences 47.4483 | nll_loss 1.052 | wps 2391.1 | ups 0.23 | wpb 10445.2 | bsz 47.4 | num_updates 20000 | lr 5e-06 | gnorm 272.331 | loss_scale 2 | train_wall 98 | gb_free 20.5 | wall 0
[2024-09-18 08:09:22,616][fairseq_cli.train][INFO] - done training in 39724.5 seconds
wandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: - 0.004 MB of 0.523 MB uploadedwandb: \ 0.053 MB of 0.523 MB uploadedwandb: | 0.053 MB of 0.523 MB uploadedwandb: / 0.053 MB of 0.523 MB uploadedwandb: - 0.053 MB of 0.523 MB uploadedwandb: \ 0.053 MB of 0.523 MB uploadedwandb: | 0.053 MB of 0.523 MB uploadedwandb: / 0.053 MB of 0.523 MB uploadedwandb: - 0.053 MB of 0.523 MB uploadedwandb: \ 0.053 MB of 0.523 MB uploadedwandb: | 0.053 MB of 0.523 MB uploadedwandb: / 0.053 MB of 0.523 MB uploadedwandb: - 0.053 MB of 0.523 MB uploadedwandb: \ 0.053 MB of 0.523 MB uploadedwandb: | 0.053 MB of 0.523 MB uploadedwandb: / 0.053 MB of 0.523 MB uploadedwandb: - 0.053 MB of 0.523 MB uploadedwandb: \ 0.053 MB of 0.523 MB uploadedwandb: | 0.053 MB of 0.523 MB uploadedwandb: / 0.053 MB of 0.523 MB uploadedwandb: - 0.053 MB of 0.523 MB uploadedwandb: \ 0.053 MB of 0.523 MB uploadedwandb: | 0.523 MB of 0.523 MB uploadedwandb: 
wandb: Run history:
wandb:              train/bsz ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà
wandb:          train/gb_free ‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÇ‚ñÑ‚ñá‚ñÖ‚ñÇ‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñà‚ñÇ‚ñÜ‚ñÇ‚ñà‚ñÑ‚ñÜ
wandb:            train/gnorm ‚ñà‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:             train/loss ‚ñà‚ñà‚ñá‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/loss_scale ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:               train/lr ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         train/nll_loss ‚ñà‚ñà‚ñá‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/nsentences ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà
wandb:          train/ntokens ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà
wandb:       train/train_wall ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ
wandb:              train/ups ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ
wandb:             train/wall ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:              train/wpb ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà
wandb:              train/wps ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñÅ
wandb:        train_inner/bsz ‚ñÉ‚ñá‚ñÇ‚ñÜ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñÅ‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñá‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ
wandb:    train_inner/gb_free ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÜ‚ñÉ‚ñá‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñà‚ñÖ‚ñÖ
wandb:      train_inner/gnorm ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train_inner/loss ‚ñà‚ñá‚ñá‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: train_inner/loss_scale ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:         train_inner/lr ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   train_inner/nll_loss ‚ñà‚ñà‚ñá‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: train_inner/nsentences ‚ñÉ‚ñá‚ñÇ‚ñÜ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÅ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñá‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ
wandb:    train_inner/ntokens ‚ñá‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÖ‚ñÅ‚ñá‚ñÅ‚ñà‚ñÅ‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñá‚ñÉ
wandb: train_inner/train_wall ‚ñÜ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÇ
wandb:        train_inner/ups ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:       train_inner/wall ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        train_inner/wpb ‚ñá‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÖ‚ñÅ‚ñá‚ñÅ‚ñà‚ñÅ‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñá‚ñÉ
wandb:        train_inner/wps ‚ñÅ‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà
wandb:         valid/best_wer ‚ñà‚ñà‚ñà‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:              valid/bsz ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:             valid/loss ‚ñà‚ñà‚ñá‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         valid/nll_loss ‚ñà‚ñà‚ñá‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       valid/nsentences ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:          valid/ntokens ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:          valid/raw_wer ‚ñà‚ñà‚ñà‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:              valid/uer ‚ñà‚ñá‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:              valid/wer ‚ñà‚ñà‚ñà‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:              valid/wpb ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:              valid/wps ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñÖ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñÉ‚ñÑ‚ñÜ
wandb: 
wandb: Run summary:
wandb:              train/bsz 47.4
wandb:          train/gb_free 20.5
wandb:            train/gnorm 272.331
wandb:             train/loss 231.682
wandb:       train/loss_scale 2.0
wandb:               train/lr 1e-05
wandb:         train/nll_loss 1.052
wandb:       train/nsentences 47.44828
wandb:          train/ntokens 10445.17241
wandb:       train/train_wall 98.0
wandb:              train/ups 0.23
wandb:             train/wall 0.0
wandb:              train/wpb 10445.2
wandb:              train/wps 2391.1
wandb:        train_inner/bsz 47.2
wandb:    train_inner/gb_free 20.5
wandb:      train_inner/gnorm 263.558
wandb:       train_inner/loss 230.951
wandb: train_inner/loss_scale 2.0
wandb:         train_inner/lr 1e-05
wandb:   train_inner/nll_loss 1.054
wandb: train_inner/nsentences 47.16
wandb:    train_inner/ntokens 10332.605
wandb: train_inner/train_wall 679.0
wandb:        train_inner/ups 0.26
wandb:       train_inner/wall 0.0
wandb:        train_inner/wpb 10332.6
wandb:        train_inner/wps 2666.5
wandb:         valid/best_wer 24.535
wandb:              valid/bsz 8.5
wandb:             valid/loss 189.768
wandb:         valid/nll_loss 0.846
wandb:       valid/nsentences 8.48
wandb:          valid/ntokens 1902.88
wandb:          valid/raw_wer 24.535
wandb:              valid/uer 16.262
wandb:              valid/wer 24.535
wandb:              valid/wpb 1902.9
wandb:              valid/wps 4706.4
wandb: 
wandb: üöÄ View run ckpts at: https://wandb.ai/aadel4/w2v2-cpt-transfer_finetuning/runs/pq5g1l7j
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240917_210718-pq5g1l7j/logs
dev:
Current directory: /home/ahmed/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining
save_dir: ckpts
w2v_name: /home/ahmed/Research/Projects/finetune_w2v_fairseq/cpt_models/w2v_robust_1M
model_path: /home/ahmed/Research/Projects/finetune_w2v_fairseq/cpt_models/w2v_robust_1M.pt
wandb_project: finetune_w2v2_continued_pretraining
config_name: base
outdir_fold: ./model_outputs/continued_pretraining//
dataset: NCTE_Full
Running on fold: 
[2024-09-18 12:40:22,005][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'tqdm', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': 'w2v2-cpt-transfer_finetuning', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 3, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 5000000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 9500, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 5000000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 20000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': True, 'update_freq': [5], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'ckpts', 'restore_file': '', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'wer', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec_ctc', 'w2v_path': '/home/ahmed/Research/Projects/finetune_w2v_fairseq/cpt_models/w2v_robust_1M.pt', 'no_pretrained_weights': False, 'dropout_input': 0.0, 'final_dropout': 0.0, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.1, 'apply_mask': True, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 64, 'mask_channel_prob': 0.5, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'freeze_finetune_updates': 10000, 'feature_grad_mult': 0.0, 'layerdrop': 0.1, 'drop_path': 0.0, 'mask_channel_min_space': 1, 'mask_channel_before': False, 'normalize': True, 'update_alibi': True, 'data': '/home/ahmed/Research/Projects/finetune_w2v_fairseq/manifest/NCTE_Full/', 'w2v_args': None, 'offload_activations': False, 'min_params_to_wrap': 100000000, 'checkpoint_activations': False, 'ddp_backend': 'no_c10d', 'zero_mask': False, 'load_ema': False, 'layer_decay': 1.0, 'layer_type': transformer, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all', 'freeze_regex': None, 'blank_weight': 0.0, 'blank_mode': 'add'}, 'task': {'_name': 'audio_finetuning', 'data': '/home/ahmed/Research/Projects/finetune_w2v_fairseq/manifest/NCTE_Full/', 'labels': 'ltr', 'multi_corpus_keys': None, 'multi_corpus_sampling_weights': None, 'binarized_dataset': False, 'sample_rate': 16000, 'normalize': True, 'enable_padding': False, 'max_sample_size': None, 'min_sample_size': 16000, 'num_batch_buckets': 0, 'tpu': False, 'text_compression_level': none, 'rebuild_batches': True, 'precompute_mask_config': None, 'post_save_script': None, 'subsample': 1.0, 'seed': 1, 'eval_wer': False, 'eval_wer_config': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_wer_tokenizer': None, 'eval_wer_post_process': 'letter', 'eval_bleu': False, 'eval_bleu_detok': None, 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_args': '{}', 'eval_bleu_print_samples': False, 'autoregressive': False, 'target_dictionary': None}, 'criterion': {'_name': 'ctc', 'zero_infinity': True, 'sentence_avg': True, 'post_process': 'letter', 'wer_kenlm_model': None, 'wer_lexicon': None, 'wer_lm_weight': 2.0, 'wer_word_score': -1.0, 'wer_sil_weight': 0.0, 'wer_args': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0001]}, 'lr_scheduler': {'_name': 'tri_stage', 'warmup_steps': 0, 'hold_steps': 0, 'decay_steps': 0, 'phase_ratio': [0.1, 0.4, 0.5], 'init_lr_scale': 0.01, 'final_lr_scale': 0.05, 'max_update': 20000.0, 'lr': [0.0001]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-09-18 12:40:22,014][fairseq.tasks.audio_finetuning][INFO] - Using dict_path : /home/ahmed/Research/Projects/finetune_w2v_fairseq/manifest/NCTE_Full/dict.ltr.txt
[2024-09-18 12:40:23,705][fairseq.models.wav2vec.wav2vec2_asr][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 3000, 'log_format': 'tqdm', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': 'w2v2_pretraining_trial_w2v_large_lv_fsh_swbd_cv', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 3, 'distributed_num_procs': 3, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:42263', 'distributed_port': 42263, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 3, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 3000000, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 1000, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3000000, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 1000000, 'stop_time_hours': 0.0, 'clip_norm': 0.5, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'ckpts', 'restore_file': '/scr-ssd/aadel4/pretrain_w2v_fairseq/outputs/2024-03-30/00-47-34/ckpts/checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 6}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.1, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 768, 'layer_norm_first': True, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': True, 'logit_temp': 0.1, 'quantize_targets': True, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 0.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 64, 'mask_channel_prob': 0.5, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.1, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'audio_pretraining', 'data': '/scr-ssd/aadel4/pretrain_w2v_fairseq/manifist', 'labels': None, 'multi_corpus_keys': None, 'multi_corpus_sampling_weights': None, 'binarized_dataset': False, 'sample_rate': 16000, 'normalize': True, 'enable_padding': False, 'max_sample_size': 320000, 'min_sample_size': 32000, 'num_batch_buckets': 0, 'tpu': False, 'text_compression_level': 'none', 'rebuild_batches': True, 'precompute_mask_config': None, 'post_save_script': None, 'subsample': 1.0, 'seed': 1}, 'criterion': None, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0001]}, 'lr_scheduler': None, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
/home/ahmed/miniconda3/envs/frsq/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.
  warnings.warn("torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.")
Wav2Vec2Model(
  (feature_extractor): ConvFeatureExtractionModel(
    (conv_layers): ModuleList(
      (0): Sequential(
        (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,))
        (1): Dropout(p=0.0, inplace=False)
        (2): Sequential(
          (0): TransposeLast()
          (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (2): TransposeLast()
        )
        (3): GELU(approximate='none')
      )
      (1-4): 4 x Sequential(
        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))
        (1): Dropout(p=0.0, inplace=False)
        (2): Sequential(
          (0): TransposeLast()
          (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (2): TransposeLast()
        )
        (3): GELU(approximate='none')
      )
      (5-6): 2 x Sequential(
        (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,))
        (1): Dropout(p=0.0, inplace=False)
        (2): Sequential(
          (0): TransposeLast()
          (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (2): TransposeLast()
        )
        (3): GELU(approximate='none')
      )
    )
  )
  (post_extract_proj): Linear(in_features=512, out_features=1024, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (quantizer): None
  (project_q): None
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-23): 24 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (final_proj): None
)
[2024-09-18 12:40:26,241][fairseq_cli.train][INFO] - Wav2VecCtc(
  (w2v_encoder): Wav2VecEncoder(
    (w2v_model): Wav2Vec2Model(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,))
            (1): Dropout(p=0.0, inplace=False)
            (2): Sequential(
              (0): TransposeLast()
              (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (2): TransposeLast()
            )
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))
            (1): Dropout(p=0.0, inplace=False)
            (2): Sequential(
              (0): TransposeLast()
              (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (2): TransposeLast()
            )
            (3): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,))
            (1): Dropout(p=0.0, inplace=False)
            (2): Sequential(
              (0): TransposeLast()
              (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (2): TransposeLast()
            )
            (3): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=1024, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.0, inplace=False)
      (quantizer): None
      (project_q): None
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-23): 24 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (final_dropout): Dropout(p=0.0, inplace=False)
    (proj): Linear(in_features=1024, out_features=47, bias=True)
  )
)
[2024-09-18 12:40:26,243][fairseq_cli.train][INFO] - task: AudioFinetuningTask
[2024-09-18 12:40:26,243][fairseq_cli.train][INFO] - model: Wav2VecCtc
[2024-09-18 12:40:26,243][fairseq_cli.train][INFO] - criterion: CtcCriterion
[2024-09-18 12:40:26,244][fairseq_cli.train][INFO] - num. shared model params: 315,486,895 (num. trained: 315,486,895)
[2024-09-18 12:40:26,245][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-09-18 12:40:26,246][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 427, skipped 0 samples
[2024-09-18 12:40:26,295][numexpr.utils][INFO] - Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-09-18 12:40:26,295][numexpr.utils][INFO] - NumExpr defaulting to 8 threads.
[2024-09-18 12:40:26,606][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2024-09-18 12:40:26,606][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 47.528 GB ; name = NVIDIA RTX A6000                        
[2024-09-18 12:40:26,606][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2024-09-18 12:40:26,606][fairseq_cli.train][INFO] - training on 1 devices (GPUs/TPUs)
[2024-09-18 12:40:26,606][fairseq_cli.train][INFO] - max tokens per device = 5000000 and max sentences per device = None
[2024-09-18 12:40:26,606][fairseq.trainer][INFO] - Preparing to load checkpoint 
[2024-09-18 12:40:26,607][fairseq.trainer][INFO] - No existing checkpoint found 
[2024-09-18 12:40:26,607][fairseq.trainer][INFO] - loading train data for epoch 1
[2024-09-18 12:40:26,608][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 2087, skipped 14 samples
[2024-09-18 12:40:26,610][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 12:40:26,610][fairseq.tasks.fairseq_task][INFO] - reuse_dataloader = True
[2024-09-18 12:40:26,610][fairseq.tasks.fairseq_task][INFO] - rebuild_batches = True
[2024-09-18 12:40:26,610][fairseq.tasks.fairseq_task][INFO] - batches will be rebuilt for each epoch
[2024-09-18 12:40:26,610][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 1
[2024-09-18 12:40:26,772][fairseq_cli.train][INFO] - begin dry-run validation on "valid" subset
[2024-09-18 12:40:26,773][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 12:40:26,773][fairseq.tasks.fairseq_task][INFO] - reuse_dataloader = True
[2024-09-18 12:40:26,773][fairseq.tasks.fairseq_task][INFO] - rebuild_batches = True
[2024-09-18 12:40:26,773][fairseq.tasks.fairseq_task][INFO] - batches will be rebuilt for each epoch
[2024-09-18 12:40:26,773][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 1
[2024-09-18 12:40:27,380][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
wandb: Currently logged in as: aadel4. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/wandb/run-20240918_124028-tqii7rdb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ckpts
wandb: ‚≠êÔ∏è View project at https://wandb.ai/aadel4/w2v2-cpt-transfer_finetuning
wandb: üöÄ View run at https://wandb.ai/aadel4/w2v2-cpt-transfer_finetuning/runs/tqii7rdb
[2024-09-18 12:40:33,727][fairseq.trainer][INFO] - begin training epoch 1
[2024-09-18 12:40:33,728][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 12:40:35,860][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
[2024-09-18 12:40:37,422][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2024-09-18 12:40:39,037][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
[2024-09-18 12:40:40,543][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
[2024-09-18 12:40:42,050][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2024-09-18 12:40:43,636][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-09-18 12:41:44,166][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1 @ 38 updates
[2024-09-18 12:41:44,167][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:41:47,715][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:41:47,822][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 1 @ 38 updates, score None) (writing took 3.656020152993733 seconds)
[2024-09-18 12:41:47,822][fairseq_cli.train][INFO] - end of epoch 1 (average epoch stats below)
[2024-09-18 12:41:47,823][train][INFO] - epoch 001 | loss 5174.98 | ntokens 10350.7 | nsentences 47.3684 | nll_loss 23.683 | wps 6116.5 | ups 0.59 | wpb 10350.7 | bsz 47.4 | num_updates 38 | lr 2.881e-06 | gnorm 881.955 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 81
[2024-09-18 12:41:47,824][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 12:41:47,848][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 2
[2024-09-18 12:41:47,862][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 12:41:47,870][fairseq.trainer][INFO] - begin training epoch 2
[2024-09-18 12:41:47,871][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 12:42:58,371][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2 @ 82 updates
[2024-09-18 12:42:58,371][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:43:02,222][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:43:02,289][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 2 @ 82 updates, score None) (writing took 3.9179295409994666 seconds)
[2024-09-18 12:43:02,289][fairseq_cli.train][INFO] - end of epoch 2 (average epoch stats below)
[2024-09-18 12:43:02,290][train][INFO] - epoch 002 | loss 5176.55 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 23.677 | wps 6107 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 82 | lr 5.059e-06 | gnorm 903.618 | loss_scale 2 | train_wall 70 | gb_free 37.4 | wall 156
[2024-09-18 12:43:02,291][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 12:43:02,307][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 3
[2024-09-18 12:43:02,319][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 12:43:02,327][fairseq.trainer][INFO] - begin training epoch 3
[2024-09-18 12:43:02,327][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 12:44:12,779][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 3 @ 126 updates
[2024-09-18 12:44:12,780][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:44:16,442][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:44:16,515][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 3 @ 126 updates, score None) (writing took 3.735827041993616 seconds)
[2024-09-18 12:44:16,515][fairseq_cli.train][INFO] - end of epoch 3 (average epoch stats below)
[2024-09-18 12:44:16,516][train][INFO] - epoch 003 | loss 5169.28 | ntokens 10335.8 | nsentences 47.2727 | nll_loss 23.643 | wps 6126.9 | ups 0.59 | wpb 10335.8 | bsz 47.3 | num_updates 126 | lr 7.237e-06 | gnorm 892.287 | loss_scale 2 | train_wall 70 | gb_free 37.6 | wall 230
[2024-09-18 12:44:16,517][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 12:44:16,542][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 4
[2024-09-18 12:44:16,555][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 12:44:16,562][fairseq.trainer][INFO] - begin training epoch 4
[2024-09-18 12:44:16,563][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 12:45:27,635][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 4 @ 170 updates
[2024-09-18 12:45:27,636][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:45:31,343][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:45:31,409][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 4 @ 170 updates, score None) (writing took 3.774178141000448 seconds)
[2024-09-18 12:45:31,410][fairseq_cli.train][INFO] - end of epoch 4 (average epoch stats below)
[2024-09-18 12:45:31,411][train][INFO] - epoch 004 | loss 5158.02 | ntokens 10335.8 | nsentences 47.2727 | nll_loss 23.591 | wps 6072.3 | ups 0.59 | wpb 10335.8 | bsz 47.3 | num_updates 170 | lr 9.415e-06 | gnorm 887.721 | loss_scale 2 | train_wall 71 | gb_free 36.9 | wall 305
[2024-09-18 12:45:31,411][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 12:45:31,427][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 5
[2024-09-18 12:45:31,442][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 12:45:31,449][fairseq.trainer][INFO] - begin training epoch 5
[2024-09-18 12:45:31,449][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 12:46:19,943][train_inner][INFO] - epoch 005:     30 / 44 loss=5180.6, ntokens=10336.1, nsentences=47.12, nll_loss=23.617, wps=6144.9, ups=0.59, wpb=10336.1, bsz=47.1, num_updates=200, lr=1.09e-05, gnorm=897.005, loss_scale=2, train_wall=330, gb_free=37.2, wall=353
[2024-09-18 12:46:42,288][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 5 @ 214 updates
[2024-09-18 12:46:42,289][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:46:45,930][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:46:46,009][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 5 @ 214 updates, score None) (writing took 3.720679502002895 seconds)
[2024-09-18 12:46:46,009][fairseq_cli.train][INFO] - end of epoch 5 (average epoch stats below)
[2024-09-18 12:46:46,010][train][INFO] - epoch 005 | loss 5146.25 | ntokens 10335 | nsentences 47.2727 | nll_loss 23.539 | wps 6095.8 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 214 | lr 1.1593e-05 | gnorm 903.098 | loss_scale 2 | train_wall 71 | gb_free 35.6 | wall 379
[2024-09-18 12:46:46,011][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 12:46:46,028][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 6
[2024-09-18 12:46:46,040][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 12:46:46,047][fairseq.trainer][INFO] - begin training epoch 6
[2024-09-18 12:46:46,048][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 12:47:56,594][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 6 @ 258 updates
[2024-09-18 12:47:56,594][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:48:00,134][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:48:00,209][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 6 @ 258 updates, score None) (writing took 3.6152357230021153 seconds)
[2024-09-18 12:48:00,209][fairseq_cli.train][INFO] - end of epoch 6 (average epoch stats below)
[2024-09-18 12:48:00,210][train][INFO] - epoch 006 | loss 5129.48 | ntokens 10335.3 | nsentences 47.2727 | nll_loss 23.462 | wps 6128.8 | ups 0.59 | wpb 10335.3 | bsz 47.3 | num_updates 258 | lr 1.3771e-05 | gnorm 912.402 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 454
[2024-09-18 12:48:00,211][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 12:48:00,226][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 7
[2024-09-18 12:48:00,239][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 12:48:00,247][fairseq.trainer][INFO] - begin training epoch 7
[2024-09-18 12:48:00,247][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 12:49:11,081][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 7 @ 302 updates
[2024-09-18 12:49:11,082][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:49:14,675][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:49:14,753][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 7 @ 302 updates, score None) (writing took 3.671838331996696 seconds)
[2024-09-18 12:49:14,753][fairseq_cli.train][INFO] - end of epoch 7 (average epoch stats below)
[2024-09-18 12:49:14,754][train][INFO] - epoch 007 | loss 5109.19 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 23.369 | wps 6100.7 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 302 | lr 1.5949e-05 | gnorm 937.884 | loss_scale 2 | train_wall 71 | gb_free 37.4 | wall 528
[2024-09-18 12:49:14,755][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 12:49:14,770][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 8
[2024-09-18 12:49:14,782][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 12:49:14,790][fairseq.trainer][INFO] - begin training epoch 8
[2024-09-18 12:49:14,790][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 12:50:25,052][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 8 @ 346 updates
[2024-09-18 12:50:25,053][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:50:28,613][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:50:28,693][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 8 @ 346 updates, score None) (writing took 3.6411113729991484 seconds)
[2024-09-18 12:50:28,694][fairseq_cli.train][INFO] - end of epoch 8 (average epoch stats below)
[2024-09-18 12:50:28,695][train][INFO] - epoch 008 | loss 5089.41 | ntokens 10335.5 | nsentences 47.2727 | nll_loss 23.278 | wps 6150.5 | ups 0.6 | wpb 10335.5 | bsz 47.3 | num_updates 346 | lr 1.8127e-05 | gnorm 925.305 | loss_scale 2 | train_wall 70 | gb_free 35.9 | wall 602
[2024-09-18 12:50:28,695][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 12:50:28,711][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 9
[2024-09-18 12:50:28,723][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 12:50:28,731][fairseq.trainer][INFO] - begin training epoch 9
[2024-09-18 12:50:28,732][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 12:51:39,213][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 9 @ 390 updates
[2024-09-18 12:51:39,213][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:51:42,725][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:51:42,802][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 9 @ 390 updates, score None) (writing took 3.589491629987606 seconds)
[2024-09-18 12:51:42,802][fairseq_cli.train][INFO] - end of epoch 9 (average epoch stats below)
[2024-09-18 12:51:42,803][train][INFO] - epoch 009 | loss 5058.43 | ntokens 10335.3 | nsentences 47.2727 | nll_loss 23.137 | wps 6136.4 | ups 0.59 | wpb 10335.3 | bsz 47.3 | num_updates 390 | lr 2.0305e-05 | gnorm 961.566 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 676
[2024-09-18 12:51:42,804][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 12:51:42,822][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 10
[2024-09-18 12:51:42,836][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 12:51:42,845][fairseq.trainer][INFO] - begin training epoch 10
[2024-09-18 12:51:42,845][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 12:51:59,708][train_inner][INFO] - epoch 010:     10 / 44 loss=5083.36, ntokens=10352.9, nsentences=47.52, nll_loss=23.333, wps=6094.2, ups=0.59, wpb=10352.9, bsz=47.5, num_updates=400, lr=2.08e-05, gnorm=928.502, loss_scale=2, train_wall=321, gb_free=35.7, wall=693
[2024-09-18 12:52:53,758][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 10 @ 434 updates
[2024-09-18 12:52:53,759][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:52:57,324][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:52:57,405][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 10 @ 434 updates, score None) (writing took 3.6462393349938793 seconds)
[2024-09-18 12:52:57,405][fairseq_cli.train][INFO] - end of epoch 10 (average epoch stats below)
[2024-09-18 12:52:57,406][train][INFO] - epoch 010 | loss 5030.8 | ntokens 10335.8 | nsentences 47.2727 | nll_loss 23.009 | wps 6096 | ups 0.59 | wpb 10335.8 | bsz 47.3 | num_updates 434 | lr 2.2483e-05 | gnorm 960.972 | loss_scale 2 | train_wall 71 | gb_free 37.4 | wall 751
[2024-09-18 12:52:57,407][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 12:52:57,422][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 11
[2024-09-18 12:52:57,435][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 12:52:57,443][fairseq.trainer][INFO] - begin training epoch 11
[2024-09-18 12:52:57,444][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 12:54:08,131][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 11 @ 478 updates
[2024-09-18 12:54:08,131][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:54:11,710][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:54:11,788][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 11 @ 478 updates, score None) (writing took 3.6571022210118826 seconds)
[2024-09-18 12:54:11,788][fairseq_cli.train][INFO] - end of epoch 11 (average epoch stats below)
[2024-09-18 12:54:11,789][train][INFO] - epoch 011 | loss 4990.34 | ntokens 10335.5 | nsentences 47.2727 | nll_loss 22.825 | wps 6113.9 | ups 0.59 | wpb 10335.5 | bsz 47.3 | num_updates 478 | lr 2.4661e-05 | gnorm 1007.06 | loss_scale 2 | train_wall 71 | gb_free 37.3 | wall 825
[2024-09-18 12:54:11,790][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 12:54:11,806][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 12
[2024-09-18 12:54:11,819][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 12:54:11,827][fairseq.trainer][INFO] - begin training epoch 12
[2024-09-18 12:54:11,827][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 12:55:22,520][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 12 @ 522 updates
[2024-09-18 12:55:22,521][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:55:26,052][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:55:26,132][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 12 @ 522 updates, score None) (writing took 3.6121257830091054 seconds)
[2024-09-18 12:55:26,132][fairseq_cli.train][INFO] - end of epoch 12 (average epoch stats below)
[2024-09-18 12:55:26,133][train][INFO] - epoch 012 | loss 4958.26 | ntokens 10335.5 | nsentences 47.2727 | nll_loss 22.678 | wps 6117.1 | ups 0.59 | wpb 10335.5 | bsz 47.3 | num_updates 522 | lr 2.6839e-05 | gnorm 993.238 | loss_scale 2 | train_wall 71 | gb_free 37.1 | wall 900
[2024-09-18 12:55:26,134][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 12:55:26,150][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 13
[2024-09-18 12:55:26,164][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 12:55:26,172][fairseq.trainer][INFO] - begin training epoch 13
[2024-09-18 12:55:26,172][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 12:56:37,106][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 13 @ 566 updates
[2024-09-18 12:56:37,107][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:56:40,658][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:56:40,739][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 13 @ 566 updates, score None) (writing took 3.632501907995902 seconds)
[2024-09-18 12:56:40,739][fairseq_cli.train][INFO] - end of epoch 13 (average epoch stats below)
[2024-09-18 12:56:40,740][train][INFO] - epoch 013 | loss 4923.84 | ntokens 10335 | nsentences 47.2727 | nll_loss 22.522 | wps 6095.3 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 566 | lr 2.9017e-05 | gnorm 994.082 | loss_scale 2 | train_wall 71 | gb_free 37.1 | wall 974
[2024-09-18 12:56:40,741][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 12:56:40,758][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 14
[2024-09-18 12:56:40,770][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 12:56:40,779][fairseq.trainer][INFO] - begin training epoch 14
[2024-09-18 12:56:40,779][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 12:57:35,586][train_inner][INFO] - epoch 014:     34 / 44 loss=4945.06, ntokens=10335.6, nsentences=47.4, nll_loss=22.678, wps=6154.4, ups=0.6, wpb=10335.6, bsz=47.4, num_updates=600, lr=3.07e-05, gnorm=989.565, loss_scale=2, train_wall=321, gb_free=37.1, wall=1029
[2024-09-18 12:57:51,308][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 14 @ 610 updates
[2024-09-18 12:57:51,308][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:57:54,863][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:57:54,940][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 14 @ 610 updates, score None) (writing took 3.632503949003876 seconds)
[2024-09-18 12:57:54,940][fairseq_cli.train][INFO] - end of epoch 14 (average epoch stats below)
[2024-09-18 12:57:54,941][train][INFO] - epoch 014 | loss 4886.78 | ntokens 10335.5 | nsentences 47.2727 | nll_loss 22.351 | wps 6128.9 | ups 0.59 | wpb 10335.5 | bsz 47.3 | num_updates 610 | lr 3.1195e-05 | gnorm 999.92 | loss_scale 2 | train_wall 70 | gb_free 37.5 | wall 1048
[2024-09-18 12:57:54,942][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 12:57:54,958][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 15
[2024-09-18 12:57:54,970][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 12:57:54,978][fairseq.trainer][INFO] - begin training epoch 15
[2024-09-18 12:57:54,979][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 12:59:05,328][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 15 @ 654 updates
[2024-09-18 12:59:05,328][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:59:08,814][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 12:59:08,892][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 15 @ 654 updates, score None) (writing took 3.564471585006686 seconds)
[2024-09-18 12:59:08,893][fairseq_cli.train][INFO] - end of epoch 15 (average epoch stats below)
[2024-09-18 12:59:08,894][train][INFO] - epoch 015 | loss 4832.59 | ntokens 10334.8 | nsentences 47.2727 | nll_loss 22.105 | wps 6149.1 | ups 0.59 | wpb 10334.8 | bsz 47.3 | num_updates 654 | lr 3.3373e-05 | gnorm 1028.9 | loss_scale 2 | train_wall 70 | gb_free 37.3 | wall 1122
[2024-09-18 12:59:08,894][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 12:59:08,910][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 16
[2024-09-18 12:59:08,922][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 12:59:08,931][fairseq.trainer][INFO] - begin training epoch 16
[2024-09-18 12:59:08,931][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:00:19,906][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 16 @ 698 updates
[2024-09-18 13:00:19,907][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:00:23,362][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:00:23,437][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 16 @ 698 updates, score None) (writing took 3.5315731769951526 seconds)
[2024-09-18 13:00:23,438][fairseq_cli.train][INFO] - end of epoch 16 (average epoch stats below)
[2024-09-18 13:00:23,439][train][INFO] - epoch 016 | loss 4781.97 | ntokens 10335.8 | nsentences 47.2727 | nll_loss 21.871 | wps 6100.7 | ups 0.59 | wpb 10335.8 | bsz 47.3 | num_updates 698 | lr 3.5551e-05 | gnorm 1036.47 | loss_scale 2 | train_wall 71 | gb_free 37.5 | wall 1197
[2024-09-18 13:00:23,440][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:00:23,455][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 17
[2024-09-18 13:00:23,468][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:00:23,478][fairseq.trainer][INFO] - begin training epoch 17
[2024-09-18 13:00:23,478][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:01:34,038][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 17 @ 742 updates
[2024-09-18 13:01:34,039][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:01:37,499][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:01:37,575][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 17 @ 742 updates, score None) (writing took 3.536577833001502 seconds)
[2024-09-18 13:01:37,575][fairseq_cli.train][INFO] - end of epoch 17 (average epoch stats below)
[2024-09-18 13:01:37,577][train][INFO] - epoch 017 | loss 4719.22 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 21.585 | wps 6134 | ups 0.59 | wpb 10335.2 | bsz 47.3 | num_updates 742 | lr 3.7729e-05 | gnorm 1066.77 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 1271
[2024-09-18 13:01:37,577][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:01:37,593][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 18
[2024-09-18 13:01:37,605][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:01:37,614][fairseq.trainer][INFO] - begin training epoch 18
[2024-09-18 13:01:37,614][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:02:48,156][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 18 @ 786 updates
[2024-09-18 13:02:48,157][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:02:51,591][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:02:51,668][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 18 @ 786 updates, score None) (writing took 3.5120973490120377 seconds)
[2024-09-18 13:02:51,668][fairseq_cli.train][INFO] - end of epoch 18 (average epoch stats below)
[2024-09-18 13:02:51,669][train][INFO] - epoch 018 | loss 4671.77 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 21.369 | wps 6137.6 | ups 0.59 | wpb 10335.2 | bsz 47.3 | num_updates 786 | lr 3.9907e-05 | gnorm 1078.17 | loss_scale 2 | train_wall 70 | gb_free 37.4 | wall 1345
[2024-09-18 13:02:51,670][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:02:51,685][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 19
[2024-09-18 13:02:51,698][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:02:51,707][fairseq.trainer][INFO] - begin training epoch 19
[2024-09-18 13:02:51,707][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:03:14,547][train_inner][INFO] - epoch 019:     14 / 44 loss=4747.35, ntokens=10334.2, nsentences=47.2, nll_loss=21.683, wps=6097.8, ups=0.59, wpb=10334.2, bsz=47.2, num_updates=800, lr=4.06e-05, gnorm=1053.23, loss_scale=2, train_wall=320, gb_free=35.6, wall=1368
[2024-09-18 13:04:02,682][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 19 @ 830 updates
[2024-09-18 13:04:02,682][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:04:06,130][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:04:06,205][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 19 @ 830 updates, score None) (writing took 3.523327517003054 seconds)
[2024-09-18 13:04:06,205][fairseq_cli.train][INFO] - end of epoch 19 (average epoch stats below)
[2024-09-18 13:04:06,206][train][INFO] - epoch 019 | loss 4611.12 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 21.091 | wps 6101.2 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 830 | lr 4.2085e-05 | gnorm 1080.2 | loss_scale 2 | train_wall 71 | gb_free 37.1 | wall 1420
[2024-09-18 13:04:06,207][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:04:06,223][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 20
[2024-09-18 13:04:06,234][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:04:06,243][fairseq.trainer][INFO] - begin training epoch 20
[2024-09-18 13:04:06,243][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:05:16,739][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 20 @ 874 updates
[2024-09-18 13:05:16,740][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:05:20,208][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:05:20,283][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 20 @ 874 updates, score None) (writing took 3.54405697301263 seconds)
[2024-09-18 13:05:20,284][fairseq_cli.train][INFO] - end of epoch 20 (average epoch stats below)
[2024-09-18 13:05:20,285][train][INFO] - epoch 020 | loss 4552.88 | ntokens 10335 | nsentences 47.2727 | nll_loss 20.825 | wps 6138.7 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 874 | lr 4.4263e-05 | gnorm 1083.92 | loss_scale 2 | train_wall 70 | gb_free 35.6 | wall 1494
[2024-09-18 13:05:20,286][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:05:20,301][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 21
[2024-09-18 13:05:20,313][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:05:20,321][fairseq.trainer][INFO] - begin training epoch 21
[2024-09-18 13:05:20,322][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:06:30,932][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 21 @ 918 updates
[2024-09-18 13:06:30,933][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:06:34,352][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:06:34,426][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 21 @ 918 updates, score None) (writing took 3.4941216750012245 seconds)
[2024-09-18 13:06:34,427][fairseq_cli.train][INFO] - end of epoch 21 (average epoch stats below)
[2024-09-18 13:06:34,428][train][INFO] - epoch 021 | loss 4439.49 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 20.306 | wps 6133.6 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 918 | lr 4.6441e-05 | gnorm 1153.57 | loss_scale 2 | train_wall 70 | gb_free 37.2 | wall 1568
[2024-09-18 13:06:34,429][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:06:34,444][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 22
[2024-09-18 13:06:34,457][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:06:34,465][fairseq.trainer][INFO] - begin training epoch 22
[2024-09-18 13:06:34,465][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:07:45,391][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 22 @ 962 updates
[2024-09-18 13:07:45,392][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:07:48,834][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:07:48,911][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 22 @ 962 updates, score None) (writing took 3.5206705699965823 seconds)
[2024-09-18 13:07:48,912][fairseq_cli.train][INFO] - end of epoch 22 (average epoch stats below)
[2024-09-18 13:07:48,913][train][INFO] - epoch 022 | loss 4391.97 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 20.089 | wps 6105.3 | ups 0.59 | wpb 10335.2 | bsz 47.3 | num_updates 962 | lr 4.8619e-05 | gnorm 1143.23 | loss_scale 2 | train_wall 71 | gb_free 35.6 | wall 1642
[2024-09-18 13:07:48,914][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:07:48,929][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 23
[2024-09-18 13:07:48,941][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:07:48,951][fairseq.trainer][INFO] - begin training epoch 23
[2024-09-18 13:07:48,951][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:08:50,014][train_inner][INFO] - epoch 023:     38 / 44 loss=4478.9, ntokens=10324.8, nsentences=47.12, nll_loss=20.441, wps=6155.5, ups=0.6, wpb=10324.8, bsz=47.1, num_updates=1000, lr=5.05e-05, gnorm=1121.2, loss_scale=2, train_wall=321, gb_free=37.2, wall=1703
[2024-09-18 13:08:59,408][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 23 @ 1006 updates
[2024-09-18 13:08:59,409][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:09:02,883][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:09:02,958][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 23 @ 1006 updates, score None) (writing took 3.5499947049975162 seconds)
[2024-09-18 13:09:02,958][fairseq_cli.train][INFO] - end of epoch 23 (average epoch stats below)
[2024-09-18 13:09:02,959][train][INFO] - epoch 023 | loss 4348.28 | ntokens 10335.8 | nsentences 47.2727 | nll_loss 19.888 | wps 6141.8 | ups 0.59 | wpb 10335.8 | bsz 47.3 | num_updates 1006 | lr 5.0797e-05 | gnorm 1114.94 | loss_scale 2 | train_wall 70 | gb_free 37.4 | wall 1716
[2024-09-18 13:09:02,960][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:09:02,976][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 24
[2024-09-18 13:09:02,987][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:09:02,997][fairseq.trainer][INFO] - begin training epoch 24
[2024-09-18 13:09:02,997][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:10:13,720][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 24 @ 1050 updates
[2024-09-18 13:10:13,721][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:10:17,184][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:10:17,259][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 24 @ 1050 updates, score None) (writing took 3.5387976220081327 seconds)
[2024-09-18 13:10:17,259][fairseq_cli.train][INFO] - end of epoch 24 (average epoch stats below)
[2024-09-18 13:10:17,260][train][INFO] - epoch 024 | loss 4241.44 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 19.4 | wps 6120.6 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 1050 | lr 5.2975e-05 | gnorm 1130.44 | loss_scale 2 | train_wall 71 | gb_free 37.3 | wall 1791
[2024-09-18 13:10:17,261][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:10:17,276][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 25
[2024-09-18 13:10:17,288][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:10:17,298][fairseq.trainer][INFO] - begin training epoch 25
[2024-09-18 13:10:17,298][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:11:28,109][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 25 @ 1094 updates
[2024-09-18 13:11:28,110][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:11:31,588][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:11:31,664][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 25 @ 1094 updates, score None) (writing took 3.5547323060018243 seconds)
[2024-09-18 13:11:31,664][fairseq_cli.train][INFO] - end of epoch 25 (average epoch stats below)
[2024-09-18 13:11:31,665][train][INFO] - epoch 025 | loss 4135 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 18.913 | wps 6112 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 1094 | lr 5.5153e-05 | gnorm 1170.89 | loss_scale 2 | train_wall 71 | gb_free 37.1 | wall 1865
[2024-09-18 13:11:31,666][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:11:31,681][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 26
[2024-09-18 13:11:31,693][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:11:31,702][fairseq.trainer][INFO] - begin training epoch 26
[2024-09-18 13:11:31,702][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:12:41,768][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 26 @ 1138 updates
[2024-09-18 13:12:41,769][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:12:45,210][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:12:45,284][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 26 @ 1138 updates, score None) (writing took 3.5153885369945783 seconds)
[2024-09-18 13:12:45,284][fairseq_cli.train][INFO] - end of epoch 26 (average epoch stats below)
[2024-09-18 13:12:45,285][train][INFO] - epoch 026 | loss 4088.33 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 18.7 | wps 6177 | ups 0.6 | wpb 10335.2 | bsz 47.3 | num_updates 1138 | lr 5.7331e-05 | gnorm 1128.69 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 1939
[2024-09-18 13:12:45,286][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:12:45,301][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 27
[2024-09-18 13:12:45,314][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:12:45,323][fairseq.trainer][INFO] - begin training epoch 27
[2024-09-18 13:12:45,323][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:13:55,779][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 27 @ 1182 updates
[2024-09-18 13:13:55,780][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:13:59,268][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:13:59,342][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 27 @ 1182 updates, score None) (writing took 3.5628555940056685 seconds)
[2024-09-18 13:13:59,343][fairseq_cli.train][INFO] - end of epoch 27 (average epoch stats below)
[2024-09-18 13:13:59,344][train][INFO] - epoch 027 | loss 3981.3 | ntokens 10335.3 | nsentences 47.2727 | nll_loss 18.21 | wps 6140.5 | ups 0.59 | wpb 10335.3 | bsz 47.3 | num_updates 1182 | lr 5.9509e-05 | gnorm 1144.84 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 2013
[2024-09-18 13:13:59,345][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:13:59,360][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 28
[2024-09-18 13:13:59,372][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:13:59,381][fairseq.trainer][INFO] - begin training epoch 28
[2024-09-18 13:13:59,381][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:14:27,969][train_inner][INFO] - epoch 028:     18 / 44 loss=4061.37, ntokens=10331.2, nsentences=47.68, nll_loss=18.744, wps=6114, ups=0.59, wpb=10331.2, bsz=47.7, num_updates=1200, lr=6.04e-05, gnorm=1136.08, loss_scale=2, train_wall=319, gb_free=35.9, wall=2041
[2024-09-18 13:15:09,701][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 28 @ 1226 updates
[2024-09-18 13:15:09,702][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:15:13,166][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:15:13,235][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 28 @ 1226 updates, score None) (writing took 3.5331806850008434 seconds)
[2024-09-18 13:15:13,235][fairseq_cli.train][INFO] - end of epoch 28 (average epoch stats below)
[2024-09-18 13:15:13,236][train][INFO] - epoch 028 | loss 3894.09 | ntokens 10335.5 | nsentences 47.2727 | nll_loss 17.811 | wps 6154.5 | ups 0.6 | wpb 10335.5 | bsz 47.3 | num_updates 1226 | lr 6.1687e-05 | gnorm 1137.26 | loss_scale 2 | train_wall 70 | gb_free 36.8 | wall 2087
[2024-09-18 13:15:13,237][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:15:13,257][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 29
[2024-09-18 13:15:13,269][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:15:13,278][fairseq.trainer][INFO] - begin training epoch 29
[2024-09-18 13:15:13,278][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:16:23,866][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 29 @ 1270 updates
[2024-09-18 13:16:23,866][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:16:27,307][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:16:27,352][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 29 @ 1270 updates, score None) (writing took 3.4865617120085517 seconds)
[2024-09-18 13:16:27,353][fairseq_cli.train][INFO] - end of epoch 29 (average epoch stats below)
[2024-09-18 13:16:27,354][train][INFO] - epoch 029 | loss 3798.03 | ntokens 10334.8 | nsentences 47.2727 | nll_loss 17.373 | wps 6135.4 | ups 0.59 | wpb 10334.8 | bsz 47.3 | num_updates 1270 | lr 6.3865e-05 | gnorm 1141.91 | loss_scale 2 | train_wall 70 | gb_free 37.2 | wall 2161
[2024-09-18 13:16:27,354][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:16:27,395][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 30
[2024-09-18 13:16:27,410][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:16:27,419][fairseq.trainer][INFO] - begin training epoch 30
[2024-09-18 13:16:27,419][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:17:38,098][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 30 @ 1314 updates
[2024-09-18 13:17:38,099][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:17:41,529][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:17:41,597][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 30 @ 1314 updates, score None) (writing took 3.4988640369992936 seconds)
[2024-09-18 13:17:41,598][fairseq_cli.train][INFO] - end of epoch 30 (average epoch stats below)
[2024-09-18 13:17:41,599][train][INFO] - epoch 030 | loss 3668.83 | ntokens 10335 | nsentences 47.2727 | nll_loss 16.781 | wps 6124.9 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 1314 | lr 6.6043e-05 | gnorm 1178.64 | loss_scale 2 | train_wall 71 | gb_free 37.1 | wall 2235
[2024-09-18 13:17:41,600][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:17:41,620][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 31
[2024-09-18 13:17:41,636][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:17:41,646][fairseq.trainer][INFO] - begin training epoch 31
[2024-09-18 13:17:41,646][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:18:52,206][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 31 @ 1358 updates
[2024-09-18 13:18:52,207][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:18:55,733][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:18:55,804][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 31 @ 1358 updates, score None) (writing took 3.598109309008578 seconds)
[2024-09-18 13:18:55,804][fairseq_cli.train][INFO] - end of epoch 31 (average epoch stats below)
[2024-09-18 13:18:55,806][train][INFO] - epoch 031 | loss 3638.67 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 16.643 | wps 6128.3 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 1358 | lr 6.8221e-05 | gnorm 1159.18 | loss_scale 2 | train_wall 70 | gb_free 36.7 | wall 2309
[2024-09-18 13:18:55,806][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:18:55,822][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 32
[2024-09-18 13:18:55,835][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:18:55,846][fairseq.trainer][INFO] - begin training epoch 32
[2024-09-18 13:18:55,846][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:20:03,538][train_inner][INFO] - epoch 032:     42 / 44 loss=3728.75, ntokens=10336.6, nsentences=46.8, nll_loss=16.882, wps=6160.7, ups=0.6, wpb=10336.6, bsz=46.8, num_updates=1400, lr=7.03e-05, gnorm=1170.16, loss_scale=2, train_wall=321, gb_free=37.2, wall=2377
[2024-09-18 13:20:06,485][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 32 @ 1402 updates
[2024-09-18 13:20:06,486][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:20:10,377][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:20:10,450][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 32 @ 1402 updates, score None) (writing took 3.964116795992595 seconds)
[2024-09-18 13:20:10,450][fairseq_cli.train][INFO] - end of epoch 32 (average epoch stats below)
[2024-09-18 13:20:10,451][train][INFO] - epoch 032 | loss 3521.92 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 16.109 | wps 6092.3 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 1402 | lr 7.0399e-05 | gnorm 1184.89 | loss_scale 2 | train_wall 71 | gb_free 36.4 | wall 2384
[2024-09-18 13:20:10,452][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:20:10,467][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 33
[2024-09-18 13:20:10,479][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:20:10,489][fairseq.trainer][INFO] - begin training epoch 33
[2024-09-18 13:20:10,489][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:21:20,936][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 33 @ 1446 updates
[2024-09-18 13:21:20,937][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:21:24,594][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:21:24,665][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 33 @ 1446 updates, score None) (writing took 3.7287375849991804 seconds)
[2024-09-18 13:21:24,666][fairseq_cli.train][INFO] - end of epoch 33 (average epoch stats below)
[2024-09-18 13:21:24,667][train][INFO] - epoch 033 | loss 3438.79 | ntokens 10334.8 | nsentences 47.2727 | nll_loss 15.73 | wps 6127.2 | ups 0.59 | wpb 10334.8 | bsz 47.3 | num_updates 1446 | lr 7.2577e-05 | gnorm 1169.86 | loss_scale 2 | train_wall 70 | gb_free 37.4 | wall 2458
[2024-09-18 13:21:24,667][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:21:24,683][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 34
[2024-09-18 13:21:24,698][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:21:24,708][fairseq.trainer][INFO] - begin training epoch 34
[2024-09-18 13:21:24,708][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:22:35,180][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 34 @ 1490 updates
[2024-09-18 13:22:35,181][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:22:38,796][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:22:38,867][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 34 @ 1490 updates, score None) (writing took 3.6866862619936 seconds)
[2024-09-18 13:22:38,867][fairseq_cli.train][INFO] - end of epoch 34 (average epoch stats below)
[2024-09-18 13:22:38,868][train][INFO] - epoch 034 | loss 3362.65 | ntokens 10335 | nsentences 47.2727 | nll_loss 15.381 | wps 6128.5 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 1490 | lr 7.4755e-05 | gnorm 1143.25 | loss_scale 2 | train_wall 70 | gb_free 35.6 | wall 2532
[2024-09-18 13:22:38,869][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:22:38,884][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 35
[2024-09-18 13:22:38,897][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:22:38,908][fairseq.trainer][INFO] - begin training epoch 35
[2024-09-18 13:22:38,908][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:23:49,348][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 35 @ 1534 updates
[2024-09-18 13:23:49,348][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:23:52,755][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:23:52,828][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 35 @ 1534 updates, score None) (writing took 3.4802809819957474 seconds)
[2024-09-18 13:23:52,828][fairseq_cli.train][INFO] - end of epoch 35 (average epoch stats below)
[2024-09-18 13:23:52,829][train][INFO] - epoch 035 | loss 3247.91 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 14.856 | wps 6148.6 | ups 0.59 | wpb 10335.2 | bsz 47.3 | num_updates 1534 | lr 7.6933e-05 | gnorm 1118.78 | loss_scale 2 | train_wall 70 | gb_free 37.4 | wall 2606
[2024-09-18 13:23:52,830][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:23:52,845][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 36
[2024-09-18 13:23:52,858][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:23:52,867][fairseq.trainer][INFO] - begin training epoch 36
[2024-09-18 13:23:52,867][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:25:03,264][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 36 @ 1578 updates
[2024-09-18 13:25:03,264][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:25:06,710][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:25:06,781][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 36 @ 1578 updates, score None) (writing took 3.5171292110026116 seconds)
[2024-09-18 13:25:06,781][fairseq_cli.train][INFO] - end of epoch 36 (average epoch stats below)
[2024-09-18 13:25:06,782][train][INFO] - epoch 036 | loss 3092.07 | ntokens 10335.5 | nsentences 47.2727 | nll_loss 14.143 | wps 6149.5 | ups 0.59 | wpb 10335.5 | bsz 47.3 | num_updates 1578 | lr 7.9111e-05 | gnorm 1106.42 | loss_scale 2 | train_wall 70 | gb_free 37.3 | wall 2680
[2024-09-18 13:25:06,783][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:25:06,798][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 37
[2024-09-18 13:25:06,813][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:25:06,824][fairseq.trainer][INFO] - begin training epoch 37
[2024-09-18 13:25:06,824][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:25:41,850][train_inner][INFO] - epoch 037:     22 / 44 loss=3275.71, ntokens=10304.9, nsentences=47.16, nll_loss=14.991, wps=6091.9, ups=0.59, wpb=10304.9, bsz=47.2, num_updates=1600, lr=8.02e-05, gnorm=1121.46, loss_scale=2, train_wall=319, gb_free=37.1, wall=2715
[2024-09-18 13:26:17,226][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 37 @ 1622 updates
[2024-09-18 13:26:17,227][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:26:20,646][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:26:20,716][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 37 @ 1622 updates, score None) (writing took 3.490386111996486 seconds)
[2024-09-18 13:26:20,717][fairseq_cli.train][INFO] - end of epoch 37 (average epoch stats below)
[2024-09-18 13:26:20,718][train][INFO] - epoch 037 | loss 3100.83 | ntokens 10335.8 | nsentences 47.2727 | nll_loss 14.182 | wps 6151 | ups 0.6 | wpb 10335.8 | bsz 47.3 | num_updates 1622 | lr 8.1289e-05 | gnorm 1020.64 | loss_scale 2 | train_wall 70 | gb_free 35.7 | wall 2754
[2024-09-18 13:26:20,719][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:26:20,733][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 38
[2024-09-18 13:26:20,745][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:26:20,755][fairseq.trainer][INFO] - begin training epoch 38
[2024-09-18 13:26:20,755][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:27:31,356][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 38 @ 1666 updates
[2024-09-18 13:27:31,357][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:27:34,775][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:27:34,847][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 38 @ 1666 updates, score None) (writing took 3.490872344991658 seconds)
[2024-09-18 13:27:34,848][fairseq_cli.train][INFO] - end of epoch 38 (average epoch stats below)
[2024-09-18 13:27:34,849][train][INFO] - epoch 038 | loss 2923.12 | ntokens 10335 | nsentences 47.2727 | nll_loss 13.37 | wps 6134.4 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 1666 | lr 8.3467e-05 | gnorm 980.75 | loss_scale 2 | train_wall 70 | gb_free 37.2 | wall 2828
[2024-09-18 13:27:34,850][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:27:34,865][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 39
[2024-09-18 13:27:34,877][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:27:34,887][fairseq.trainer][INFO] - begin training epoch 39
[2024-09-18 13:27:34,888][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:28:45,496][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 39 @ 1710 updates
[2024-09-18 13:28:45,496][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:28:48,962][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:28:49,032][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 39 @ 1710 updates, score None) (writing took 3.536233025995898 seconds)
[2024-09-18 13:28:49,032][fairseq_cli.train][INFO] - end of epoch 39 (average epoch stats below)
[2024-09-18 13:28:49,033][train][INFO] - epoch 039 | loss 2873.04 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 13.141 | wps 6130 | ups 0.59 | wpb 10335.2 | bsz 47.3 | num_updates 1710 | lr 8.5645e-05 | gnorm 921.577 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 2902
[2024-09-18 13:28:49,034][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:28:49,049][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 40
[2024-09-18 13:28:49,063][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:28:49,072][fairseq.trainer][INFO] - begin training epoch 40
[2024-09-18 13:28:49,072][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:29:59,642][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 40 @ 1754 updates
[2024-09-18 13:29:59,643][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:30:03,108][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:30:03,181][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 40 @ 1754 updates, score None) (writing took 3.539211149007315 seconds)
[2024-09-18 13:30:03,182][fairseq_cli.train][INFO] - end of epoch 40 (average epoch stats below)
[2024-09-18 13:30:03,183][train][INFO] - epoch 040 | loss 2843.79 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 13.007 | wps 6132.9 | ups 0.59 | wpb 10335.2 | bsz 47.3 | num_updates 1754 | lr 8.7823e-05 | gnorm 848.089 | loss_scale 2 | train_wall 70 | gb_free 37.4 | wall 2977
[2024-09-18 13:30:03,184][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:30:03,199][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 41
[2024-09-18 13:30:03,213][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:30:03,223][fairseq.trainer][INFO] - begin training epoch 41
[2024-09-18 13:30:03,223][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:31:13,692][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 41 @ 1798 updates
[2024-09-18 13:31:13,692][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:31:17,163][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:31:17,231][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 41 @ 1798 updates, score None) (writing took 3.5393872870045016 seconds)
[2024-09-18 13:31:17,232][fairseq_cli.train][INFO] - end of epoch 41 (average epoch stats below)
[2024-09-18 13:31:17,233][train][INFO] - epoch 041 | loss 2724.06 | ntokens 10335.8 | nsentences 47.2727 | nll_loss 12.459 | wps 6141.5 | ups 0.59 | wpb 10335.8 | bsz 47.3 | num_updates 1798 | lr 9.0001e-05 | gnorm 803.148 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 3051
[2024-09-18 13:31:17,235][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:31:17,254][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 42
[2024-09-18 13:31:17,267][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:31:17,277][fairseq.trainer][INFO] - begin training epoch 42
[2024-09-18 13:31:17,277][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:31:20,622][train_inner][INFO] - epoch 042:      2 / 44 loss=2860.03, ntokens=10360.9, nsentences=47.4, nll_loss=13.084, wps=6116.8, ups=0.59, wpb=10360.9, bsz=47.4, num_updates=1800, lr=9.01e-05, gnorm=901.611, loss_scale=2, train_wall=320, gb_free=37.1, wall=3054
[2024-09-18 13:32:27,974][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 42 @ 1842 updates
[2024-09-18 13:32:27,975][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:32:31,441][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:32:31,515][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 42 @ 1842 updates, score None) (writing took 3.5405319029960083 seconds)
[2024-09-18 13:32:31,515][fairseq_cli.train][INFO] - end of epoch 42 (average epoch stats below)
[2024-09-18 13:32:31,516][train][INFO] - epoch 042 | loss 2659.2 | ntokens 10334.8 | nsentences 47.2727 | nll_loss 12.164 | wps 6121.7 | ups 0.59 | wpb 10334.8 | bsz 47.3 | num_updates 1842 | lr 9.2179e-05 | gnorm 741.994 | loss_scale 2 | train_wall 71 | gb_free 37.1 | wall 3125
[2024-09-18 13:32:31,517][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:32:31,532][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 43
[2024-09-18 13:32:31,544][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:32:31,555][fairseq.trainer][INFO] - begin training epoch 43
[2024-09-18 13:32:31,555][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:33:42,120][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 43 @ 1886 updates
[2024-09-18 13:33:42,121][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:33:45,571][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:33:45,638][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 43 @ 1886 updates, score None) (writing took 3.5176441429939587 seconds)
[2024-09-18 13:33:45,638][fairseq_cli.train][INFO] - end of epoch 43 (average epoch stats below)
[2024-09-18 13:33:45,639][train][INFO] - epoch 043 | loss 2585.94 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 11.828 | wps 6135.2 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 1886 | lr 9.4357e-05 | gnorm 681.394 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 3199
[2024-09-18 13:33:45,640][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:33:45,660][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 44
[2024-09-18 13:33:45,678][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:33:45,688][fairseq.trainer][INFO] - begin training epoch 44
[2024-09-18 13:33:45,688][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:34:56,131][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 44 @ 1930 updates
[2024-09-18 13:34:56,131][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:34:59,568][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:34:59,638][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 44 @ 1930 updates, score None) (writing took 3.507618960997206 seconds)
[2024-09-18 13:34:59,639][fairseq_cli.train][INFO] - end of epoch 44 (average epoch stats below)
[2024-09-18 13:34:59,640][train][INFO] - epoch 044 | loss 2453.13 | ntokens 10335.3 | nsentences 47.2727 | nll_loss 11.22 | wps 6145.3 | ups 0.59 | wpb 10335.3 | bsz 47.3 | num_updates 1930 | lr 9.6535e-05 | gnorm 624.511 | loss_scale 2 | train_wall 70 | gb_free 36 | wall 3273
[2024-09-18 13:34:59,641][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:34:59,656][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 45
[2024-09-18 13:34:59,668][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:34:59,679][fairseq.trainer][INFO] - begin training epoch 45
[2024-09-18 13:34:59,679][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:36:10,001][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 45 @ 1974 updates
[2024-09-18 13:36:10,002][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:36:13,462][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:36:13,535][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 45 @ 1974 updates, score None) (writing took 3.5336009860038757 seconds)
[2024-09-18 13:36:13,535][fairseq_cli.train][INFO] - end of epoch 45 (average epoch stats below)
[2024-09-18 13:36:13,536][train][INFO] - epoch 045 | loss 2508.77 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 11.475 | wps 6154 | ups 0.6 | wpb 10335.2 | bsz 47.3 | num_updates 1974 | lr 9.8713e-05 | gnorm 576.433 | loss_scale 2 | train_wall 70 | gb_free 37.2 | wall 3347
[2024-09-18 13:36:13,537][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:36:13,552][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 46
[2024-09-18 13:36:13,564][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:36:13,574][fairseq.trainer][INFO] - begin training epoch 46
[2024-09-18 13:36:13,574][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:36:55,558][train_inner][INFO] - epoch 046:     26 / 44 loss=2512.38, ntokens=10360.8, nsentences=47.36, nll_loss=11.484, wps=6186.8, ups=0.6, wpb=10360.8, bsz=47.4, num_updates=2000, lr=0.0001, gnorm=635.6, loss_scale=2, train_wall=320, gb_free=36.3, wall=3389
[2024-09-18 13:37:24,008][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 46 @ 2018 updates
[2024-09-18 13:37:24,008][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:37:27,430][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:37:27,500][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 46 @ 2018 updates, score None) (writing took 3.492872293005348 seconds)
[2024-09-18 13:37:27,501][fairseq_cli.train][INFO] - end of epoch 46 (average epoch stats below)
[2024-09-18 13:37:27,502][train][INFO] - epoch 046 | loss 2367.65 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 10.829 | wps 6148.3 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 2018 | lr 0.0001 | gnorm 520.358 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 3421
[2024-09-18 13:37:27,503][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:37:27,518][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 47
[2024-09-18 13:37:27,533][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:37:27,545][fairseq.trainer][INFO] - begin training epoch 47
[2024-09-18 13:37:27,545][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:38:38,197][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 47 @ 2062 updates
[2024-09-18 13:38:38,198][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:38:41,662][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:38:41,735][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 47 @ 2062 updates, score None) (writing took 3.538377583012334 seconds)
[2024-09-18 13:38:41,736][fairseq_cli.train][INFO] - end of epoch 47 (average epoch stats below)
[2024-09-18 13:38:41,737][train][INFO] - epoch 047 | loss 2325.04 | ntokens 10335.5 | nsentences 47.2727 | nll_loss 10.634 | wps 6126.1 | ups 0.59 | wpb 10335.5 | bsz 47.3 | num_updates 2062 | lr 0.0001 | gnorm 496.972 | loss_scale 2 | train_wall 71 | gb_free 37.1 | wall 3495
[2024-09-18 13:38:41,738][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:38:41,753][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 48
[2024-09-18 13:38:41,765][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:38:41,775][fairseq.trainer][INFO] - begin training epoch 48
[2024-09-18 13:38:41,775][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:39:52,343][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 48 @ 2106 updates
[2024-09-18 13:39:52,344][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:39:55,769][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:39:55,816][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 48 @ 2106 updates, score None) (writing took 3.4727004549931735 seconds)
[2024-09-18 13:39:55,817][fairseq_cli.train][INFO] - end of epoch 48 (average epoch stats below)
[2024-09-18 13:39:55,818][train][INFO] - epoch 048 | loss 2365.76 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 10.821 | wps 6138.6 | ups 0.59 | wpb 10335.2 | bsz 47.3 | num_updates 2106 | lr 0.0001 | gnorm 462.704 | loss_scale 2 | train_wall 70 | gb_free 38 | wall 3569
[2024-09-18 13:39:55,818][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:39:55,860][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 49
[2024-09-18 13:39:55,873][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:39:55,884][fairseq.trainer][INFO] - begin training epoch 49
[2024-09-18 13:39:55,884][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:41:06,385][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 49 @ 2150 updates
[2024-09-18 13:41:06,386][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:41:09,880][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:41:09,952][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 49 @ 2150 updates, score None) (writing took 3.5669640630076174 seconds)
[2024-09-18 13:41:09,953][fairseq_cli.train][INFO] - end of epoch 49 (average epoch stats below)
[2024-09-18 13:41:09,954][train][INFO] - epoch 049 | loss 2268.99 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 10.378 | wps 6134.1 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 2150 | lr 0.0001 | gnorm 436.918 | loss_scale 2 | train_wall 70 | gb_free 37.2 | wall 3643
[2024-09-18 13:41:09,955][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:41:09,970][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 50
[2024-09-18 13:41:09,985][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:41:09,996][fairseq.trainer][INFO] - begin training epoch 50
[2024-09-18 13:41:09,996][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:42:20,765][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 50 @ 2194 updates
[2024-09-18 13:42:20,766][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:42:24,190][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:42:24,264][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 50 @ 2194 updates, score None) (writing took 3.498170453007333 seconds)
[2024-09-18 13:42:24,264][fairseq_cli.train][INFO] - end of epoch 50 (average epoch stats below)
[2024-09-18 13:42:24,265][train][INFO] - epoch 050 | loss 2315.47 | ntokens 10335 | nsentences 47.2727 | nll_loss 10.591 | wps 6119.4 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 2194 | lr 0.0001 | gnorm 416.775 | loss_scale 2 | train_wall 71 | gb_free 35.6 | wall 3718
[2024-09-18 13:42:24,266][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:42:24,281][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 51
[2024-09-18 13:42:24,294][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:42:24,305][fairseq.trainer][INFO] - begin training epoch 51
[2024-09-18 13:42:24,305][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:42:34,269][train_inner][INFO] - epoch 051:      6 / 44 loss=2329.38, ntokens=10310.1, nsentences=47.28, nll_loss=10.682, wps=6087.9, ups=0.59, wpb=10310.1, bsz=47.3, num_updates=2200, lr=0.0001, gnorm=457.473, loss_scale=2, train_wall=320, gb_free=37.1, wall=3728
[2024-09-18 13:43:35,062][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 51 @ 2238 updates
[2024-09-18 13:43:35,062][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:43:38,527][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:43:38,598][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 51 @ 2238 updates, score None) (writing took 3.536873630990158 seconds)
[2024-09-18 13:43:38,599][fairseq_cli.train][INFO] - end of epoch 51 (average epoch stats below)
[2024-09-18 13:43:38,600][train][INFO] - epoch 051 | loss 2192.88 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 10.03 | wps 6117.8 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 2238 | lr 0.0001 | gnorm 395.584 | loss_scale 2 | train_wall 71 | gb_free 37.1 | wall 3792
[2024-09-18 13:43:38,601][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:43:38,616][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 52
[2024-09-18 13:43:38,628][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:43:38,642][fairseq.trainer][INFO] - begin training epoch 52
[2024-09-18 13:43:38,642][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:44:49,055][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 52 @ 2282 updates
[2024-09-18 13:44:49,056][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:44:52,471][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:44:52,545][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 52 @ 2282 updates, score None) (writing took 3.489996008007438 seconds)
[2024-09-18 13:44:52,546][fairseq_cli.train][INFO] - end of epoch 52 (average epoch stats below)
[2024-09-18 13:44:52,547][train][INFO] - epoch 052 | loss 2173.98 | ntokens 10335.5 | nsentences 47.2727 | nll_loss 9.943 | wps 6150 | ups 0.6 | wpb 10335.5 | bsz 47.3 | num_updates 2282 | lr 0.0001 | gnorm 388.375 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 3866
[2024-09-18 13:44:52,547][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:44:52,563][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 53
[2024-09-18 13:44:52,575][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:44:52,586][fairseq.trainer][INFO] - begin training epoch 53
[2024-09-18 13:44:52,586][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:46:02,957][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 53 @ 2326 updates
[2024-09-18 13:46:02,958][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:46:06,390][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:46:06,462][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 53 @ 2326 updates, score None) (writing took 3.504398211996886 seconds)
[2024-09-18 13:46:06,462][fairseq_cli.train][INFO] - end of epoch 53 (average epoch stats below)
[2024-09-18 13:46:06,463][train][INFO] - epoch 053 | loss 2121.69 | ntokens 10334.8 | nsentences 47.2727 | nll_loss 9.705 | wps 6152 | ups 0.6 | wpb 10334.8 | bsz 47.3 | num_updates 2326 | lr 0.0001 | gnorm 342.338 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 3940
[2024-09-18 13:46:06,464][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:46:06,479][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 54
[2024-09-18 13:46:06,491][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:46:06,505][fairseq.trainer][INFO] - begin training epoch 54
[2024-09-18 13:46:06,505][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:47:17,100][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 54 @ 2370 updates
[2024-09-18 13:47:17,101][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:47:20,529][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:47:20,574][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 54 @ 2370 updates, score None) (writing took 3.4733495530090295 seconds)
[2024-09-18 13:47:20,574][fairseq_cli.train][INFO] - end of epoch 54 (average epoch stats below)
[2024-09-18 13:47:20,575][train][INFO] - epoch 054 | loss 2122.97 | ntokens 10335 | nsentences 47.2727 | nll_loss 9.711 | wps 6135.9 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 2370 | lr 0.0001 | gnorm 360.43 | loss_scale 2 | train_wall 70 | gb_free 36.4 | wall 4014
[2024-09-18 13:47:20,576][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:47:20,618][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 55
[2024-09-18 13:47:20,630][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:47:20,640][fairseq.trainer][INFO] - begin training epoch 55
[2024-09-18 13:47:20,641][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:48:09,181][train_inner][INFO] - epoch 055:     30 / 44 loss=2132.86, ntokens=10358.6, nsentences=47.2, nll_loss=9.719, wps=6185.9, ups=0.6, wpb=10358.6, bsz=47.2, num_updates=2400, lr=0.0001, gnorm=362.912, loss_scale=2, train_wall=320, gb_free=37.2, wall=4063
[2024-09-18 13:48:31,083][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 55 @ 2414 updates
[2024-09-18 13:48:31,084][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:48:34,524][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:48:34,598][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 55 @ 2414 updates, score None) (writing took 3.5145690119970823 seconds)
[2024-09-18 13:48:34,598][fairseq_cli.train][INFO] - end of epoch 55 (average epoch stats below)
[2024-09-18 13:48:34,599][train][INFO] - epoch 055 | loss 2054.21 | ntokens 10335.3 | nsentences 47.2727 | nll_loss 9.396 | wps 6143.4 | ups 0.59 | wpb 10335.3 | bsz 47.3 | num_updates 2414 | lr 0.0001 | gnorm 313.933 | loss_scale 2 | train_wall 70 | gb_free 37.4 | wall 4088
[2024-09-18 13:48:34,600][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:48:34,615][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 56
[2024-09-18 13:48:34,627][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:48:34,638][fairseq.trainer][INFO] - begin training epoch 56
[2024-09-18 13:48:34,638][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:49:45,298][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 56 @ 2458 updates
[2024-09-18 13:49:45,299][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:49:48,746][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:49:48,819][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 56 @ 2458 updates, score None) (writing took 3.5211674269958166 seconds)
[2024-09-18 13:49:48,820][fairseq_cli.train][INFO] - end of epoch 56 (average epoch stats below)
[2024-09-18 13:49:48,821][train][INFO] - epoch 056 | loss 2065.3 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 9.447 | wps 6127 | ups 0.59 | wpb 10335.2 | bsz 47.3 | num_updates 2458 | lr 0.0001 | gnorm 307.341 | loss_scale 2 | train_wall 71 | gb_free 35.8 | wall 4162
[2024-09-18 13:49:48,822][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:49:48,837][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 57
[2024-09-18 13:49:48,850][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:49:48,861][fairseq.trainer][INFO] - begin training epoch 57
[2024-09-18 13:49:48,861][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:50:59,791][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 57 @ 2502 updates
[2024-09-18 13:50:59,792][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:51:03,221][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:51:03,292][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 57 @ 2502 updates, score None) (writing took 3.5007599289965583 seconds)
[2024-09-18 13:51:03,293][fairseq_cli.train][INFO] - end of epoch 57 (average epoch stats below)
[2024-09-18 13:51:03,294][train][INFO] - epoch 057 | loss 1997 | ntokens 10335.5 | nsentences 47.2727 | nll_loss 9.134 | wps 6106.5 | ups 0.59 | wpb 10335.5 | bsz 47.3 | num_updates 2502 | lr 0.0001 | gnorm 304.164 | loss_scale 2 | train_wall 71 | gb_free 37.1 | wall 4237
[2024-09-18 13:51:03,294][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:51:03,310][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 58
[2024-09-18 13:51:03,322][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:51:03,333][fairseq.trainer][INFO] - begin training epoch 58
[2024-09-18 13:51:03,333][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:52:14,043][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 58 @ 2546 updates
[2024-09-18 13:52:14,043][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:52:17,480][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:52:17,554][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 58 @ 2546 updates, score None) (writing took 3.5115508049930213 seconds)
[2024-09-18 13:52:17,555][fairseq_cli.train][INFO] - end of epoch 58 (average epoch stats below)
[2024-09-18 13:52:17,556][train][INFO] - epoch 058 | loss 1976.2 | ntokens 10335.5 | nsentences 47.2727 | nll_loss 9.039 | wps 6123.8 | ups 0.59 | wpb 10335.5 | bsz 47.3 | num_updates 2546 | lr 0.0001 | gnorm 281.045 | loss_scale 2 | train_wall 71 | gb_free 36.4 | wall 4311
[2024-09-18 13:52:17,557][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:52:17,573][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 59
[2024-09-18 13:52:17,585][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:52:17,598][fairseq.trainer][INFO] - begin training epoch 59
[2024-09-18 13:52:17,598][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:53:28,277][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 59 @ 2590 updates
[2024-09-18 13:53:28,278][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:53:31,717][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:53:31,788][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 59 @ 2590 updates, score None) (writing took 3.5105413449928164 seconds)
[2024-09-18 13:53:31,788][fairseq_cli.train][INFO] - end of epoch 59 (average epoch stats below)
[2024-09-18 13:53:31,789][train][INFO] - epoch 059 | loss 1941.84 | ntokens 10335.3 | nsentences 47.2727 | nll_loss 8.882 | wps 6126.1 | ups 0.59 | wpb 10335.3 | bsz 47.3 | num_updates 2590 | lr 0.0001 | gnorm 285.761 | loss_scale 2 | train_wall 71 | gb_free 36.4 | wall 4385
[2024-09-18 13:53:31,790][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:53:31,805][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 60
[2024-09-18 13:53:31,817][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:53:31,831][fairseq.trainer][INFO] - begin training epoch 60
[2024-09-18 13:53:31,831][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:53:48,003][train_inner][INFO] - epoch 060:     10 / 44 loss=1989.57, ntokens=10329.2, nsentences=47.56, nll_loss=9.161, wps=6097.2, ups=0.59, wpb=10329.2, bsz=47.6, num_updates=2600, lr=0.0001, gnorm=294.914, loss_scale=2, train_wall=320, gb_free=37.2, wall=4401
[2024-09-18 13:54:42,487][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 60 @ 2634 updates
[2024-09-18 13:54:42,488][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:54:45,950][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:54:45,997][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 60 @ 2634 updates, score None) (writing took 3.5097856990032597 seconds)
[2024-09-18 13:54:45,998][fairseq_cli.train][INFO] - end of epoch 60 (average epoch stats below)
[2024-09-18 13:54:45,999][train][INFO] - epoch 060 | loss 2003.79 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 9.165 | wps 6128.1 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 2634 | lr 0.0001 | gnorm 292.229 | loss_scale 2 | train_wall 71 | gb_free 37.1 | wall 4459
[2024-09-18 13:54:45,999][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:54:46,041][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 61
[2024-09-18 13:54:46,053][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:54:46,065][fairseq.trainer][INFO] - begin training epoch 61
[2024-09-18 13:54:46,065][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:55:56,554][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 61 @ 2678 updates
[2024-09-18 13:55:56,555][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:56:00,000][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:56:00,072][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 61 @ 2678 updates, score None) (writing took 3.517834354992374 seconds)
[2024-09-18 13:56:00,072][fairseq_cli.train][INFO] - end of epoch 61 (average epoch stats below)
[2024-09-18 13:56:00,073][train][INFO] - epoch 061 | loss 1933.61 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 8.844 | wps 6139.2 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 2678 | lr 0.0001 | gnorm 282.638 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 4533
[2024-09-18 13:56:00,074][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:56:00,089][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 62
[2024-09-18 13:56:00,101][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:56:00,116][fairseq.trainer][INFO] - begin training epoch 62
[2024-09-18 13:56:00,116][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:57:11,125][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 62 @ 2722 updates
[2024-09-18 13:57:11,125][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:57:14,572][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:57:14,642][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 62 @ 2722 updates, score None) (writing took 3.5171221389900893 seconds)
[2024-09-18 13:57:14,642][fairseq_cli.train][INFO] - end of epoch 62 (average epoch stats below)
[2024-09-18 13:57:14,643][train][INFO] - epoch 062 | loss 1941.81 | ntokens 10335.5 | nsentences 47.2727 | nll_loss 8.881 | wps 6098.6 | ups 0.59 | wpb 10335.5 | bsz 47.3 | num_updates 2722 | lr 0.0001 | gnorm 282.72 | loss_scale 2 | train_wall 71 | gb_free 36.7 | wall 4608
[2024-09-18 13:57:14,644][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:57:14,659][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 63
[2024-09-18 13:57:14,674][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:57:14,687][fairseq.trainer][INFO] - begin training epoch 63
[2024-09-18 13:57:14,687][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:58:25,003][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 63 @ 2766 updates
[2024-09-18 13:58:25,004][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:58:28,457][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:58:28,527][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 63 @ 2766 updates, score None) (writing took 3.5235007779992884 seconds)
[2024-09-18 13:58:28,527][fairseq_cli.train][INFO] - end of epoch 63 (average epoch stats below)
[2024-09-18 13:58:28,528][train][INFO] - epoch 063 | loss 1939.65 | ntokens 10335.3 | nsentences 47.2727 | nll_loss 8.872 | wps 6154.9 | ups 0.6 | wpb 10335.3 | bsz 47.3 | num_updates 2766 | lr 0.0001 | gnorm 282.748 | loss_scale 2 | train_wall 70 | gb_free 37.2 | wall 4682
[2024-09-18 13:58:28,529][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:58:28,545][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 64
[2024-09-18 13:58:28,557][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:58:28,569][fairseq.trainer][INFO] - begin training epoch 64
[2024-09-18 13:58:28,569][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 13:59:23,342][train_inner][INFO] - epoch 064:     34 / 44 loss=1957.55, ntokens=10335.1, nsentences=47.04, nll_loss=8.91, wps=6164, ups=0.6, wpb=10335.1, bsz=47, num_updates=2800, lr=0.0001, gnorm=285.139, loss_scale=2, train_wall=321, gb_free=35.9, wall=4737
[2024-09-18 13:59:39,136][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 64 @ 2810 updates
[2024-09-18 13:59:39,137][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:59:42,580][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 13:59:42,651][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 64 @ 2810 updates, score None) (writing took 3.5148792029940523 seconds)
[2024-09-18 13:59:42,651][fairseq_cli.train][INFO] - end of epoch 64 (average epoch stats below)
[2024-09-18 13:59:42,652][train][INFO] - epoch 064 | loss 1901.57 | ntokens 10334.8 | nsentences 47.2727 | nll_loss 8.698 | wps 6134.8 | ups 0.59 | wpb 10334.8 | bsz 47.3 | num_updates 2810 | lr 0.0001 | gnorm 280.689 | loss_scale 2 | train_wall 70 | gb_free 37.2 | wall 4756
[2024-09-18 13:59:42,653][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 13:59:42,669][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 65
[2024-09-18 13:59:42,684][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 13:59:42,699][fairseq.trainer][INFO] - begin training epoch 65
[2024-09-18 13:59:42,699][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:00:53,115][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 65 @ 2854 updates
[2024-09-18 14:00:53,115][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:00:56,563][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:00:56,635][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 65 @ 2854 updates, score None) (writing took 3.5207893179904204 seconds)
[2024-09-18 14:00:56,636][fairseq_cli.train][INFO] - end of epoch 65 (average epoch stats below)
[2024-09-18 14:00:56,637][train][INFO] - epoch 065 | loss 1885.07 | ntokens 10335.8 | nsentences 47.2727 | nll_loss 8.622 | wps 6146.9 | ups 0.59 | wpb 10335.8 | bsz 47.3 | num_updates 2854 | lr 0.0001 | gnorm 271.868 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 4830
[2024-09-18 14:00:56,638][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:00:56,653][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 66
[2024-09-18 14:00:56,665][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:00:56,677][fairseq.trainer][INFO] - begin training epoch 66
[2024-09-18 14:00:56,677][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:02:07,536][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 66 @ 2898 updates
[2024-09-18 14:02:07,537][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:02:10,978][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:02:11,025][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 66 @ 2898 updates, score None) (writing took 3.4889841449912637 seconds)
[2024-09-18 14:02:11,026][fairseq_cli.train][INFO] - end of epoch 66 (average epoch stats below)
[2024-09-18 14:02:11,027][train][INFO] - epoch 066 | loss 1863.83 | ntokens 10335 | nsentences 47.2727 | nll_loss 8.525 | wps 6113 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 2898 | lr 0.0001 | gnorm 279.491 | loss_scale 2 | train_wall 71 | gb_free 35.9 | wall 4904
[2024-09-18 14:02:11,028][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:02:11,069][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 67
[2024-09-18 14:02:11,081][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:02:11,094][fairseq.trainer][INFO] - begin training epoch 67
[2024-09-18 14:02:11,094][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:03:21,457][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 67 @ 2942 updates
[2024-09-18 14:03:21,457][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:03:24,894][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:03:24,965][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 67 @ 2942 updates, score None) (writing took 3.508265829994343 seconds)
[2024-09-18 14:03:24,965][fairseq_cli.train][INFO] - end of epoch 67 (average epoch stats below)
[2024-09-18 14:03:24,967][train][INFO] - epoch 067 | loss 1823.89 | ntokens 10335 | nsentences 47.2727 | nll_loss 8.343 | wps 6150.3 | ups 0.6 | wpb 10335 | bsz 47.3 | num_updates 2942 | lr 0.0001 | gnorm 262.161 | loss_scale 2 | train_wall 70 | gb_free 35.9 | wall 4978
[2024-09-18 14:03:24,968][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:03:24,983][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 68
[2024-09-18 14:03:24,995][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:03:25,007][fairseq.trainer][INFO] - begin training epoch 68
[2024-09-18 14:03:25,008][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:04:35,870][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 68 @ 2986 updates
[2024-09-18 14:04:35,871][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:04:39,270][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:04:39,317][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 68 @ 2986 updates, score None) (writing took 3.4464679209922906 seconds)
[2024-09-18 14:04:39,317][fairseq_cli.train][INFO] - end of epoch 68 (average epoch stats below)
[2024-09-18 14:04:39,318][train][INFO] - epoch 068 | loss 1858.42 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 8.5 | wps 6116.4 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 2986 | lr 0.0001 | gnorm 266.777 | loss_scale 2 | train_wall 71 | gb_free 35.6 | wall 5053
[2024-09-18 14:04:39,319][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:04:39,359][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 69
[2024-09-18 14:04:39,371][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:04:39,386][fairseq.trainer][INFO] - begin training epoch 69
[2024-09-18 14:04:39,386][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:05:01,795][train_inner][INFO] - epoch 069:     14 / 44 loss=1860.42, ntokens=10314.4, nsentences=47.04, nll_loss=8.485, wps=6095, ups=0.59, wpb=10314.4, bsz=47, num_updates=3000, lr=0.0001, gnorm=270.089, loss_scale=2, train_wall=320, gb_free=37.2, wall=5075
[2024-09-18 14:05:49,893][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 69 @ 3030 updates
[2024-09-18 14:05:49,893][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:05:53,363][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:05:53,434][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 69 @ 3030 updates, score None) (writing took 3.541047284990782 seconds)
[2024-09-18 14:05:53,434][fairseq_cli.train][INFO] - end of epoch 69 (average epoch stats below)
[2024-09-18 14:05:53,435][train][INFO] - epoch 069 | loss 1742.29 | ntokens 10334.8 | nsentences 47.2727 | nll_loss 7.969 | wps 6135.4 | ups 0.59 | wpb 10334.8 | bsz 47.3 | num_updates 3030 | lr 0.0001 | gnorm 249.63 | loss_scale 2 | train_wall 70 | gb_free 35.8 | wall 5127
[2024-09-18 14:05:53,436][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:05:53,451][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 70
[2024-09-18 14:05:53,464][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:05:53,477][fairseq.trainer][INFO] - begin training epoch 70
[2024-09-18 14:05:53,477][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:07:03,521][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 70 @ 3074 updates
[2024-09-18 14:07:03,522][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:07:07,003][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:07:07,073][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 70 @ 3074 updates, score None) (writing took 3.5521582340006717 seconds)
[2024-09-18 14:07:07,074][fairseq_cli.train][INFO] - end of epoch 70 (average epoch stats below)
[2024-09-18 14:07:07,075][train][INFO] - epoch 070 | loss 1763.74 | ntokens 10335 | nsentences 47.2727 | nll_loss 8.067 | wps 6175.3 | ups 0.6 | wpb 10335 | bsz 47.3 | num_updates 3074 | lr 0.0001 | gnorm 246.788 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 5200
[2024-09-18 14:07:07,076][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:07:07,091][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 71
[2024-09-18 14:07:07,103][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:07:07,119][fairseq.trainer][INFO] - begin training epoch 71
[2024-09-18 14:07:07,119][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:08:17,730][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 71 @ 3118 updates
[2024-09-18 14:08:17,731][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:08:21,188][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:08:21,260][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 71 @ 3118 updates, score None) (writing took 3.52976318000583 seconds)
[2024-09-18 14:08:21,260][fairseq_cli.train][INFO] - end of epoch 71 (average epoch stats below)
[2024-09-18 14:08:21,261][train][INFO] - epoch 071 | loss 1746.15 | ntokens 10335.5 | nsentences 47.2727 | nll_loss 7.987 | wps 6130.1 | ups 0.59 | wpb 10335.5 | bsz 47.3 | num_updates 3118 | lr 0.0001 | gnorm 251.198 | loss_scale 2 | train_wall 70 | gb_free 36.4 | wall 5275
[2024-09-18 14:08:21,262][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:08:21,278][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 72
[2024-09-18 14:08:21,290][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:08:21,302][fairseq.trainer][INFO] - begin training epoch 72
[2024-09-18 14:08:21,303][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:09:32,106][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 72 @ 3162 updates
[2024-09-18 14:09:32,106][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:09:35,534][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:09:35,580][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 72 @ 3162 updates, score None) (writing took 3.4743323379952926 seconds)
[2024-09-18 14:09:35,580][fairseq_cli.train][INFO] - end of epoch 72 (average epoch stats below)
[2024-09-18 14:09:35,581][train][INFO] - epoch 072 | loss 1727.66 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 7.902 | wps 6118.9 | ups 0.59 | wpb 10335.2 | bsz 47.3 | num_updates 3162 | lr 0.0001 | gnorm 236.516 | loss_scale 2 | train_wall 71 | gb_free 37.7 | wall 5349
[2024-09-18 14:09:35,582][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:09:35,624][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 73
[2024-09-18 14:09:35,636][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:09:35,648][fairseq.trainer][INFO] - begin training epoch 73
[2024-09-18 14:09:35,648][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:10:36,526][train_inner][INFO] - epoch 073:     38 / 44 loss=1730.27, ntokens=10342.8, nsentences=47.44, nll_loss=7.936, wps=6179.8, ups=0.6, wpb=10342.8, bsz=47.4, num_updates=3200, lr=0.0001, gnorm=245.06, loss_scale=2, train_wall=320, gb_free=37.1, wall=5410
[2024-09-18 14:10:46,049][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 73 @ 3206 updates
[2024-09-18 14:10:46,049][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:10:49,513][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:10:49,586][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 73 @ 3206 updates, score None) (writing took 3.537311720006983 seconds)
[2024-09-18 14:10:49,586][fairseq_cli.train][INFO] - end of epoch 73 (average epoch stats below)
[2024-09-18 14:10:49,587][train][INFO] - epoch 073 | loss 1696.1 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 7.758 | wps 6144.9 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 3206 | lr 0.0001 | gnorm 244.753 | loss_scale 2 | train_wall 70 | gb_free 35.6 | wall 5423
[2024-09-18 14:10:49,588][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:10:49,603][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 74
[2024-09-18 14:10:49,615][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:10:49,629][fairseq.trainer][INFO] - begin training epoch 74
[2024-09-18 14:10:49,629][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:12:00,133][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 74 @ 3250 updates
[2024-09-18 14:12:00,134][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:12:03,571][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:12:03,643][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 74 @ 3250 updates, score None) (writing took 3.509550524991937 seconds)
[2024-09-18 14:12:03,643][fairseq_cli.train][INFO] - end of epoch 74 (average epoch stats below)
[2024-09-18 14:12:03,644][train][INFO] - epoch 074 | loss 1676.42 | ntokens 10334.8 | nsentences 47.2727 | nll_loss 7.668 | wps 6140.4 | ups 0.59 | wpb 10334.8 | bsz 47.3 | num_updates 3250 | lr 0.0001 | gnorm 232.326 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 5497
[2024-09-18 14:12:03,645][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:12:03,660][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 75
[2024-09-18 14:12:03,674][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:12:03,688][fairseq.trainer][INFO] - begin training epoch 75
[2024-09-18 14:12:03,689][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:12:47,349][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-09-18 14:13:14,265][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 75 @ 3293 updates
[2024-09-18 14:13:14,266][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:13:17,718][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:13:17,790][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 75 @ 3293 updates, score None) (writing took 3.5243111529998714 seconds)
[2024-09-18 14:13:17,790][fairseq_cli.train][INFO] - end of epoch 75 (average epoch stats below)
[2024-09-18 14:13:17,791][train][INFO] - epoch 075 | loss 1738.51 | ntokens 10330 | nsentences 47.4419 | nll_loss 7.984 | wps 5990.8 | ups 0.58 | wpb 10330 | bsz 47.4 | num_updates 3293 | lr 0.0001 | gnorm 259.619 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 5571
[2024-09-18 14:13:17,792][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:13:17,807][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 76
[2024-09-18 14:13:17,821][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:13:17,834][fairseq.trainer][INFO] - begin training epoch 76
[2024-09-18 14:13:17,834][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:14:28,630][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 76 @ 3337 updates
[2024-09-18 14:14:28,631][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:14:32,091][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:14:32,162][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 76 @ 3337 updates, score None) (writing took 3.5323321430041688 seconds)
[2024-09-18 14:14:32,163][fairseq_cli.train][INFO] - end of epoch 76 (average epoch stats below)
[2024-09-18 14:14:32,164][train][INFO] - epoch 076 | loss 1731.91 | ntokens 10335 | nsentences 47.2727 | nll_loss 7.922 | wps 6114.4 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 3337 | lr 0.0001 | gnorm 243.742 | loss_scale 2 | train_wall 71 | gb_free 36.3 | wall 5646
[2024-09-18 14:14:32,165][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:14:32,180][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 77
[2024-09-18 14:14:32,193][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:14:32,206][fairseq.trainer][INFO] - begin training epoch 77
[2024-09-18 14:14:32,206][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:15:42,832][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 77 @ 3381 updates
[2024-09-18 14:15:42,833][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:15:46,288][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:15:46,359][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 77 @ 3381 updates, score None) (writing took 3.5268422100052703 seconds)
[2024-09-18 14:15:46,360][fairseq_cli.train][INFO] - end of epoch 77 (average epoch stats below)
[2024-09-18 14:15:46,361][train][INFO] - epoch 077 | loss 1662.67 | ntokens 10335 | nsentences 47.2727 | nll_loss 7.605 | wps 6128.9 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 3381 | lr 0.0001 | gnorm 232.84 | loss_scale 2 | train_wall 71 | gb_free 35.8 | wall 5720
[2024-09-18 14:15:46,362][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:15:46,377][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 78
[2024-09-18 14:15:46,389][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:15:46,401][fairseq.trainer][INFO] - begin training epoch 78
[2024-09-18 14:15:46,401][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:16:16,624][train_inner][INFO] - epoch 078:     19 / 44 loss=1701.93, ntokens=10336.4, nsentences=47.24, nll_loss=7.778, wps=6078.5, ups=0.59, wpb=10336.4, bsz=47.2, num_updates=3400, lr=0.0001, gnorm=240.866, loss_scale=2, train_wall=322, gb_free=37.1, wall=5750
[2024-09-18 14:16:56,829][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 78 @ 3425 updates
[2024-09-18 14:16:56,830][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:17:00,266][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:17:00,338][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 78 @ 3425 updates, score None) (writing took 3.508949648006819 seconds)
[2024-09-18 14:17:00,339][fairseq_cli.train][INFO] - end of epoch 78 (average epoch stats below)
[2024-09-18 14:17:00,340][train][INFO] - epoch 078 | loss 1657.13 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 7.58 | wps 6147.1 | ups 0.59 | wpb 10335.2 | bsz 47.3 | num_updates 3425 | lr 0.0001 | gnorm 225.24 | loss_scale 2 | train_wall 70 | gb_free 37.3 | wall 5794
[2024-09-18 14:17:00,341][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:17:00,356][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 79
[2024-09-18 14:17:00,368][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:17:00,381][fairseq.trainer][INFO] - begin training epoch 79
[2024-09-18 14:17:00,381][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:18:11,311][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 79 @ 3469 updates
[2024-09-18 14:18:11,312][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:18:14,745][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:18:14,815][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 79 @ 3469 updates, score None) (writing took 3.504329987001256 seconds)
[2024-09-18 14:18:14,816][fairseq_cli.train][INFO] - end of epoch 79 (average epoch stats below)
[2024-09-18 14:18:14,817][train][INFO] - epoch 079 | loss 1632.1 | ntokens 10335.8 | nsentences 47.2727 | nll_loss 7.465 | wps 6106.3 | ups 0.59 | wpb 10335.8 | bsz 47.3 | num_updates 3469 | lr 0.0001 | gnorm 212.872 | loss_scale 2 | train_wall 71 | gb_free 35.7 | wall 5868
[2024-09-18 14:18:14,818][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:18:14,833][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 80
[2024-09-18 14:18:14,846][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:18:14,858][fairseq.trainer][INFO] - begin training epoch 80
[2024-09-18 14:18:14,858][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:19:25,392][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 80 @ 3513 updates
[2024-09-18 14:19:25,393][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:19:28,861][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:19:28,932][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 80 @ 3513 updates, score None) (writing took 3.5400275329884607 seconds)
[2024-09-18 14:19:28,933][fairseq_cli.train][INFO] - end of epoch 80 (average epoch stats below)
[2024-09-18 14:19:28,934][train][INFO] - epoch 080 | loss 1627.66 | ntokens 10335.5 | nsentences 47.2727 | nll_loss 7.445 | wps 6135.8 | ups 0.59 | wpb 10335.5 | bsz 47.3 | num_updates 3513 | lr 0.0001 | gnorm 228.716 | loss_scale 2 | train_wall 70 | gb_free 37.2 | wall 5942
[2024-09-18 14:19:28,935][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:19:28,950][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 81
[2024-09-18 14:19:28,962][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:19:28,976][fairseq.trainer][INFO] - begin training epoch 81
[2024-09-18 14:19:28,976][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:20:39,590][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 81 @ 3557 updates
[2024-09-18 14:20:39,591][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:20:43,054][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:20:43,127][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 81 @ 3557 updates, score None) (writing took 3.536864346009679 seconds)
[2024-09-18 14:20:43,128][fairseq_cli.train][INFO] - end of epoch 81 (average epoch stats below)
[2024-09-18 14:20:43,129][train][INFO] - epoch 081 | loss 1608.31 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 7.356 | wps 6129.2 | ups 0.59 | wpb 10335.2 | bsz 47.3 | num_updates 3557 | lr 0.0001 | gnorm 218.462 | loss_scale 2 | train_wall 70 | gb_free 35.9 | wall 6017
[2024-09-18 14:20:43,130][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:20:43,145][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 82
[2024-09-18 14:20:43,158][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:20:43,171][fairseq.trainer][INFO] - begin training epoch 82
[2024-09-18 14:20:43,171][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:21:52,219][train_inner][INFO] - epoch 082:     43 / 44 loss=1613.63, ntokens=10342, nsentences=47.48, nll_loss=7.408, wps=6163.4, ups=0.6, wpb=10342, bsz=47.5, num_updates=3600, lr=0.0001, gnorm=216.693, loss_scale=2, train_wall=321, gb_free=36.7, wall=6086
[2024-09-18 14:21:53,709][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 82 @ 3601 updates
[2024-09-18 14:21:53,710][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:21:57,153][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:21:57,224][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 82 @ 3601 updates, score None) (writing took 3.515376461989945 seconds)
[2024-09-18 14:21:57,225][fairseq_cli.train][INFO] - end of epoch 82 (average epoch stats below)
[2024-09-18 14:21:57,226][train][INFO] - epoch 082 | loss 1612.42 | ntokens 10335 | nsentences 47.2727 | nll_loss 7.375 | wps 6137.1 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 3601 | lr 0.0001 | gnorm 210.245 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 6091
[2024-09-18 14:21:57,227][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:21:57,242][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 83
[2024-09-18 14:21:57,257][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:21:57,274][fairseq.trainer][INFO] - begin training epoch 83
[2024-09-18 14:21:57,274][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:23:08,014][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 83 @ 3645 updates
[2024-09-18 14:23:08,014][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:23:11,470][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:23:11,517][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 83 @ 3645 updates, score None) (writing took 3.5037884149933234 seconds)
[2024-09-18 14:23:11,518][fairseq_cli.train][INFO] - end of epoch 83 (average epoch stats below)
[2024-09-18 14:23:11,519][train][INFO] - epoch 083 | loss 1572.96 | ntokens 10334.8 | nsentences 47.2727 | nll_loss 7.195 | wps 6120.8 | ups 0.59 | wpb 10334.8 | bsz 47.3 | num_updates 3645 | lr 0.0001 | gnorm 207.004 | loss_scale 2 | train_wall 71 | gb_free 37.2 | wall 6165
[2024-09-18 14:23:11,520][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:23:11,562][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 84
[2024-09-18 14:23:11,574][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:23:11,587][fairseq.trainer][INFO] - begin training epoch 84
[2024-09-18 14:23:11,587][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:24:21,988][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 84 @ 3689 updates
[2024-09-18 14:24:21,989][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:24:25,494][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:24:25,566][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 84 @ 3689 updates, score None) (writing took 3.5780704469943885 seconds)
[2024-09-18 14:24:25,567][fairseq_cli.train][INFO] - end of epoch 84 (average epoch stats below)
[2024-09-18 14:24:25,568][train][INFO] - epoch 084 | loss 1584.99 | ntokens 10334.8 | nsentences 47.2727 | nll_loss 7.25 | wps 6141 | ups 0.59 | wpb 10334.8 | bsz 47.3 | num_updates 3689 | lr 0.0001 | gnorm 207.016 | loss_scale 2 | train_wall 70 | gb_free 37.4 | wall 6239
[2024-09-18 14:24:25,569][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:24:25,584][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 85
[2024-09-18 14:24:25,597][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:24:25,615][fairseq.trainer][INFO] - begin training epoch 85
[2024-09-18 14:24:25,615][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:25:36,259][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 85 @ 3733 updates
[2024-09-18 14:25:36,259][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:25:39,725][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:25:39,794][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 85 @ 3733 updates, score None) (writing took 3.535021283008973 seconds)
[2024-09-18 14:25:39,794][fairseq_cli.train][INFO] - end of epoch 85 (average epoch stats below)
[2024-09-18 14:25:39,795][train][INFO] - epoch 085 | loss 1566.01 | ntokens 10335.3 | nsentences 47.2727 | nll_loss 7.163 | wps 6126.6 | ups 0.59 | wpb 10335.3 | bsz 47.3 | num_updates 3733 | lr 0.0001 | gnorm 198.611 | loss_scale 2 | train_wall 71 | gb_free 37.2 | wall 6313
[2024-09-18 14:25:39,796][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:25:39,812][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 86
[2024-09-18 14:25:39,824][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:25:39,837][fairseq.trainer][INFO] - begin training epoch 86
[2024-09-18 14:25:39,837][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:26:50,424][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 86 @ 3777 updates
[2024-09-18 14:26:50,425][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:26:53,845][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:26:53,914][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 86 @ 3777 updates, score None) (writing took 3.490462697998737 seconds)
[2024-09-18 14:26:53,915][fairseq_cli.train][INFO] - end of epoch 86 (average epoch stats below)
[2024-09-18 14:26:53,916][train][INFO] - epoch 086 | loss 1540.63 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 7.047 | wps 6135.5 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 3777 | lr 0.0001 | gnorm 195.079 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 6387
[2024-09-18 14:26:53,917][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:26:53,931][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 87
[2024-09-18 14:26:53,944][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:26:53,958][fairseq.trainer][INFO] - begin training epoch 87
[2024-09-18 14:26:53,958][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:27:31,050][train_inner][INFO] - epoch 087:     23 / 44 loss=1580.58, ntokens=10341.2, nsentences=47, nll_loss=7.184, wps=6104.1, ups=0.59, wpb=10341.2, bsz=47, num_updates=3800, lr=0.0001, gnorm=207.796, loss_scale=2, train_wall=320, gb_free=36.3, wall=6424
[2024-09-18 14:28:04,621][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 87 @ 3821 updates
[2024-09-18 14:28:04,622][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:28:08,053][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:28:08,123][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 87 @ 3821 updates, score None) (writing took 3.502340481005376 seconds)
[2024-09-18 14:28:08,124][fairseq_cli.train][INFO] - end of epoch 87 (average epoch stats below)
[2024-09-18 14:28:08,125][train][INFO] - epoch 087 | loss 1579.55 | ntokens 10335.5 | nsentences 47.2727 | nll_loss 7.225 | wps 6128.2 | ups 0.59 | wpb 10335.5 | bsz 47.3 | num_updates 3821 | lr 0.0001 | gnorm 218.581 | loss_scale 2 | train_wall 71 | gb_free 36.4 | wall 6462
[2024-09-18 14:28:08,126][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:28:08,141][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 88
[2024-09-18 14:28:08,153][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:28:08,167][fairseq.trainer][INFO] - begin training epoch 88
[2024-09-18 14:28:08,167][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:29:18,897][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 88 @ 3865 updates
[2024-09-18 14:29:18,898][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:29:22,292][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:29:22,362][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 88 @ 3865 updates, score None) (writing took 3.4645671540056355 seconds)
[2024-09-18 14:29:22,362][fairseq_cli.train][INFO] - end of epoch 88 (average epoch stats below)
[2024-09-18 14:29:22,363][train][INFO] - epoch 088 | loss 1529.88 | ntokens 10335.8 | nsentences 47.2727 | nll_loss 6.997 | wps 6125.9 | ups 0.59 | wpb 10335.8 | bsz 47.3 | num_updates 3865 | lr 0.0001 | gnorm 195.778 | loss_scale 2 | train_wall 71 | gb_free 37.2 | wall 6536
[2024-09-18 14:29:22,364][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:29:22,379][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 89
[2024-09-18 14:29:22,391][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:29:22,405][fairseq.trainer][INFO] - begin training epoch 89
[2024-09-18 14:29:22,405][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:30:33,392][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 89 @ 3909 updates
[2024-09-18 14:30:33,393][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:30:36,891][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:30:36,964][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 89 @ 3909 updates, score None) (writing took 3.5718249199999264 seconds)
[2024-09-18 14:30:36,964][fairseq_cli.train][INFO] - end of epoch 89 (average epoch stats below)
[2024-09-18 14:30:36,965][train][INFO] - epoch 089 | loss 1496.61 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 6.845 | wps 6095.9 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 3909 | lr 0.0001 | gnorm 186.127 | loss_scale 2 | train_wall 71 | gb_free 37.4 | wall 6610
[2024-09-18 14:30:36,966][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:30:36,982][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 90
[2024-09-18 14:30:36,995][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:30:37,010][fairseq.trainer][INFO] - begin training epoch 90
[2024-09-18 14:30:37,010][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:31:47,483][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 90 @ 3953 updates
[2024-09-18 14:31:47,484][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:31:50,918][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:31:50,987][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 90 @ 3953 updates, score None) (writing took 3.5038342110055964 seconds)
[2024-09-18 14:31:50,988][fairseq_cli.train][INFO] - end of epoch 90 (average epoch stats below)
[2024-09-18 14:31:50,989][train][INFO] - epoch 090 | loss 1508.53 | ntokens 10335.8 | nsentences 47.2727 | nll_loss 6.9 | wps 6143.7 | ups 0.59 | wpb 10335.8 | bsz 47.3 | num_updates 3953 | lr 0.0001 | gnorm 194.348 | loss_scale 2 | train_wall 70 | gb_free 37.4 | wall 6684
[2024-09-18 14:31:50,989][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:31:51,005][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 91
[2024-09-18 14:31:51,017][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:31:51,031][fairseq.trainer][INFO] - begin training epoch 91
[2024-09-18 14:31:51,031][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:33:01,716][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 91 @ 3997 updates
[2024-09-18 14:33:01,717][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:33:05,113][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:33:05,184][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 91 @ 3997 updates, score None) (writing took 3.467617791000521 seconds)
[2024-09-18 14:33:05,184][fairseq_cli.train][INFO] - end of epoch 91 (average epoch stats below)
[2024-09-18 14:33:05,185][train][INFO] - epoch 091 | loss 1461.32 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 6.684 | wps 6129.1 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 3997 | lr 0.0001 | gnorm 178.356 | loss_scale 2 | train_wall 71 | gb_free 37.1 | wall 6759
[2024-09-18 14:33:05,186][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:33:05,202][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 92
[2024-09-18 14:33:05,214][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:33:05,228][fairseq.trainer][INFO] - begin training epoch 92
[2024-09-18 14:33:05,229][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:33:10,098][train_inner][INFO] - epoch 092:      3 / 44 loss=1487.62, ntokens=10328.3, nsentences=47.8, nll_loss=6.885, wps=6092.6, ups=0.59, wpb=10328.3, bsz=47.8, num_updates=4000, lr=0.0001, gnorm=188.1, loss_scale=2, train_wall=321, gb_free=37.1, wall=6763
[2024-09-18 14:34:16,060][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 92 @ 4041 updates
[2024-09-18 14:34:16,061][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:34:19,555][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:34:19,626][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 92 @ 4041 updates, score None) (writing took 3.566078085990739 seconds)
[2024-09-18 14:34:19,626][fairseq_cli.train][INFO] - end of epoch 92 (average epoch stats below)
[2024-09-18 14:34:19,630][train][INFO] - epoch 092 | loss 1482.51 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 6.781 | wps 6108.6 | ups 0.59 | wpb 10335.2 | bsz 47.3 | num_updates 4041 | lr 0.0001 | gnorm 178.018 | loss_scale 2 | train_wall 71 | gb_free 37.4 | wall 6833
[2024-09-18 14:34:19,630][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:34:19,646][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 93
[2024-09-18 14:34:19,658][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:34:19,672][fairseq.trainer][INFO] - begin training epoch 93
[2024-09-18 14:34:19,673][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:35:30,308][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 93 @ 4085 updates
[2024-09-18 14:35:30,309][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:35:33,802][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:35:33,871][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 93 @ 4085 updates, score None) (writing took 3.562813279000693 seconds)
[2024-09-18 14:35:33,872][fairseq_cli.train][INFO] - end of epoch 93 (average epoch stats below)
[2024-09-18 14:35:33,873][train][INFO] - epoch 093 | loss 1473.04 | ntokens 10335.3 | nsentences 47.2727 | nll_loss 6.738 | wps 6125.3 | ups 0.59 | wpb 10335.3 | bsz 47.3 | num_updates 4085 | lr 0.0001 | gnorm 187.825 | loss_scale 2 | train_wall 71 | gb_free 37.4 | wall 6907
[2024-09-18 14:35:33,874][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:35:33,893][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 94
[2024-09-18 14:35:33,906][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:35:33,923][fairseq.trainer][INFO] - begin training epoch 94
[2024-09-18 14:35:33,923][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:36:44,211][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 94 @ 4129 updates
[2024-09-18 14:36:44,211][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:36:47,632][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:36:47,701][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 94 @ 4129 updates, score None) (writing took 3.4905702679971 seconds)
[2024-09-18 14:36:47,702][fairseq_cli.train][INFO] - end of epoch 94 (average epoch stats below)
[2024-09-18 14:36:47,703][train][INFO] - epoch 094 | loss 1494.15 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 6.834 | wps 6159.6 | ups 0.6 | wpb 10335.4 | bsz 47.3 | num_updates 4129 | lr 0.0001 | gnorm 191.906 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 6981
[2024-09-18 14:36:47,703][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:36:47,721][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 95
[2024-09-18 14:36:47,737][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:36:47,751][fairseq.trainer][INFO] - begin training epoch 95
[2024-09-18 14:36:47,751][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:37:58,175][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 95 @ 4173 updates
[2024-09-18 14:37:58,176][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:38:01,635][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:38:01,706][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 95 @ 4173 updates, score None) (writing took 3.530206106006517 seconds)
[2024-09-18 14:38:01,706][fairseq_cli.train][INFO] - end of epoch 95 (average epoch stats below)
[2024-09-18 14:38:01,707][train][INFO] - epoch 095 | loss 1479 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 6.765 | wps 6144.9 | ups 0.59 | wpb 10335.2 | bsz 47.3 | num_updates 4173 | lr 0.0001 | gnorm 185.133 | loss_scale 2 | train_wall 70 | gb_free 37.4 | wall 7055
[2024-09-18 14:38:01,708][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:38:01,723][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 96
[2024-09-18 14:38:01,735][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:38:01,749][fairseq.trainer][INFO] - begin training epoch 96
[2024-09-18 14:38:01,749][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:38:45,095][train_inner][INFO] - epoch 096:     27 / 44 loss=1482.52, ntokens=10322.5, nsentences=47.08, nll_loss=6.762, wps=6162.9, ups=0.6, wpb=10322.5, bsz=47.1, num_updates=4200, lr=0.0001, gnorm=183.469, loss_scale=2, train_wall=320, gb_free=35.9, wall=7098
[2024-09-18 14:39:12,202][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 96 @ 4217 updates
[2024-09-18 14:39:12,203][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:39:15,678][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:39:15,750][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 96 @ 4217 updates, score None) (writing took 3.547702679003123 seconds)
[2024-09-18 14:39:15,750][fairseq_cli.train][INFO] - end of epoch 96 (average epoch stats below)
[2024-09-18 14:39:15,751][train][INFO] - epoch 096 | loss 1435.92 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 6.568 | wps 6141.7 | ups 0.59 | wpb 10335.2 | bsz 47.3 | num_updates 4217 | lr 0.0001 | gnorm 163.709 | loss_scale 2 | train_wall 70 | gb_free 37.4 | wall 7129
[2024-09-18 14:39:15,752][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:39:15,767][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 97
[2024-09-18 14:39:15,779][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:39:15,794][fairseq.trainer][INFO] - begin training epoch 97
[2024-09-18 14:39:15,794][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:40:26,586][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 97 @ 4261 updates
[2024-09-18 14:40:26,586][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:40:30,034][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:40:30,104][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 97 @ 4261 updates, score None) (writing took 3.518892558000516 seconds)
[2024-09-18 14:40:30,105][fairseq_cli.train][INFO] - end of epoch 97 (average epoch stats below)
[2024-09-18 14:40:30,106][train][INFO] - epoch 097 | loss 1458.66 | ntokens 10335 | nsentences 47.2727 | nll_loss 6.672 | wps 6115.9 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 4261 | lr 0.0001 | gnorm 172.458 | loss_scale 2 | train_wall 71 | gb_free 35.9 | wall 7203
[2024-09-18 14:40:30,107][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:40:30,123][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 98
[2024-09-18 14:40:30,137][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:40:30,150][fairseq.trainer][INFO] - begin training epoch 98
[2024-09-18 14:40:30,151][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:41:40,817][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 98 @ 4305 updates
[2024-09-18 14:41:40,818][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:41:44,276][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:41:44,347][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 98 @ 4305 updates, score None) (writing took 3.530367657003808 seconds)
[2024-09-18 14:41:44,348][fairseq_cli.train][INFO] - end of epoch 98 (average epoch stats below)
[2024-09-18 14:41:44,349][train][INFO] - epoch 098 | loss 1428.61 | ntokens 10335.3 | nsentences 47.2727 | nll_loss 6.534 | wps 6125.3 | ups 0.59 | wpb 10335.3 | bsz 47.3 | num_updates 4305 | lr 0.0001 | gnorm 174.842 | loss_scale 2 | train_wall 71 | gb_free 37.2 | wall 7278
[2024-09-18 14:41:44,350][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:41:44,365][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 99
[2024-09-18 14:41:44,377][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:41:44,393][fairseq.trainer][INFO] - begin training epoch 99
[2024-09-18 14:41:44,393][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:42:54,782][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 99 @ 4349 updates
[2024-09-18 14:42:54,782][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:42:58,223][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:42:58,296][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 99 @ 4349 updates, score None) (writing took 3.514643073998741 seconds)
[2024-09-18 14:42:58,297][fairseq_cli.train][INFO] - end of epoch 99 (average epoch stats below)
[2024-09-18 14:42:58,298][train][INFO] - epoch 099 | loss 1432.52 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 6.552 | wps 6149.6 | ups 0.6 | wpb 10335.2 | bsz 47.3 | num_updates 4349 | lr 0.0001 | gnorm 165.857 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 7352
[2024-09-18 14:42:58,298][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:42:58,314][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 100
[2024-09-18 14:42:58,326][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:42:58,344][fairseq.trainer][INFO] - begin training epoch 100
[2024-09-18 14:42:58,344][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:44:08,890][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 100 @ 4393 updates
[2024-09-18 14:44:08,890][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:44:12,385][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:44:12,453][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 100 @ 4393 updates, score None) (writing took 3.562920779993874 seconds)
[2024-09-18 14:44:12,453][fairseq_cli.train][INFO] - end of epoch 100 (average epoch stats below)
[2024-09-18 14:44:12,454][train][INFO] - epoch 100 | loss 1427.71 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 6.53 | wps 6132.3 | ups 0.59 | wpb 10335.2 | bsz 47.3 | num_updates 4393 | lr 0.0001 | gnorm 168.644 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 7426
[2024-09-18 14:44:12,455][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:44:12,473][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 101
[2024-09-18 14:44:12,486][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:44:12,501][fairseq.trainer][INFO] - begin training epoch 101
[2024-09-18 14:44:12,501][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:44:24,035][train_inner][INFO] - epoch 101:      7 / 44 loss=1446.04, ntokens=10330.6, nsentences=46.92, nll_loss=6.568, wps=6095.9, ups=0.59, wpb=10330.6, bsz=46.9, num_updates=4400, lr=0.0001, gnorm=170.478, loss_scale=2, train_wall=320, gb_free=36, wall=7437
[2024-09-18 14:45:23,114][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 101 @ 4437 updates
[2024-09-18 14:45:23,114][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:45:26,553][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:45:26,627][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 101 @ 4437 updates, score None) (writing took 3.513500130997272 seconds)
[2024-09-18 14:45:26,628][fairseq_cli.train][INFO] - end of epoch 101 (average epoch stats below)
[2024-09-18 14:45:26,629][train][INFO] - epoch 101 | loss 1431.61 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 6.548 | wps 6131 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 4437 | lr 0.0001 | gnorm 169.948 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 7500
[2024-09-18 14:45:26,629][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:45:26,645][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 102
[2024-09-18 14:45:26,657][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:45:26,674][fairseq.trainer][INFO] - begin training epoch 102
[2024-09-18 14:45:26,674][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:46:37,118][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 102 @ 4481 updates
[2024-09-18 14:46:37,119][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:46:40,529][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:46:40,600][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 102 @ 4481 updates, score None) (writing took 3.481903802996385 seconds)
[2024-09-18 14:46:40,600][fairseq_cli.train][INFO] - end of epoch 102 (average epoch stats below)
[2024-09-18 14:46:40,601][train][INFO] - epoch 102 | loss 1427.88 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 6.531 | wps 6147.6 | ups 0.59 | wpb 10335.2 | bsz 47.3 | num_updates 4481 | lr 0.0001 | gnorm 181.323 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 7574
[2024-09-18 14:46:40,602][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:46:40,618][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 103
[2024-09-18 14:46:40,630][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:46:40,647][fairseq.trainer][INFO] - begin training epoch 103
[2024-09-18 14:46:40,647][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:47:51,565][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 103 @ 4525 updates
[2024-09-18 14:47:51,566][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:47:55,021][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:47:55,093][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 103 @ 4525 updates, score None) (writing took 3.5274611609929707 seconds)
[2024-09-18 14:47:55,093][fairseq_cli.train][INFO] - end of epoch 103 (average epoch stats below)
[2024-09-18 14:47:55,094][train][INFO] - epoch 103 | loss 1389.38 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 6.355 | wps 6104.7 | ups 0.59 | wpb 10335.2 | bsz 47.3 | num_updates 4525 | lr 0.0001 | gnorm 145.998 | loss_scale 2 | train_wall 71 | gb_free 37.3 | wall 7648
[2024-09-18 14:47:55,095][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:47:55,111][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 104
[2024-09-18 14:47:55,123][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:47:55,138][fairseq.trainer][INFO] - begin training epoch 104
[2024-09-18 14:47:55,138][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:49:05,925][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 104 @ 4569 updates
[2024-09-18 14:49:05,926][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:49:09,360][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:49:09,432][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 104 @ 4569 updates, score None) (writing took 3.5070781579997856 seconds)
[2024-09-18 14:49:09,432][fairseq_cli.train][INFO] - end of epoch 104 (average epoch stats below)
[2024-09-18 14:49:09,434][train][INFO] - epoch 104 | loss 1405.65 | ntokens 10335.3 | nsentences 47.2727 | nll_loss 6.429 | wps 6117.3 | ups 0.59 | wpb 10335.3 | bsz 47.3 | num_updates 4569 | lr 0.0001 | gnorm 163.748 | loss_scale 2 | train_wall 71 | gb_free 37.1 | wall 7723
[2024-09-18 14:49:09,434][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:49:09,450][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 105
[2024-09-18 14:49:09,462][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:49:09,479][fairseq.trainer][INFO] - begin training epoch 105
[2024-09-18 14:49:09,479][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:49:59,801][train_inner][INFO] - epoch 105:     31 / 44 loss=1400.28, ntokens=10357.1, nsentences=47.56, nll_loss=6.43, wps=6169.3, ups=0.6, wpb=10357.1, bsz=47.6, num_updates=4600, lr=0.0001, gnorm=162.537, loss_scale=2, train_wall=321, gb_free=37.2, wall=7773
[2024-09-18 14:50:20,316][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 105 @ 4613 updates
[2024-09-18 14:50:20,317][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:50:23,795][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:50:23,867][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 105 @ 4613 updates, score None) (writing took 3.55021151699475 seconds)
[2024-09-18 14:50:23,867][fairseq_cli.train][INFO] - end of epoch 105 (average epoch stats below)
[2024-09-18 14:50:23,870][train][INFO] - epoch 105 | loss 1374.55 | ntokens 10335 | nsentences 47.2727 | nll_loss 6.287 | wps 6109.2 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 4613 | lr 0.0001 | gnorm 156.155 | loss_scale 2 | train_wall 71 | gb_free 37.2 | wall 7797
[2024-09-18 14:50:23,871][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:50:23,886][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 106
[2024-09-18 14:50:23,898][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:50:23,915][fairseq.trainer][INFO] - begin training epoch 106
[2024-09-18 14:50:23,915][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:51:34,558][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 106 @ 4657 updates
[2024-09-18 14:51:34,559][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:51:37,998][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:51:38,069][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 106 @ 4657 updates, score None) (writing took 3.510591497004498 seconds)
[2024-09-18 14:51:38,069][fairseq_cli.train][INFO] - end of epoch 106 (average epoch stats below)
[2024-09-18 14:51:38,070][train][INFO] - epoch 106 | loss 1367.62 | ntokens 10335 | nsentences 47.2727 | nll_loss 6.256 | wps 6128.7 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 4657 | lr 0.0001 | gnorm 140.103 | loss_scale 2 | train_wall 71 | gb_free 37.3 | wall 7871
[2024-09-18 14:51:38,071][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:51:38,086][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 107
[2024-09-18 14:51:38,099][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:51:38,115][fairseq.trainer][INFO] - begin training epoch 107
[2024-09-18 14:51:38,115][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:52:48,687][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 107 @ 4701 updates
[2024-09-18 14:52:48,688][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:52:52,134][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:52:52,205][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 107 @ 4701 updates, score None) (writing took 3.5184123149956577 seconds)
[2024-09-18 14:52:52,206][fairseq_cli.train][INFO] - end of epoch 107 (average epoch stats below)
[2024-09-18 14:52:52,207][train][INFO] - epoch 107 | loss 1377.89 | ntokens 10335.8 | nsentences 47.2727 | nll_loss 6.302 | wps 6134.4 | ups 0.59 | wpb 10335.8 | bsz 47.3 | num_updates 4701 | lr 0.0001 | gnorm 145.996 | loss_scale 2 | train_wall 70 | gb_free 36 | wall 7946
[2024-09-18 14:52:52,208][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:52:52,227][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 108
[2024-09-18 14:52:52,240][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:52:52,257][fairseq.trainer][INFO] - begin training epoch 108
[2024-09-18 14:52:52,257][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:54:02,838][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 108 @ 4745 updates
[2024-09-18 14:54:02,839][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:54:06,256][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:54:06,328][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 108 @ 4745 updates, score None) (writing took 3.490210064002895 seconds)
[2024-09-18 14:54:06,329][fairseq_cli.train][INFO] - end of epoch 108 (average epoch stats below)
[2024-09-18 14:54:06,330][train][INFO] - epoch 108 | loss 1394.75 | ntokens 10335.5 | nsentences 47.2727 | nll_loss 6.379 | wps 6135.4 | ups 0.59 | wpb 10335.5 | bsz 47.3 | num_updates 4745 | lr 0.0001 | gnorm 158.65 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 8020
[2024-09-18 14:54:06,331][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:54:06,346][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 109
[2024-09-18 14:54:06,358][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:54:06,375][fairseq.trainer][INFO] - begin training epoch 109
[2024-09-18 14:54:06,375][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:55:17,224][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 109 @ 4789 updates
[2024-09-18 14:55:17,225][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:55:20,682][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:55:20,754][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 109 @ 4789 updates, score None) (writing took 3.529816763009876 seconds)
[2024-09-18 14:55:20,755][fairseq_cli.train][INFO] - end of epoch 109 (average epoch stats below)
[2024-09-18 14:55:20,756][train][INFO] - epoch 109 | loss 1370.15 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 6.267 | wps 6110.3 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 4789 | lr 0.0001 | gnorm 141.23 | loss_scale 2 | train_wall 71 | gb_free 37.1 | wall 8094
[2024-09-18 14:55:20,756][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:55:20,771][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 110
[2024-09-18 14:55:20,785][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:55:20,805][fairseq.trainer][INFO] - begin training epoch 110
[2024-09-18 14:55:20,805][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:55:38,834][train_inner][INFO] - epoch 110:     11 / 44 loss=1381.64, ntokens=10320.1, nsentences=47, nll_loss=6.292, wps=6088, ups=0.59, wpb=10320.1, bsz=47, num_updates=4800, lr=0.0001, gnorm=147.527, loss_scale=2, train_wall=321, gb_free=37.1, wall=8112
[2024-09-18 14:56:31,489][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 110 @ 4833 updates
[2024-09-18 14:56:31,490][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:56:34,952][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:56:35,023][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 110 @ 4833 updates, score None) (writing took 3.533938146007131 seconds)
[2024-09-18 14:56:35,023][fairseq_cli.train][INFO] - end of epoch 110 (average epoch stats below)
[2024-09-18 14:56:35,024][train][INFO] - epoch 110 | loss 1350.72 | ntokens 10335.5 | nsentences 47.2727 | nll_loss 6.178 | wps 6123.3 | ups 0.59 | wpb 10335.5 | bsz 47.3 | num_updates 4833 | lr 0.0001 | gnorm 148.859 | loss_scale 2 | train_wall 71 | gb_free 36.3 | wall 8168
[2024-09-18 14:56:35,025][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:56:35,040][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 111
[2024-09-18 14:56:35,052][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:56:35,074][fairseq.trainer][INFO] - begin training epoch 111
[2024-09-18 14:56:35,074][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:57:45,837][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 111 @ 4877 updates
[2024-09-18 14:57:45,837][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:57:49,283][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:57:49,355][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 111 @ 4877 updates, score None) (writing took 3.518374473991571 seconds)
[2024-09-18 14:57:49,355][fairseq_cli.train][INFO] - end of epoch 111 (average epoch stats below)
[2024-09-18 14:57:49,356][train][INFO] - epoch 111 | loss 1394.09 | ntokens 10335 | nsentences 47.2727 | nll_loss 6.377 | wps 6117.8 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 4877 | lr 0.0001 | gnorm 167.472 | loss_scale 2 | train_wall 71 | gb_free 37.3 | wall 8243
[2024-09-18 14:57:49,357][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:57:49,373][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 112
[2024-09-18 14:57:49,385][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:57:49,400][fairseq.trainer][INFO] - begin training epoch 112
[2024-09-18 14:57:49,401][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 14:58:59,918][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 112 @ 4921 updates
[2024-09-18 14:58:59,919][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:59:03,342][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 14:59:03,407][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 112 @ 4921 updates, score None) (writing took 3.488634852998075 seconds)
[2024-09-18 14:59:03,407][fairseq_cli.train][INFO] - end of epoch 112 (average epoch stats below)
[2024-09-18 14:59:03,408][train][INFO] - epoch 112 | loss 1339.17 | ntokens 10334.8 | nsentences 47.2727 | nll_loss 6.126 | wps 6140.8 | ups 0.59 | wpb 10334.8 | bsz 47.3 | num_updates 4921 | lr 0.0001 | gnorm 130.816 | loss_scale 2 | train_wall 70 | gb_free 37.3 | wall 8317
[2024-09-18 14:59:03,409][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 14:59:03,428][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 113
[2024-09-18 14:59:03,440][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 14:59:03,456][fairseq.trainer][INFO] - begin training epoch 113
[2024-09-18 14:59:03,456][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:00:14,145][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 113 @ 4965 updates
[2024-09-18 15:00:14,146][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:00:17,608][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:00:17,674][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 113 @ 4965 updates, score None) (writing took 3.5291606620012317 seconds)
[2024-09-18 15:00:17,675][fairseq_cli.train][INFO] - end of epoch 113 (average epoch stats below)
[2024-09-18 15:00:17,676][train][INFO] - epoch 113 | loss 1337.78 | ntokens 10335.5 | nsentences 47.2727 | nll_loss 6.119 | wps 6123.4 | ups 0.59 | wpb 10335.5 | bsz 47.3 | num_updates 4965 | lr 0.0001 | gnorm 133.387 | loss_scale 2 | train_wall 71 | gb_free 37.1 | wall 8391
[2024-09-18 15:00:17,676][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:00:17,696][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 114
[2024-09-18 15:00:17,708][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:00:17,723][fairseq.trainer][INFO] - begin training epoch 114
[2024-09-18 15:00:17,723][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:01:14,555][train_inner][INFO] - epoch 114:     35 / 44 loss=1352.19, ntokens=10348.1, nsentences=47.36, nll_loss=6.189, wps=6164.7, ups=0.6, wpb=10348.1, bsz=47.4, num_updates=5000, lr=0.0001, gnorm=144.402, loss_scale=2, train_wall=321, gb_free=37.2, wall=8448
[2024-09-18 15:01:28,396][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 114 @ 5009 updates
[2024-09-18 15:01:28,396][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:01:31,836][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:01:31,906][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 114 @ 5009 updates, score None) (writing took 3.50999787200999 seconds)
[2024-09-18 15:01:31,906][fairseq_cli.train][INFO] - end of epoch 114 (average epoch stats below)
[2024-09-18 15:01:31,907][train][INFO] - epoch 114 | loss 1338.95 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 6.124 | wps 6126.2 | ups 0.59 | wpb 10335.2 | bsz 47.3 | num_updates 5009 | lr 0.0001 | gnorm 139.391 | loss_scale 2 | train_wall 71 | gb_free 37.1 | wall 8465
[2024-09-18 15:01:31,908][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:01:31,924][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 115
[2024-09-18 15:01:31,937][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:01:31,955][fairseq.trainer][INFO] - begin training epoch 115
[2024-09-18 15:01:31,955][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:02:42,553][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 115 @ 5053 updates
[2024-09-18 15:02:42,554][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:02:45,988][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:02:46,059][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 115 @ 5053 updates, score None) (writing took 3.505784767010482 seconds)
[2024-09-18 15:02:46,059][fairseq_cli.train][INFO] - end of epoch 115 (average epoch stats below)
[2024-09-18 15:02:46,060][train][INFO] - epoch 115 | loss 1345.92 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 6.156 | wps 6132.6 | ups 0.59 | wpb 10335.2 | bsz 47.3 | num_updates 5053 | lr 0.0001 | gnorm 156.579 | loss_scale 2 | train_wall 70 | gb_free 37.2 | wall 8539
[2024-09-18 15:02:46,061][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:02:46,076][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 116
[2024-09-18 15:02:46,093][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:02:46,110][fairseq.trainer][INFO] - begin training epoch 116
[2024-09-18 15:02:46,111][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:03:56,732][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 116 @ 5097 updates
[2024-09-18 15:03:56,732][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:04:00,175][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:04:00,247][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 116 @ 5097 updates, score None) (writing took 3.515030669004773 seconds)
[2024-09-18 15:04:00,247][fairseq_cli.train][INFO] - end of epoch 116 (average epoch stats below)
[2024-09-18 15:04:00,248][train][INFO] - epoch 116 | loss 1337.99 | ntokens 10335 | nsentences 47.2727 | nll_loss 6.12 | wps 6129.6 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 5097 | lr 0.0001 | gnorm 137.138 | loss_scale 2 | train_wall 71 | gb_free 37.4 | wall 8614
[2024-09-18 15:04:00,249][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:04:00,264][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 117
[2024-09-18 15:04:00,276][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:04:00,294][fairseq.trainer][INFO] - begin training epoch 117
[2024-09-18 15:04:00,294][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:05:10,926][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 117 @ 5141 updates
[2024-09-18 15:05:10,926][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:05:14,409][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:05:14,460][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 117 @ 5141 updates, score None) (writing took 3.534017275000224 seconds)
[2024-09-18 15:05:14,460][fairseq_cli.train][INFO] - end of epoch 117 (average epoch stats below)
[2024-09-18 15:05:14,461][train][INFO] - epoch 117 | loss 1322.5 | ntokens 10334.8 | nsentences 47.2727 | nll_loss 6.049 | wps 6127.4 | ups 0.59 | wpb 10334.8 | bsz 47.3 | num_updates 5141 | lr 0.0001 | gnorm 145.405 | loss_scale 2 | train_wall 71 | gb_free 37.1 | wall 8688
[2024-09-18 15:05:14,462][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:05:14,498][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 118
[2024-09-18 15:05:14,510][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:05:14,529][fairseq.trainer][INFO] - begin training epoch 118
[2024-09-18 15:05:14,529][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:06:25,161][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 118 @ 5185 updates
[2024-09-18 15:06:25,162][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:06:28,544][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:06:28,618][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 118 @ 5185 updates, score None) (writing took 3.4571106160001364 seconds)
[2024-09-18 15:06:28,619][fairseq_cli.train][INFO] - end of epoch 118 (average epoch stats below)
[2024-09-18 15:06:28,620][train][INFO] - epoch 118 | loss 1327.62 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 6.072 | wps 6132.3 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 5185 | lr 0.0001 | gnorm 140.761 | loss_scale 2 | train_wall 71 | gb_free 36 | wall 8762
[2024-09-18 15:06:28,621][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:06:28,636][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 119
[2024-09-18 15:06:28,649][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:06:28,665][fairseq.trainer][INFO] - begin training epoch 119
[2024-09-18 15:06:28,665][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:06:52,557][train_inner][INFO] - epoch 119:     15 / 44 loss=1330.93, ntokens=10308.6, nsentences=47.16, nll_loss=6.089, wps=6099.8, ups=0.59, wpb=10308.6, bsz=47.2, num_updates=5200, lr=0.0001, gnorm=142.809, loss_scale=2, train_wall=320, gb_free=37.3, wall=8786
[2024-09-18 15:07:39,026][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 119 @ 5229 updates
[2024-09-18 15:07:39,027][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:07:42,504][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:07:42,576][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 119 @ 5229 updates, score None) (writing took 3.5495855800108984 seconds)
[2024-09-18 15:07:42,576][fairseq_cli.train][INFO] - end of epoch 119 (average epoch stats below)
[2024-09-18 15:07:42,577][train][INFO] - epoch 119 | loss 1305.98 | ntokens 10335 | nsentences 47.2727 | nll_loss 5.974 | wps 6148.7 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 5229 | lr 0.0001 | gnorm 128.42 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 8836
[2024-09-18 15:07:42,578][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:07:42,594][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 120
[2024-09-18 15:07:42,614][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:07:42,630][fairseq.trainer][INFO] - begin training epoch 120
[2024-09-18 15:07:42,630][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:08:53,296][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 120 @ 5273 updates
[2024-09-18 15:08:53,297][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:08:56,730][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:08:56,800][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 120 @ 5273 updates, score None) (writing took 3.5038189539918676 seconds)
[2024-09-18 15:08:56,800][fairseq_cli.train][INFO] - end of epoch 120 (average epoch stats below)
[2024-09-18 15:08:56,801][train][INFO] - epoch 120 | loss 1292.83 | ntokens 10335.8 | nsentences 47.2727 | nll_loss 5.913 | wps 6127.1 | ups 0.59 | wpb 10335.8 | bsz 47.3 | num_updates 5273 | lr 0.0001 | gnorm 128.572 | loss_scale 2 | train_wall 71 | gb_free 36.8 | wall 8910
[2024-09-18 15:08:56,802][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:08:56,817][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 121
[2024-09-18 15:08:56,834][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:08:56,850][fairseq.trainer][INFO] - begin training epoch 121
[2024-09-18 15:08:56,851][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:10:07,393][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 121 @ 5317 updates
[2024-09-18 15:10:07,394][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:10:10,865][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:10:10,935][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 121 @ 5317 updates, score None) (writing took 3.541665472002933 seconds)
[2024-09-18 15:10:10,935][fairseq_cli.train][INFO] - end of epoch 121 (average epoch stats below)
[2024-09-18 15:10:10,936][train][INFO] - epoch 121 | loss 1306.75 | ntokens 10334.8 | nsentences 47.2727 | nll_loss 5.977 | wps 6133.9 | ups 0.59 | wpb 10334.8 | bsz 47.3 | num_updates 5317 | lr 0.0001 | gnorm 141.009 | loss_scale 2 | train_wall 70 | gb_free 37.3 | wall 8984
[2024-09-18 15:10:10,937][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:10:10,957][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 122
[2024-09-18 15:10:10,975][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:10:10,997][fairseq.trainer][INFO] - begin training epoch 122
[2024-09-18 15:10:10,997][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:11:21,509][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 122 @ 5361 updates
[2024-09-18 15:11:21,510][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:11:24,954][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:11:25,026][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 122 @ 5361 updates, score None) (writing took 3.516581989999395 seconds)
[2024-09-18 15:11:25,026][fairseq_cli.train][INFO] - end of epoch 122 (average epoch stats below)
[2024-09-18 15:11:25,027][train][INFO] - epoch 122 | loss 1275.2 | ntokens 10335.5 | nsentences 47.2727 | nll_loss 5.833 | wps 6138 | ups 0.59 | wpb 10335.5 | bsz 47.3 | num_updates 5361 | lr 0.0001 | gnorm 114.571 | loss_scale 2 | train_wall 70 | gb_free 37.3 | wall 9058
[2024-09-18 15:11:25,028][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:11:25,044][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 123
[2024-09-18 15:11:25,059][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:11:25,076][fairseq.trainer][INFO] - begin training epoch 123
[2024-09-18 15:11:25,076][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:12:28,120][train_inner][INFO] - epoch 123:     39 / 44 loss=1294.59, ntokens=10363.7, nsentences=47.32, nll_loss=5.911, wps=6176.9, ups=0.6, wpb=10363.7, bsz=47.3, num_updates=5400, lr=0.0001, gnorm=126.797, loss_scale=2, train_wall=321, gb_free=37.2, wall=9122
[2024-09-18 15:12:35,656][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 123 @ 5405 updates
[2024-09-18 15:12:35,657][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:12:39,123][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:12:39,194][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 123 @ 5405 updates, score None) (writing took 3.537449581999681 seconds)
[2024-09-18 15:12:39,194][fairseq_cli.train][INFO] - end of epoch 123 (average epoch stats below)
[2024-09-18 15:12:39,195][train][INFO] - epoch 123 | loss 1281.51 | ntokens 10335.5 | nsentences 47.2727 | nll_loss 5.861 | wps 6131.6 | ups 0.59 | wpb 10335.5 | bsz 47.3 | num_updates 5405 | lr 0.0001 | gnorm 121.171 | loss_scale 2 | train_wall 70 | gb_free 36.9 | wall 9133
[2024-09-18 15:12:39,196][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:12:39,211][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 124
[2024-09-18 15:12:39,226][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:12:39,242][fairseq.trainer][INFO] - begin training epoch 124
[2024-09-18 15:12:39,242][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:13:50,070][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 124 @ 5449 updates
[2024-09-18 15:13:50,070][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:13:53,475][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:13:53,521][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 124 @ 5449 updates, score None) (writing took 3.451238244000706 seconds)
[2024-09-18 15:13:53,521][fairseq_cli.train][INFO] - end of epoch 124 (average epoch stats below)
[2024-09-18 15:13:53,522][train][INFO] - epoch 124 | loss 1304.31 | ntokens 10335.5 | nsentences 47.2727 | nll_loss 5.966 | wps 6118.5 | ups 0.59 | wpb 10335.5 | bsz 47.3 | num_updates 5449 | lr 0.0001 | gnorm 132.908 | loss_scale 2 | train_wall 71 | gb_free 37.3 | wall 9207
[2024-09-18 15:13:53,523][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:13:53,539][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 125
[2024-09-18 15:13:53,555][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:13:53,573][fairseq.trainer][INFO] - begin training epoch 125
[2024-09-18 15:13:53,573][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:15:03,807][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 125 @ 5493 updates
[2024-09-18 15:15:03,808][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:15:07,272][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:15:07,348][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 125 @ 5493 updates, score None) (writing took 3.5414170239964733 seconds)
[2024-09-18 15:15:07,349][fairseq_cli.train][INFO] - end of epoch 125 (average epoch stats below)
[2024-09-18 15:15:07,350][train][INFO] - epoch 125 | loss 1274.59 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 5.83 | wps 6159.7 | ups 0.6 | wpb 10335.2 | bsz 47.3 | num_updates 5493 | lr 0.0001 | gnorm 122.587 | loss_scale 2 | train_wall 70 | gb_free 35.9 | wall 9281
[2024-09-18 15:15:07,351][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:15:07,366][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 126
[2024-09-18 15:15:07,384][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:15:07,399][fairseq.trainer][INFO] - begin training epoch 126
[2024-09-18 15:15:07,399][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:16:18,132][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 126 @ 5537 updates
[2024-09-18 15:16:18,133][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:16:21,562][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:16:21,632][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 126 @ 5537 updates, score None) (writing took 3.4997927630029153 seconds)
[2024-09-18 15:16:21,632][fairseq_cli.train][INFO] - end of epoch 126 (average epoch stats below)
[2024-09-18 15:16:21,633][train][INFO] - epoch 126 | loss 1262.68 | ntokens 10335.5 | nsentences 47.2727 | nll_loss 5.775 | wps 6122.1 | ups 0.59 | wpb 10335.5 | bsz 47.3 | num_updates 5537 | lr 0.0001 | gnorm 128.73 | loss_scale 2 | train_wall 71 | gb_free 35.6 | wall 9355
[2024-09-18 15:16:21,634][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:16:21,650][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 127
[2024-09-18 15:16:21,666][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:16:21,689][fairseq.trainer][INFO] - begin training epoch 127
[2024-09-18 15:16:21,689][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:17:32,794][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 127 @ 5581 updates
[2024-09-18 15:17:32,795][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:17:36,204][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:17:36,273][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 127 @ 5581 updates, score None) (writing took 3.47864755000046 seconds)
[2024-09-18 15:17:36,273][fairseq_cli.train][INFO] - end of epoch 127 (average epoch stats below)
[2024-09-18 15:17:36,274][train][INFO] - epoch 127 | loss 1247.36 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 5.705 | wps 6092.6 | ups 0.59 | wpb 10335.2 | bsz 47.3 | num_updates 5581 | lr 0.0001 | gnorm 109.678 | loss_scale 2 | train_wall 71 | gb_free 37.4 | wall 9430
[2024-09-18 15:17:36,275][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:17:36,290][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 128
[2024-09-18 15:17:36,310][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:17:36,326][fairseq.trainer][INFO] - begin training epoch 128
[2024-09-18 15:17:36,326][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:18:06,870][train_inner][INFO] - epoch 128:     19 / 44 loss=1262.42, ntokens=10315.1, nsentences=47.44, nll_loss=5.806, wps=6090.1, ups=0.59, wpb=10315.1, bsz=47.4, num_updates=5600, lr=0.0001, gnorm=122.113, loss_scale=2, train_wall=320, gb_free=37.2, wall=9460
[2024-09-18 15:18:47,074][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 128 @ 5625 updates
[2024-09-18 15:18:47,075][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:18:50,547][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:18:50,615][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 128 @ 5625 updates, score None) (writing took 3.5409478560031857 seconds)
[2024-09-18 15:18:50,616][fairseq_cli.train][INFO] - end of epoch 128 (average epoch stats below)
[2024-09-18 15:18:50,617][train][INFO] - epoch 128 | loss 1253.6 | ntokens 10335.8 | nsentences 47.2727 | nll_loss 5.734 | wps 6117.3 | ups 0.59 | wpb 10335.8 | bsz 47.3 | num_updates 5625 | lr 0.0001 | gnorm 120.905 | loss_scale 2 | train_wall 71 | gb_free 35.8 | wall 9504
[2024-09-18 15:18:50,618][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:18:50,641][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 129
[2024-09-18 15:18:50,654][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:18:50,671][fairseq.trainer][INFO] - begin training epoch 129
[2024-09-18 15:18:50,672][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:20:01,370][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 129 @ 5669 updates
[2024-09-18 15:20:01,371][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:20:04,829][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:20:04,896][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 129 @ 5669 updates, score None) (writing took 3.525013303005835 seconds)
[2024-09-18 15:20:04,896][fairseq_cli.train][INFO] - end of epoch 129 (average epoch stats below)
[2024-09-18 15:20:04,897][train][INFO] - epoch 129 | loss 1260.13 | ntokens 10335 | nsentences 47.2727 | nll_loss 5.764 | wps 6122 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 5669 | lr 0.0001 | gnorm 125.502 | loss_scale 2 | train_wall 71 | gb_free 37.1 | wall 9578
[2024-09-18 15:20:04,898][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:20:04,917][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 130
[2024-09-18 15:20:04,929][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:20:04,953][fairseq.trainer][INFO] - begin training epoch 130
[2024-09-18 15:20:04,953][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:21:15,452][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 130 @ 5713 updates
[2024-09-18 15:21:15,453][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:21:18,904][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:21:18,974][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 130 @ 5713 updates, score None) (writing took 3.5225669280043803 seconds)
[2024-09-18 15:21:18,975][fairseq_cli.train][INFO] - end of epoch 130 (average epoch stats below)
[2024-09-18 15:21:18,976][train][INFO] - epoch 130 | loss 1264.31 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 5.783 | wps 6138.9 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 5713 | lr 0.0001 | gnorm 117.285 | loss_scale 2 | train_wall 70 | gb_free 37.3 | wall 9652
[2024-09-18 15:21:18,977][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:21:18,992][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 131
[2024-09-18 15:21:19,005][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:21:19,029][fairseq.trainer][INFO] - begin training epoch 131
[2024-09-18 15:21:19,029][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:22:29,669][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 131 @ 5757 updates
[2024-09-18 15:22:29,670][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:22:33,068][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:22:33,141][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 131 @ 5757 updates, score None) (writing took 3.4719791450042976 seconds)
[2024-09-18 15:22:33,142][fairseq_cli.train][INFO] - end of epoch 131 (average epoch stats below)
[2024-09-18 15:22:33,143][train][INFO] - epoch 131 | loss 1252.33 | ntokens 10334.8 | nsentences 47.2727 | nll_loss 5.728 | wps 6131.3 | ups 0.59 | wpb 10334.8 | bsz 47.3 | num_updates 5757 | lr 0.0001 | gnorm 121.755 | loss_scale 2 | train_wall 71 | gb_free 37.3 | wall 9727
[2024-09-18 15:22:33,144][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:22:33,158][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 132
[2024-09-18 15:22:33,171][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:22:33,188][fairseq.trainer][INFO] - begin training epoch 132
[2024-09-18 15:22:33,188][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:23:42,135][train_inner][INFO] - epoch 132:     43 / 44 loss=1253.68, ntokens=10351.3, nsentences=47.36, nll_loss=5.736, wps=6175, ups=0.6, wpb=10351.3, bsz=47.4, num_updates=5800, lr=0.0001, gnorm=120.579, loss_scale=2, train_wall=321, gb_free=37.2, wall=9796
[2024-09-18 15:23:43,491][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 132 @ 5801 updates
[2024-09-18 15:23:43,492][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:23:46,922][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:23:46,992][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 132 @ 5801 updates, score None) (writing took 3.5001546630082885 seconds)
[2024-09-18 15:23:46,992][fairseq_cli.train][INFO] - end of epoch 132 (average epoch stats below)
[2024-09-18 15:23:46,993][train][INFO] - epoch 132 | loss 1234.53 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 5.647 | wps 6157.9 | ups 0.6 | wpb 10335.4 | bsz 47.3 | num_updates 5801 | lr 0.0001 | gnorm 112.397 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 9800
[2024-09-18 15:23:46,994][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:23:47,009][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 133
[2024-09-18 15:23:47,025][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:23:47,044][fairseq.trainer][INFO] - begin training epoch 133
[2024-09-18 15:23:47,044][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:24:57,591][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 133 @ 5845 updates
[2024-09-18 15:24:57,592][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:25:01,036][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:25:01,106][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 133 @ 5845 updates, score None) (writing took 3.5154396959987935 seconds)
[2024-09-18 15:25:01,107][fairseq_cli.train][INFO] - end of epoch 133 (average epoch stats below)
[2024-09-18 15:25:01,108][train][INFO] - epoch 133 | loss 1249.15 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 5.713 | wps 6135.9 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 5845 | lr 0.0001 | gnorm 114.293 | loss_scale 2 | train_wall 70 | gb_free 37.2 | wall 9875
[2024-09-18 15:25:01,109][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:25:01,124][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 134
[2024-09-18 15:25:01,139][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:25:01,155][fairseq.trainer][INFO] - begin training epoch 134
[2024-09-18 15:25:01,155][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:26:11,554][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 134 @ 5889 updates
[2024-09-18 15:26:11,554][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:26:14,929][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:26:14,999][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 134 @ 5889 updates, score None) (writing took 3.4447714100097073 seconds)
[2024-09-18 15:26:14,999][fairseq_cli.train][INFO] - end of epoch 134 (average epoch stats below)
[2024-09-18 15:26:15,000][train][INFO] - epoch 134 | loss 1240.81 | ntokens 10335.3 | nsentences 47.2727 | nll_loss 5.675 | wps 6154.4 | ups 0.6 | wpb 10335.3 | bsz 47.3 | num_updates 5889 | lr 0.0001 | gnorm 109.254 | loss_scale 2 | train_wall 70 | gb_free 36 | wall 9948
[2024-09-18 15:26:15,001][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:26:15,016][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 135
[2024-09-18 15:26:15,028][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:26:15,046][fairseq.trainer][INFO] - begin training epoch 135
[2024-09-18 15:26:15,047][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:27:25,814][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 135 @ 5933 updates
[2024-09-18 15:27:25,815][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:27:29,264][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:27:29,330][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 135 @ 5933 updates, score None) (writing took 3.5156735530035803 seconds)
[2024-09-18 15:27:29,330][fairseq_cli.train][INFO] - end of epoch 135 (average epoch stats below)
[2024-09-18 15:27:29,331][train][INFO] - epoch 135 | loss 1229.72 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 5.625 | wps 6118 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 5933 | lr 0.0001 | gnorm 107.079 | loss_scale 2 | train_wall 71 | gb_free 37.2 | wall 10023
[2024-09-18 15:27:29,332][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:27:29,353][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 136
[2024-09-18 15:27:29,365][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:27:29,381][fairseq.trainer][INFO] - begin training epoch 136
[2024-09-18 15:27:29,381][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:28:39,871][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 136 @ 5977 updates
[2024-09-18 15:28:39,872][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:28:43,280][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:28:43,349][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 136 @ 5977 updates, score None) (writing took 3.4780048730026465 seconds)
[2024-09-18 15:28:43,349][fairseq_cli.train][INFO] - end of epoch 136 (average epoch stats below)
[2024-09-18 15:28:43,350][train][INFO] - epoch 136 | loss 1235.43 | ntokens 10335.8 | nsentences 47.2727 | nll_loss 5.65 | wps 6144.1 | ups 0.59 | wpb 10335.8 | bsz 47.3 | num_updates 5977 | lr 0.0001 | gnorm 109.811 | loss_scale 2 | train_wall 70 | gb_free 37.2 | wall 10097
[2024-09-18 15:28:43,351][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:28:43,367][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 137
[2024-09-18 15:28:43,379][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:28:43,395][fairseq.trainer][INFO] - begin training epoch 137
[2024-09-18 15:28:43,396][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:29:20,739][train_inner][INFO] - epoch 137:     23 / 44 loss=1242.17, ntokens=10337.7, nsentences=47.16, nll_loss=5.667, wps=6106.1, ups=0.59, wpb=10337.7, bsz=47.2, num_updates=6000, lr=0.0001, gnorm=110.643, loss_scale=2, train_wall=320, gb_free=37.2, wall=10134
[2024-09-18 15:29:54,292][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 137 @ 6021 updates
[2024-09-18 15:29:54,293][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:29:57,760][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:29:57,828][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 137 @ 6021 updates, score None) (writing took 3.5355867670004955 seconds)
[2024-09-18 15:29:57,829][fairseq_cli.train][INFO] - end of epoch 137 (average epoch stats below)
[2024-09-18 15:29:57,830][train][INFO] - epoch 137 | loss 1240.12 | ntokens 10335 | nsentences 47.2727 | nll_loss 5.672 | wps 6105.7 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 6021 | lr 0.0001 | gnorm 113.43 | loss_scale 2 | train_wall 71 | gb_free 37.2 | wall 10171
[2024-09-18 15:29:57,830][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:29:57,851][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 138
[2024-09-18 15:29:57,866][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:29:57,882][fairseq.trainer][INFO] - begin training epoch 138
[2024-09-18 15:29:57,883][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:31:08,321][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 138 @ 6065 updates
[2024-09-18 15:31:08,322][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:31:11,819][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:31:11,890][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 138 @ 6065 updates, score None) (writing took 3.5692892790102633 seconds)
[2024-09-18 15:31:11,891][fairseq_cli.train][INFO] - end of epoch 138 (average epoch stats below)
[2024-09-18 15:31:11,892][train][INFO] - epoch 138 | loss 1230.88 | ntokens 10335 | nsentences 47.2727 | nll_loss 5.63 | wps 6140 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 6065 | lr 0.0001 | gnorm 107.05 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 10245
[2024-09-18 15:31:11,893][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:31:11,908][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 139
[2024-09-18 15:31:11,924][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:31:11,941][fairseq.trainer][INFO] - begin training epoch 139
[2024-09-18 15:31:11,942][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:32:22,596][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 139 @ 6109 updates
[2024-09-18 15:32:22,596][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:32:26,060][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:32:26,107][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 139 @ 6109 updates, score None) (writing took 3.5118204049940687 seconds)
[2024-09-18 15:32:26,108][fairseq_cli.train][INFO] - end of epoch 139 (average epoch stats below)
[2024-09-18 15:32:26,109][train][INFO] - epoch 139 | loss 1221.08 | ntokens 10335.8 | nsentences 47.2727 | nll_loss 5.585 | wps 6127.7 | ups 0.59 | wpb 10335.8 | bsz 47.3 | num_updates 6109 | lr 0.0001 | gnorm 109.172 | loss_scale 2 | train_wall 71 | gb_free 37.4 | wall 10320
[2024-09-18 15:32:26,110][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:32:26,124][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 140
[2024-09-18 15:32:26,136][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:32:26,159][fairseq.trainer][INFO] - begin training epoch 140
[2024-09-18 15:32:26,159][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:33:36,904][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 140 @ 6153 updates
[2024-09-18 15:33:36,905][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:33:40,346][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:33:40,423][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 140 @ 6153 updates, score None) (writing took 3.518627508005011 seconds)
[2024-09-18 15:33:40,423][fairseq_cli.train][INFO] - end of epoch 140 (average epoch stats below)
[2024-09-18 15:33:40,424][train][INFO] - epoch 140 | loss 1215.14 | ntokens 10335 | nsentences 47.2727 | nll_loss 5.558 | wps 6119.1 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 6153 | lr 0.0001 | gnorm 104.846 | loss_scale 2 | train_wall 71 | gb_free 37.1 | wall 10394
[2024-09-18 15:33:40,425][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:33:40,441][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 141
[2024-09-18 15:33:40,453][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:33:40,470][fairseq.trainer][INFO] - begin training epoch 141
[2024-09-18 15:33:40,470][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:34:50,950][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 141 @ 6197 updates
[2024-09-18 15:34:50,951][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:34:54,393][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:34:54,460][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 141 @ 6197 updates, score None) (writing took 3.5098979190079262 seconds)
[2024-09-18 15:34:54,461][fairseq_cli.train][INFO] - end of epoch 141 (average epoch stats below)
[2024-09-18 15:34:54,462][train][INFO] - epoch 141 | loss 1213.69 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 5.551 | wps 6142.3 | ups 0.59 | wpb 10335.2 | bsz 47.3 | num_updates 6197 | lr 0.0001 | gnorm 103.079 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 10468
[2024-09-18 15:34:54,462][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:34:54,477][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 142
[2024-09-18 15:34:54,489][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:34:54,513][fairseq.trainer][INFO] - begin training epoch 142
[2024-09-18 15:34:54,513][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:34:59,228][train_inner][INFO] - epoch 142:      3 / 44 loss=1219.7, ntokens=10322.1, nsentences=47.24, nll_loss=5.582, wps=6098.9, ups=0.59, wpb=10322.1, bsz=47.2, num_updates=6200, lr=0.0001, gnorm=105.844, loss_scale=2, train_wall=320, gb_free=37.2, wall=10473
[2024-09-18 15:36:05,282][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 142 @ 6241 updates
[2024-09-18 15:36:05,283][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:36:08,692][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:36:08,762][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 142 @ 6241 updates, score None) (writing took 3.4793888790009078 seconds)
[2024-09-18 15:36:08,762][fairseq_cli.train][INFO] - end of epoch 142 (average epoch stats below)
[2024-09-18 15:36:08,763][train][INFO] - epoch 142 | loss 1221.47 | ntokens 10335.8 | nsentences 47.2727 | nll_loss 5.587 | wps 6120.7 | ups 0.59 | wpb 10335.8 | bsz 47.3 | num_updates 6241 | lr 0.0001 | gnorm 102.307 | loss_scale 2 | train_wall 71 | gb_free 37.1 | wall 10542
[2024-09-18 15:36:08,764][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:36:08,780][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 143
[2024-09-18 15:36:08,792][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:36:08,811][fairseq.trainer][INFO] - begin training epoch 143
[2024-09-18 15:36:08,811][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:37:19,358][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 143 @ 6285 updates
[2024-09-18 15:37:19,359][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:37:22,728][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:37:22,798][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 143 @ 6285 updates, score None) (writing took 3.439513079007156 seconds)
[2024-09-18 15:37:22,798][fairseq_cli.train][INFO] - end of epoch 143 (average epoch stats below)
[2024-09-18 15:37:22,799][train][INFO] - epoch 143 | loss 1213.03 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 5.548 | wps 6142.5 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 6285 | lr 0.0001 | gnorm 100.984 | loss_scale 2 | train_wall 70 | gb_free 37.4 | wall 10616
[2024-09-18 15:37:22,800][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:37:22,815][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 144
[2024-09-18 15:37:22,829][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:37:22,846][fairseq.trainer][INFO] - begin training epoch 144
[2024-09-18 15:37:22,846][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:38:33,434][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 144 @ 6329 updates
[2024-09-18 15:38:33,435][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:38:36,890][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:38:36,961][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 144 @ 6329 updates, score None) (writing took 3.526857648001169 seconds)
[2024-09-18 15:38:36,961][fairseq_cli.train][INFO] - end of epoch 144 (average epoch stats below)
[2024-09-18 15:38:36,962][train][INFO] - epoch 144 | loss 1192.8 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 5.456 | wps 6131.9 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 6329 | lr 0.0001 | gnorm 96.62 | loss_scale 2 | train_wall 70 | gb_free 35.8 | wall 10690
[2024-09-18 15:38:36,963][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:38:36,979][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 145
[2024-09-18 15:38:36,991][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:38:37,011][fairseq.trainer][INFO] - begin training epoch 145
[2024-09-18 15:38:37,011][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:39:47,400][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 145 @ 6373 updates
[2024-09-18 15:39:47,401][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:39:50,807][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:39:50,877][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 145 @ 6373 updates, score None) (writing took 3.477161504997639 seconds)
[2024-09-18 15:39:50,878][fairseq_cli.train][INFO] - end of epoch 145 (average epoch stats below)
[2024-09-18 15:39:50,879][train][INFO] - epoch 145 | loss 1194.62 | ntokens 10334.8 | nsentences 47.2727 | nll_loss 5.464 | wps 6152 | ups 0.6 | wpb 10334.8 | bsz 47.3 | num_updates 6373 | lr 0.0001 | gnorm 96.944 | loss_scale 2 | train_wall 70 | gb_free 37.2 | wall 10764
[2024-09-18 15:39:50,880][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:39:50,895][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 146
[2024-09-18 15:39:50,907][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:39:50,926][fairseq.trainer][INFO] - begin training epoch 146
[2024-09-18 15:39:50,926][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:40:34,597][train_inner][INFO] - epoch 146:     27 / 44 loss=1204.93, ntokens=10355.4, nsentences=47.4, nll_loss=5.515, wps=6175.5, ups=0.6, wpb=10355.4, bsz=47.4, num_updates=6400, lr=0.0001, gnorm=99.613, loss_scale=2, train_wall=321, gb_free=35.6, wall=10808
[2024-09-18 15:41:01,708][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 146 @ 6417 updates
[2024-09-18 15:41:01,708][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:41:05,269][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:41:05,397][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 146 @ 6417 updates, score None) (writing took 3.689174760991591 seconds)
[2024-09-18 15:41:05,397][fairseq_cli.train][INFO] - end of epoch 146 (average epoch stats below)
[2024-09-18 15:41:05,398][train][INFO] - epoch 146 | loss 1200.76 | ntokens 10335.5 | nsentences 47.2727 | nll_loss 5.492 | wps 6102.7 | ups 0.59 | wpb 10335.5 | bsz 47.3 | num_updates 6417 | lr 0.0001 | gnorm 100.719 | loss_scale 2 | train_wall 71 | gb_free 35.8 | wall 10839
[2024-09-18 15:41:05,399][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:41:05,414][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 147
[2024-09-18 15:41:05,427][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:41:05,444][fairseq.trainer][INFO] - begin training epoch 147
[2024-09-18 15:41:05,444][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:42:16,057][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 147 @ 6461 updates
[2024-09-18 15:42:16,058][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:42:19,694][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:42:19,819][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 147 @ 6461 updates, score None) (writing took 3.761949806998018 seconds)
[2024-09-18 15:42:19,820][fairseq_cli.train][INFO] - end of epoch 147 (average epoch stats below)
[2024-09-18 15:42:19,821][train][INFO] - epoch 147 | loss 1195.1 | ntokens 10335 | nsentences 47.2727 | nll_loss 5.466 | wps 6110.3 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 6461 | lr 0.0001 | gnorm 93.689 | loss_scale 2 | train_wall 70 | gb_free 37.1 | wall 10913
[2024-09-18 15:42:19,821][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:42:19,836][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 148
[2024-09-18 15:42:19,849][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:42:19,866][fairseq.trainer][INFO] - begin training epoch 148
[2024-09-18 15:42:19,867][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:43:30,533][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 148 @ 6505 updates
[2024-09-18 15:43:30,533][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:43:34,190][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:43:34,316][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 148 @ 6505 updates, score None) (writing took 3.7834031550009968 seconds)
[2024-09-18 15:43:34,317][fairseq_cli.train][INFO] - end of epoch 148 (average epoch stats below)
[2024-09-18 15:43:34,318][train][INFO] - epoch 148 | loss 1194.21 | ntokens 10335 | nsentences 47.2727 | nll_loss 5.462 | wps 6104.2 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 6505 | lr 0.0001 | gnorm 94.071 | loss_scale 2 | train_wall 71 | gb_free 37.1 | wall 10988
[2024-09-18 15:43:34,319][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:43:34,334][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 149
[2024-09-18 15:43:34,346][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:43:34,364][fairseq.trainer][INFO] - begin training epoch 149
[2024-09-18 15:43:34,364][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:44:45,003][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 149 @ 6549 updates
[2024-09-18 15:44:45,004][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:44:48,659][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:44:48,783][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 149 @ 6549 updates, score None) (writing took 3.7800245500111487 seconds)
[2024-09-18 15:44:48,783][fairseq_cli.train][INFO] - end of epoch 149 (average epoch stats below)
[2024-09-18 15:44:48,784][train][INFO] - epoch 149 | loss 1188 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 5.434 | wps 6106.9 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 6549 | lr 0.0001 | gnorm 95.476 | loss_scale 2 | train_wall 71 | gb_free 37.1 | wall 11062
[2024-09-18 15:44:48,785][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:44:48,801][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 150
[2024-09-18 15:44:48,813][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:44:48,831][fairseq.trainer][INFO] - begin training epoch 150
[2024-09-18 15:44:48,831][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:45:59,172][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 150 @ 6593 updates
[2024-09-18 15:45:59,173][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:46:02,811][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:46:02,936][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 150 @ 6593 updates, score None) (writing took 3.7635593800077913 seconds)
[2024-09-18 15:46:02,936][fairseq_cli.train][INFO] - end of epoch 150 (average epoch stats below)
[2024-09-18 15:46:02,938][train][INFO] - epoch 150 | loss 1182.65 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 5.409 | wps 6132.6 | ups 0.59 | wpb 10335.2 | bsz 47.3 | num_updates 6593 | lr 0.0001 | gnorm 93.574 | loss_scale 4 | train_wall 70 | gb_free 37.2 | wall 11136
[2024-09-18 15:46:02,938][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:46:02,954][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 151
[2024-09-18 15:46:02,967][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:46:02,984][fairseq.trainer][INFO] - begin training epoch 151
[2024-09-18 15:46:02,984][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:46:14,198][train_inner][INFO] - epoch 151:      7 / 44 loss=1189.32, ntokens=10318.5, nsentences=47.28, nll_loss=5.45, wps=6076.8, ups=0.59, wpb=10318.5, bsz=47.3, num_updates=6600, lr=0.0001, gnorm=95.689, loss_scale=4, train_wall=320, gb_free=37.2, wall=11148
[2024-09-18 15:47:13,313][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 151 @ 6637 updates
[2024-09-18 15:47:13,313][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:47:17,010][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:47:17,133][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 151 @ 6637 updates, score None) (writing took 3.8200157370010857 seconds)
[2024-09-18 15:47:17,133][fairseq_cli.train][INFO] - end of epoch 151 (average epoch stats below)
[2024-09-18 15:47:17,134][train][INFO] - epoch 151 | loss 1185.35 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 5.422 | wps 6129.2 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 6637 | lr 0.0001 | gnorm 97.057 | loss_scale 4 | train_wall 70 | gb_free 37.3 | wall 11211
[2024-09-18 15:47:17,135][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:47:17,152][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 152
[2024-09-18 15:47:17,166][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:47:17,184][fairseq.trainer][INFO] - begin training epoch 152
[2024-09-18 15:47:17,184][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:48:28,066][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 152 @ 6681 updates
[2024-09-18 15:48:28,066][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:48:31,741][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:48:31,856][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 152 @ 6681 updates, score None) (writing took 3.790168966006604 seconds)
[2024-09-18 15:48:31,856][fairseq_cli.train][INFO] - end of epoch 152 (average epoch stats below)
[2024-09-18 15:48:31,857][train][INFO] - epoch 152 | loss 1166.3 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 5.335 | wps 6086 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 6681 | lr 0.0001 | gnorm 89.538 | loss_scale 4 | train_wall 71 | gb_free 37.1 | wall 11285
[2024-09-18 15:48:31,858][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:48:31,882][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 153
[2024-09-18 15:48:31,903][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:48:31,936][fairseq.trainer][INFO] - begin training epoch 153
[2024-09-18 15:48:31,936][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:49:42,673][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 153 @ 6725 updates
[2024-09-18 15:49:42,674][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:49:46,334][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:49:46,458][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 153 @ 6725 updates, score None) (writing took 3.7852014880045317 seconds)
[2024-09-18 15:49:46,459][fairseq_cli.train][INFO] - end of epoch 153 (average epoch stats below)
[2024-09-18 15:49:46,460][train][INFO] - epoch 153 | loss 1184.18 | ntokens 10335.3 | nsentences 47.2727 | nll_loss 5.416 | wps 6095.8 | ups 0.59 | wpb 10335.3 | bsz 47.3 | num_updates 6725 | lr 0.0001 | gnorm 95.662 | loss_scale 4 | train_wall 71 | gb_free 37.1 | wall 11360
[2024-09-18 15:49:46,460][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:49:46,476][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 154
[2024-09-18 15:49:46,488][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:49:46,505][fairseq.trainer][INFO] - begin training epoch 154
[2024-09-18 15:49:46,506][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:50:56,789][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 154 @ 6769 updates
[2024-09-18 15:50:56,790][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:51:00,402][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:51:00,526][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 154 @ 6769 updates, score None) (writing took 3.737499052003841 seconds)
[2024-09-18 15:51:00,527][fairseq_cli.train][INFO] - end of epoch 154 (average epoch stats below)
[2024-09-18 15:51:00,528][train][INFO] - epoch 154 | loss 1187.23 | ntokens 10335.8 | nsentences 47.2727 | nll_loss 5.43 | wps 6140 | ups 0.59 | wpb 10335.8 | bsz 47.3 | num_updates 6769 | lr 0.0001 | gnorm 100.87 | loss_scale 4 | train_wall 70 | gb_free 37.4 | wall 11434
[2024-09-18 15:51:00,529][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:51:00,544][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 155
[2024-09-18 15:51:00,556][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:51:00,574][fairseq.trainer][INFO] - begin training epoch 155
[2024-09-18 15:51:00,574][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:51:50,082][train_inner][INFO] - epoch 155:     31 / 44 loss=1176.92, ntokens=10344, nsentences=47.24, nll_loss=5.375, wps=6159.3, ups=0.6, wpb=10344, bsz=47.2, num_updates=6800, lr=0.0001, gnorm=93.008, loss_scale=4, train_wall=320, gb_free=37.1, wall=11483
[2024-09-18 15:52:10,947][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 155 @ 6813 updates
[2024-09-18 15:52:10,948][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:52:14,612][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:52:14,737][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 155 @ 6813 updates, score None) (writing took 3.7897665060008876 seconds)
[2024-09-18 15:52:14,737][fairseq_cli.train][INFO] - end of epoch 155 (average epoch stats below)
[2024-09-18 15:52:14,738][train][INFO] - epoch 155 | loss 1150.18 | ntokens 10335.3 | nsentences 47.2727 | nll_loss 5.261 | wps 6128 | ups 0.59 | wpb 10335.3 | bsz 47.3 | num_updates 6813 | lr 0.0001 | gnorm 80.122 | loss_scale 4 | train_wall 70 | gb_free 36.2 | wall 11508
[2024-09-18 15:52:14,739][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:52:14,754][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 156
[2024-09-18 15:52:14,767][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:52:14,785][fairseq.trainer][INFO] - begin training epoch 156
[2024-09-18 15:52:14,785][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:53:25,175][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 156 @ 6857 updates
[2024-09-18 15:53:25,176][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:53:28,859][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:53:28,985][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 156 @ 6857 updates, score None) (writing took 3.8094772059994284 seconds)
[2024-09-18 15:53:28,985][fairseq_cli.train][INFO] - end of epoch 156 (average epoch stats below)
[2024-09-18 15:53:28,986][train][INFO] - epoch 156 | loss 1165.38 | ntokens 10334.8 | nsentences 47.2727 | nll_loss 5.331 | wps 6124.6 | ups 0.59 | wpb 10334.8 | bsz 47.3 | num_updates 6857 | lr 0.0001 | gnorm 91.067 | loss_scale 4 | train_wall 70 | gb_free 37.2 | wall 11582
[2024-09-18 15:53:28,987][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:53:29,002][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 157
[2024-09-18 15:53:29,014][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:53:29,032][fairseq.trainer][INFO] - begin training epoch 157
[2024-09-18 15:53:29,033][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:54:39,361][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 157 @ 6901 updates
[2024-09-18 15:54:39,361][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:54:43,032][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:54:43,156][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 157 @ 6901 updates, score None) (writing took 3.7951637329970254 seconds)
[2024-09-18 15:54:43,156][fairseq_cli.train][INFO] - end of epoch 157 (average epoch stats below)
[2024-09-18 15:54:43,157][train][INFO] - epoch 157 | loss 1149.94 | ntokens 10334.8 | nsentences 47.2727 | nll_loss 5.26 | wps 6130.9 | ups 0.59 | wpb 10334.8 | bsz 47.3 | num_updates 6901 | lr 0.0001 | gnorm 87.681 | loss_scale 4 | train_wall 70 | gb_free 36.9 | wall 11657
[2024-09-18 15:54:43,158][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:54:43,173][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 158
[2024-09-18 15:54:43,185][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:54:43,203][fairseq.trainer][INFO] - begin training epoch 158
[2024-09-18 15:54:43,203][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:55:53,855][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 158 @ 6945 updates
[2024-09-18 15:55:53,856][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:55:57,532][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:55:57,657][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 158 @ 6945 updates, score None) (writing took 3.8019347350054886 seconds)
[2024-09-18 15:55:57,658][fairseq_cli.train][INFO] - end of epoch 158 (average epoch stats below)
[2024-09-18 15:55:57,659][train][INFO] - epoch 158 | loss 1157.38 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 5.294 | wps 6104 | ups 0.59 | wpb 10335.2 | bsz 47.3 | num_updates 6945 | lr 0.0001 | gnorm 94.435 | loss_scale 4 | train_wall 71 | gb_free 37.1 | wall 11731
[2024-09-18 15:55:57,660][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:55:57,676][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 159
[2024-09-18 15:55:57,688][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:55:57,706][fairseq.trainer][INFO] - begin training epoch 159
[2024-09-18 15:55:57,706][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:57:08,681][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 159 @ 6989 updates
[2024-09-18 15:57:08,681][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:57:12,282][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:57:12,405][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 159 @ 6989 updates, score None) (writing took 3.7241502049873816 seconds)
[2024-09-18 15:57:12,405][fairseq_cli.train][INFO] - end of epoch 159 (average epoch stats below)
[2024-09-18 15:57:12,406][train][INFO] - epoch 159 | loss 1161.97 | ntokens 10335 | nsentences 47.2727 | nll_loss 5.315 | wps 6083.8 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 6989 | lr 0.0001 | gnorm 86.316 | loss_scale 4 | train_wall 71 | gb_free 37.4 | wall 11806
[2024-09-18 15:57:12,407][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:57:12,422][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 160
[2024-09-18 15:57:12,434][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:57:12,452][fairseq.trainer][INFO] - begin training epoch 160
[2024-09-18 15:57:12,452][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:57:29,833][train_inner][INFO] - epoch 160:     11 / 44 loss=1157.77, ntokens=10325.1, nsentences=47.24, nll_loss=5.297, wps=6078.1, ups=0.59, wpb=10325.1, bsz=47.2, num_updates=7000, lr=0.0001, gnorm=88.954, loss_scale=4, train_wall=320, gb_free=37.2, wall=11823
[2024-09-18 15:58:22,963][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 160 @ 7033 updates
[2024-09-18 15:58:22,963][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:58:26,653][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:58:26,778][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 160 @ 7033 updates, score None) (writing took 3.8148311479890253 seconds)
[2024-09-18 15:58:26,778][fairseq_cli.train][INFO] - end of epoch 160 (average epoch stats below)
[2024-09-18 15:58:26,779][train][INFO] - epoch 160 | loss 1158.82 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 5.3 | wps 6114.5 | ups 0.59 | wpb 10335.2 | bsz 47.3 | num_updates 7033 | lr 0.0001 | gnorm 89.398 | loss_scale 4 | train_wall 70 | gb_free 37.2 | wall 11880
[2024-09-18 15:58:26,780][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:58:26,795][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 161
[2024-09-18 15:58:26,809][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:58:26,827][fairseq.trainer][INFO] - begin training epoch 161
[2024-09-18 15:58:26,827][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 15:59:37,519][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 161 @ 7077 updates
[2024-09-18 15:59:37,520][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:59:41,187][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 15:59:41,311][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 161 @ 7077 updates, score None) (writing took 3.7917376730038086 seconds)
[2024-09-18 15:59:41,311][fairseq_cli.train][INFO] - end of epoch 161 (average epoch stats below)
[2024-09-18 15:59:41,312][train][INFO] - epoch 161 | loss 1154.37 | ntokens 10335.3 | nsentences 47.2727 | nll_loss 5.28 | wps 6101.4 | ups 0.59 | wpb 10335.3 | bsz 47.3 | num_updates 7077 | lr 0.0001 | gnorm 88.823 | loss_scale 4 | train_wall 71 | gb_free 37.3 | wall 11955
[2024-09-18 15:59:41,313][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 15:59:41,329][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 162
[2024-09-18 15:59:41,344][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 15:59:41,362][fairseq.trainer][INFO] - begin training epoch 162
[2024-09-18 15:59:41,363][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:00:52,114][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 162 @ 7121 updates
[2024-09-18 16:00:52,115][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:00:55,791][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:00:55,918][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 162 @ 7121 updates, score None) (writing took 3.8039565389917698 seconds)
[2024-09-18 16:00:55,918][fairseq_cli.train][INFO] - end of epoch 162 (average epoch stats below)
[2024-09-18 16:00:55,919][train][INFO] - epoch 162 | loss 1144.57 | ntokens 10335 | nsentences 47.2727 | nll_loss 5.235 | wps 6095.2 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 7121 | lr 0.0001 | gnorm 86.122 | loss_scale 4 | train_wall 71 | gb_free 37.4 | wall 12029
[2024-09-18 16:00:55,920][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:00:55,935][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 163
[2024-09-18 16:00:55,950][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:00:55,969][fairseq.trainer][INFO] - begin training epoch 163
[2024-09-18 16:00:55,969][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:02:06,913][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 163 @ 7165 updates
[2024-09-18 16:02:06,914][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:02:10,679][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:02:10,803][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 163 @ 7165 updates, score None) (writing took 3.890428864993737 seconds)
[2024-09-18 16:02:10,804][fairseq_cli.train][INFO] - end of epoch 163 (average epoch stats below)
[2024-09-18 16:02:10,805][train][INFO] - epoch 163 | loss 1157.59 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 5.295 | wps 6072.6 | ups 0.59 | wpb 10335.2 | bsz 47.3 | num_updates 7165 | lr 0.0001 | gnorm 95.29 | loss_scale 4 | train_wall 71 | gb_free 37.1 | wall 12104
[2024-09-18 16:02:10,806][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:02:10,821][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 164
[2024-09-18 16:02:10,833][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:02:10,852][fairseq.trainer][INFO] - begin training epoch 164
[2024-09-18 16:02:10,852][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:03:07,620][train_inner][INFO] - epoch 164:     35 / 44 loss=1162.52, ntokens=10339.7, nsentences=46.88, nll_loss=5.271, wps=6122, ups=0.59, wpb=10339.7, bsz=46.9, num_updates=7200, lr=0.0001, gnorm=90.44, loss_scale=4, train_wall=322, gb_free=37.4, wall=12161
[2024-09-18 16:03:21,693][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 164 @ 7209 updates
[2024-09-18 16:03:21,694][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:03:25,349][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:03:25,474][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 164 @ 7209 updates, score None) (writing took 3.780462355003692 seconds)
[2024-09-18 16:03:25,474][fairseq_cli.train][INFO] - end of epoch 164 (average epoch stats below)
[2024-09-18 16:03:25,475][train][INFO] - epoch 164 | loss 1145.02 | ntokens 10335 | nsentences 47.2727 | nll_loss 5.237 | wps 6090.1 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 7209 | lr 0.0001 | gnorm 88.395 | loss_scale 4 | train_wall 71 | gb_free 37.4 | wall 12179
[2024-09-18 16:03:25,476][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:03:25,491][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 165
[2024-09-18 16:03:25,503][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:03:25,522][fairseq.trainer][INFO] - begin training epoch 165
[2024-09-18 16:03:25,522][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:04:36,607][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 165 @ 7253 updates
[2024-09-18 16:04:36,607][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:04:40,314][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:04:40,439][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 165 @ 7253 updates, score None) (writing took 3.832319720997475 seconds)
[2024-09-18 16:04:40,440][fairseq_cli.train][INFO] - end of epoch 165 (average epoch stats below)
[2024-09-18 16:04:40,441][train][INFO] - epoch 165 | loss 1158.28 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 5.298 | wps 6066.3 | ups 0.59 | wpb 10335.4 | bsz 47.3 | num_updates 7253 | lr 0.0001 | gnorm 99.953 | loss_scale 4 | train_wall 71 | gb_free 37.1 | wall 12254
[2024-09-18 16:04:40,441][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:04:40,457][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 166
[2024-09-18 16:04:40,475][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:04:40,494][fairseq.trainer][INFO] - begin training epoch 166
[2024-09-18 16:04:40,494][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:05:50,823][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 166 @ 7297 updates
[2024-09-18 16:05:50,824][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:05:54,497][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:05:54,621][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 166 @ 7297 updates, score None) (writing took 3.7970928860013373 seconds)
[2024-09-18 16:05:54,621][fairseq_cli.train][INFO] - end of epoch 166 (average epoch stats below)
[2024-09-18 16:05:54,622][train][INFO] - epoch 166 | loss 1148.46 | ntokens 10335.3 | nsentences 47.2727 | nll_loss 5.253 | wps 6130.3 | ups 0.59 | wpb 10335.3 | bsz 47.3 | num_updates 7297 | lr 0.0001 | gnorm 88.332 | loss_scale 4 | train_wall 70 | gb_free 37.1 | wall 12328
[2024-09-18 16:05:54,623][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:05:54,638][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 167
[2024-09-18 16:05:54,650][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:05:54,669][fairseq.trainer][INFO] - begin training epoch 167
[2024-09-18 16:05:54,669][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:07:04,360][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 167 @ 7341 updates
[2024-09-18 16:07:04,361][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:07:07,912][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:07:08,033][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 167 @ 7341 updates, score None) (writing took 3.6733251310070045 seconds)
[2024-09-18 16:07:08,034][fairseq_cli.train][INFO] - end of epoch 167 (average epoch stats below)
[2024-09-18 16:07:08,035][train][INFO] - epoch 167 | loss 1143.68 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 5.231 | wps 6194.6 | ups 0.6 | wpb 10335.4 | bsz 47.3 | num_updates 7341 | lr 0.0001 | gnorm 86.156 | loss_scale 4 | train_wall 70 | gb_free 36.8 | wall 12401
[2024-09-18 16:07:08,036][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:07:08,050][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 168
[2024-09-18 16:07:08,063][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:07:08,081][fairseq.trainer][INFO] - begin training epoch 168
[2024-09-18 16:07:08,081][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:08:17,857][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 168 @ 7385 updates
[2024-09-18 16:08:17,858][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:08:21,455][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:08:21,579][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 168 @ 7385 updates, score None) (writing took 3.721731803001603 seconds)
[2024-09-18 16:08:21,579][fairseq_cli.train][INFO] - end of epoch 168 (average epoch stats below)
[2024-09-18 16:08:21,580][train][INFO] - epoch 168 | loss 1122.46 | ntokens 10335 | nsentences 47.2727 | nll_loss 5.134 | wps 6183.2 | ups 0.6 | wpb 10335 | bsz 47.3 | num_updates 7385 | lr 0.0001 | gnorm 72.497 | loss_scale 4 | train_wall 70 | gb_free 37.2 | wall 12475
[2024-09-18 16:08:21,581][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:08:21,596][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 169
[2024-09-18 16:08:21,608][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:08:21,627][fairseq.trainer][INFO] - begin training epoch 169
[2024-09-18 16:08:21,627][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:08:45,059][train_inner][INFO] - epoch 169:     15 / 44 loss=1120.61, ntokens=10318.9, nsentences=48.16, nll_loss=5.23, wps=6116, ups=0.59, wpb=10318.9, bsz=48.2, num_updates=7400, lr=0.0001, gnorm=85.337, loss_scale=4, train_wall=318, gb_free=37.1, wall=12498
[2024-09-18 16:09:31,407][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 169 @ 7429 updates
[2024-09-18 16:09:31,408][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:09:35,034][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:09:35,156][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 169 @ 7429 updates, score None) (writing took 3.748661189005361 seconds)
[2024-09-18 16:09:35,156][fairseq_cli.train][INFO] - end of epoch 169 (average epoch stats below)
[2024-09-18 16:09:35,157][train][INFO] - epoch 169 | loss 1129.7 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 5.167 | wps 6180.8 | ups 0.6 | wpb 10335.4 | bsz 47.3 | num_updates 7429 | lr 0.0001 | gnorm 77.067 | loss_scale 4 | train_wall 70 | gb_free 37.1 | wall 12549
[2024-09-18 16:09:35,158][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:09:35,174][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 170
[2024-09-18 16:09:35,186][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:09:35,205][fairseq.trainer][INFO] - begin training epoch 170
[2024-09-18 16:09:35,205][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:10:44,448][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 170 @ 7473 updates
[2024-09-18 16:10:44,448][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:10:48,037][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:10:48,161][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 170 @ 7473 updates, score None) (writing took 3.713345661992207 seconds)
[2024-09-18 16:10:48,161][fairseq_cli.train][INFO] - end of epoch 170 (average epoch stats below)
[2024-09-18 16:10:48,162][train][INFO] - epoch 170 | loss 1136.53 | ntokens 10334.8 | nsentences 47.2727 | nll_loss 5.199 | wps 6228.8 | ups 0.6 | wpb 10334.8 | bsz 47.3 | num_updates 7473 | lr 0.0001 | gnorm 79.774 | loss_scale 4 | train_wall 69 | gb_free 37.1 | wall 12622
[2024-09-18 16:10:48,163][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:10:48,179][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 171
[2024-09-18 16:10:48,191][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:10:48,210][fairseq.trainer][INFO] - begin training epoch 171
[2024-09-18 16:10:48,210][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:11:57,929][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 171 @ 7517 updates
[2024-09-18 16:11:57,930][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:12:01,518][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:12:01,640][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 171 @ 7517 updates, score None) (writing took 3.7115931439911947 seconds)
[2024-09-18 16:12:01,641][fairseq_cli.train][INFO] - end of epoch 171 (average epoch stats below)
[2024-09-18 16:12:01,642][train][INFO] - epoch 171 | loss 1125.25 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 5.147 | wps 6188.9 | ups 0.6 | wpb 10335.2 | bsz 47.3 | num_updates 7517 | lr 0.0001 | gnorm 81.164 | loss_scale 4 | train_wall 70 | gb_free 37.1 | wall 12695
[2024-09-18 16:12:01,643][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:12:01,658][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 172
[2024-09-18 16:12:01,670][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:12:01,689][fairseq.trainer][INFO] - begin training epoch 172
[2024-09-18 16:12:01,689][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:13:11,123][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 172 @ 7561 updates
[2024-09-18 16:13:11,124][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:13:14,735][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:13:14,858][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 172 @ 7561 updates, score None) (writing took 3.734223829000257 seconds)
[2024-09-18 16:13:14,858][fairseq_cli.train][INFO] - end of epoch 172 (average epoch stats below)
[2024-09-18 16:13:14,859][train][INFO] - epoch 172 | loss 1121.96 | ntokens 10334.8 | nsentences 47.2727 | nll_loss 5.132 | wps 6210.8 | ups 0.6 | wpb 10334.8 | bsz 47.3 | num_updates 7561 | lr 0.0001 | gnorm 78.429 | loss_scale 4 | train_wall 69 | gb_free 36 | wall 12768
[2024-09-18 16:13:14,860][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:13:14,875][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 173
[2024-09-18 16:13:14,887][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:13:14,906][fairseq.trainer][INFO] - begin training epoch 173
[2024-09-18 16:13:14,906][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:14:16,650][train_inner][INFO] - epoch 173:     39 / 44 loss=1139.63, ntokens=10349.2, nsentences=46.92, nll_loss=5.167, wps=6242.2, ups=0.6, wpb=10349.2, bsz=46.9, num_updates=7600, lr=0.0001, gnorm=84.184, loss_scale=4, train_wall=316, gb_free=37.1, wall=12830
[2024-09-18 16:14:24,004][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 173 @ 7605 updates
[2024-09-18 16:14:24,004][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:14:27,494][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:14:27,617][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 173 @ 7605 updates, score None) (writing took 3.613229816008243 seconds)
[2024-09-18 16:14:27,617][fairseq_cli.train][INFO] - end of epoch 173 (average epoch stats below)
[2024-09-18 16:14:27,618][train][INFO] - epoch 173 | loss 1134.91 | ntokens 10334.8 | nsentences 47.2727 | nll_loss 5.191 | wps 6249.9 | ups 0.6 | wpb 10334.8 | bsz 47.3 | num_updates 7605 | lr 0.0001 | gnorm 100.752 | loss_scale 4 | train_wall 69 | gb_free 37.2 | wall 12841
[2024-09-18 16:14:27,619][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:14:27,635][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 174
[2024-09-18 16:14:27,646][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:14:27,665][fairseq.trainer][INFO] - begin training epoch 174
[2024-09-18 16:14:27,665][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:15:37,165][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 174 @ 7649 updates
[2024-09-18 16:15:37,166][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:15:40,763][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:15:40,886][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 174 @ 7649 updates, score None) (writing took 3.7212303299893392 seconds)
[2024-09-18 16:15:40,887][fairseq_cli.train][INFO] - end of epoch 174 (average epoch stats below)
[2024-09-18 16:15:40,888][train][INFO] - epoch 174 | loss 1116.32 | ntokens 10334.8 | nsentences 47.2727 | nll_loss 5.106 | wps 6206.4 | ups 0.6 | wpb 10334.8 | bsz 47.3 | num_updates 7649 | lr 0.0001 | gnorm 75.499 | loss_scale 4 | train_wall 69 | gb_free 37.1 | wall 12914
[2024-09-18 16:15:40,889][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:15:40,904][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 175
[2024-09-18 16:15:40,916][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:15:40,935][fairseq.trainer][INFO] - begin training epoch 175
[2024-09-18 16:15:40,935][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:16:50,665][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 175 @ 7693 updates
[2024-09-18 16:16:50,665][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:16:54,241][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:16:54,362][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 175 @ 7693 updates, score None) (writing took 3.6971025050006574 seconds)
[2024-09-18 16:16:54,362][fairseq_cli.train][INFO] - end of epoch 175 (average epoch stats below)
[2024-09-18 16:16:54,363][train][INFO] - epoch 175 | loss 1126.51 | ntokens 10335.3 | nsentences 47.2727 | nll_loss 5.153 | wps 6189.3 | ups 0.6 | wpb 10335.3 | bsz 47.3 | num_updates 7693 | lr 0.0001 | gnorm 87.491 | loss_scale 4 | train_wall 70 | gb_free 37.3 | wall 12988
[2024-09-18 16:16:54,364][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:16:54,381][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 176
[2024-09-18 16:16:54,393][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:16:54,412][fairseq.trainer][INFO] - begin training epoch 176
[2024-09-18 16:16:54,412][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:18:03,935][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 176 @ 7737 updates
[2024-09-18 16:18:03,936][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:18:07,545][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:18:07,667][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 176 @ 7737 updates, score None) (writing took 3.732607993995771 seconds)
[2024-09-18 16:18:07,668][fairseq_cli.train][INFO] - end of epoch 176 (average epoch stats below)
[2024-09-18 16:18:07,669][train][INFO] - epoch 176 | loss 1114.74 | ntokens 10335.5 | nsentences 47.2727 | nll_loss 5.099 | wps 6203.8 | ups 0.6 | wpb 10335.5 | bsz 47.3 | num_updates 7737 | lr 0.0001 | gnorm 76.605 | loss_scale 4 | train_wall 69 | gb_free 36.2 | wall 13061
[2024-09-18 16:18:07,670][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:18:07,685][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 177
[2024-09-18 16:18:07,697][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:18:07,716][fairseq.trainer][INFO] - begin training epoch 177
[2024-09-18 16:18:07,716][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:19:17,259][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 177 @ 7781 updates
[2024-09-18 16:19:17,259][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:19:20,853][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:19:20,974][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 177 @ 7781 updates, score None) (writing took 3.7152608549949946 seconds)
[2024-09-18 16:19:20,974][fairseq_cli.train][INFO] - end of epoch 177 (average epoch stats below)
[2024-09-18 16:19:20,975][train][INFO] - epoch 177 | loss 1111.69 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 5.085 | wps 6203.4 | ups 0.6 | wpb 10335.2 | bsz 47.3 | num_updates 7781 | lr 0.0001 | gnorm 75.43 | loss_scale 4 | train_wall 69 | gb_free 37.1 | wall 13134
[2024-09-18 16:19:20,976][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:19:20,991][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 178
[2024-09-18 16:19:21,003][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:19:21,022][fairseq.trainer][INFO] - begin training epoch 178
[2024-09-18 16:19:21,022][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:19:51,101][train_inner][INFO] - epoch 178:     19 / 44 loss=1115.11, ntokens=10339.7, nsentences=47.32, nll_loss=5.103, wps=6183.1, ups=0.6, wpb=10339.7, bsz=47.3, num_updates=7800, lr=0.0001, gnorm=78.038, loss_scale=4, train_wall=315, gb_free=37.3, wall=13164
[2024-09-18 16:20:30,888][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 178 @ 7825 updates
[2024-09-18 16:20:30,889][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:20:34,497][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:20:34,619][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 178 @ 7825 updates, score None) (writing took 3.731070453999564 seconds)
[2024-09-18 16:20:34,620][fairseq_cli.train][INFO] - end of epoch 178 (average epoch stats below)
[2024-09-18 16:20:34,621][train][INFO] - epoch 178 | loss 1107.5 | ntokens 10335.4 | nsentences 47.2727 | nll_loss 5.066 | wps 6175.1 | ups 0.6 | wpb 10335.4 | bsz 47.3 | num_updates 7825 | lr 0.0001 | gnorm 70.493 | loss_scale 4 | train_wall 70 | gb_free 35.9 | wall 13208
[2024-09-18 16:20:34,621][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:20:34,637][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 179
[2024-09-18 16:20:34,649][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:20:34,668][fairseq.trainer][INFO] - begin training epoch 179
[2024-09-18 16:20:34,668][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:21:44,168][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 179 @ 7869 updates
[2024-09-18 16:21:44,169][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:21:47,754][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:21:47,876][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 179 @ 7869 updates, score None) (writing took 3.707618234009715 seconds)
[2024-09-18 16:21:47,876][fairseq_cli.train][INFO] - end of epoch 179 (average epoch stats below)
[2024-09-18 16:21:47,877][train][INFO] - epoch 179 | loss 1105.99 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 5.059 | wps 6207.6 | ups 0.6 | wpb 10335.2 | bsz 47.3 | num_updates 7869 | lr 0.0001 | gnorm 75.439 | loss_scale 4 | train_wall 69 | gb_free 37.1 | wall 13281
[2024-09-18 16:21:47,878][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:21:47,893][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 180
[2024-09-18 16:21:47,906][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:21:47,925][fairseq.trainer][INFO] - begin training epoch 180
[2024-09-18 16:21:47,925][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:22:57,624][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 180 @ 7913 updates
[2024-09-18 16:22:57,624][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:23:01,217][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:23:01,341][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 180 @ 7913 updates, score None) (writing took 3.7173355250124587 seconds)
[2024-09-18 16:23:01,341][fairseq_cli.train][INFO] - end of epoch 180 (average epoch stats below)
[2024-09-18 16:23:01,342][train][INFO] - epoch 180 | loss 1109.19 | ntokens 10335.5 | nsentences 47.2727 | nll_loss 5.073 | wps 6190.3 | ups 0.6 | wpb 10335.5 | bsz 47.3 | num_updates 7913 | lr 0.0001 | gnorm 78.274 | loss_scale 4 | train_wall 70 | gb_free 36 | wall 13355
[2024-09-18 16:23:01,343][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:23:01,359][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 181
[2024-09-18 16:23:01,371][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:23:01,390][fairseq.trainer][INFO] - begin training epoch 181
[2024-09-18 16:23:01,391][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:24:10,836][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 181 @ 7957 updates
[2024-09-18 16:24:10,837][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:24:14,431][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:24:14,552][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 181 @ 7957 updates, score None) (writing took 3.7162398979999125 seconds)
[2024-09-18 16:24:14,553][fairseq_cli.train][INFO] - end of epoch 181 (average epoch stats below)
[2024-09-18 16:24:14,554][train][INFO] - epoch 181 | loss 1108.18 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 5.069 | wps 6211.5 | ups 0.6 | wpb 10335.2 | bsz 47.3 | num_updates 7957 | lr 0.0001 | gnorm 74.598 | loss_scale 4 | train_wall 69 | gb_free 37.3 | wall 13428
[2024-09-18 16:24:14,555][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:24:14,570][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 182
[2024-09-18 16:24:14,582][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:24:14,601][fairseq.trainer][INFO] - begin training epoch 182
[2024-09-18 16:24:14,601][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:25:22,745][train_inner][INFO] - epoch 182:     43 / 44 loss=1110.39, ntokens=10341.1, nsentences=47.16, nll_loss=5.064, wps=6236.3, ups=0.6, wpb=10341.1, bsz=47.2, num_updates=8000, lr=0.0001, gnorm=74.518, loss_scale=4, train_wall=316, gb_free=37.1, wall=13496
[2024-09-18 16:25:24,081][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 182 @ 8001 updates
[2024-09-18 16:25:24,082][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:25:27,674][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:25:27,796][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 182 @ 8001 updates, score None) (writing took 3.7149920989904786 seconds)
[2024-09-18 16:25:27,798][fairseq_cli.train][INFO] - end of epoch 182 (average epoch stats below)
[2024-09-18 16:25:27,799][train][INFO] - epoch 182 | loss 1104.78 | ntokens 10335.8 | nsentences 47.2727 | nll_loss 5.053 | wps 6209 | ups 0.6 | wpb 10335.8 | bsz 47.3 | num_updates 8001 | lr 0.0001 | gnorm 72.631 | loss_scale 4 | train_wall 69 | gb_free 37.2 | wall 13501
[2024-09-18 16:25:27,799][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:25:27,815][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 183
[2024-09-18 16:25:27,827][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:25:27,846][fairseq.trainer][INFO] - begin training epoch 183
[2024-09-18 16:25:27,847][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:26:37,291][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 183 @ 8045 updates
[2024-09-18 16:26:37,291][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:26:40,913][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:26:41,031][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 183 @ 8045 updates, score None) (writing took 3.7408145270019304 seconds)
[2024-09-18 16:26:41,032][fairseq_cli.train][INFO] - end of epoch 183 (average epoch stats below)
[2024-09-18 16:26:41,033][train][INFO] - epoch 183 | loss 1109.53 | ntokens 10335.3 | nsentences 47.2727 | nll_loss 5.075 | wps 6209.6 | ups 0.6 | wpb 10335.3 | bsz 47.3 | num_updates 8045 | lr 0.0001 | gnorm 74.85 | loss_scale 4 | train_wall 69 | gb_free 37.1 | wall 13574
[2024-09-18 16:26:41,034][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:26:41,054][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 184
[2024-09-18 16:26:41,066][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:26:41,085][fairseq.trainer][INFO] - begin training epoch 184
[2024-09-18 16:26:41,086][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:27:50,871][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 184 @ 8089 updates
[2024-09-18 16:27:50,872][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:27:54,467][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:27:54,590][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 184 @ 8089 updates, score None) (writing took 3.718643419997534 seconds)
[2024-09-18 16:27:54,590][fairseq_cli.train][INFO] - end of epoch 184 (average epoch stats below)
[2024-09-18 16:27:54,591][train][INFO] - epoch 184 | loss 1101.95 | ntokens 10335.8 | nsentences 47.2727 | nll_loss 5.04 | wps 6182.6 | ups 0.6 | wpb 10335.8 | bsz 47.3 | num_updates 8089 | lr 0.0001 | gnorm 74.258 | loss_scale 4 | train_wall 70 | gb_free 37.1 | wall 13648
[2024-09-18 16:27:54,592][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:27:54,607][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 185
[2024-09-18 16:27:54,619][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:27:54,639][fairseq.trainer][INFO] - begin training epoch 185
[2024-09-18 16:27:54,639][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:29:03,926][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 185 @ 8133 updates
[2024-09-18 16:29:03,927][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:29:07,492][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:29:07,617][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 185 @ 8133 updates, score None) (writing took 3.6902854290092364 seconds)
[2024-09-18 16:29:07,617][fairseq_cli.train][INFO] - end of epoch 185 (average epoch stats below)
[2024-09-18 16:29:07,618][train][INFO] - epoch 185 | loss 1105.76 | ntokens 10335.5 | nsentences 47.2727 | nll_loss 5.058 | wps 6227.5 | ups 0.6 | wpb 10335.5 | bsz 47.3 | num_updates 8133 | lr 0.0001 | gnorm 74.324 | loss_scale 4 | train_wall 69 | gb_free 37.5 | wall 13721
[2024-09-18 16:29:07,619][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:29:07,634][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 186
[2024-09-18 16:29:07,651][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:29:07,672][fairseq.trainer][INFO] - begin training epoch 186
[2024-09-18 16:29:07,672][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:30:17,383][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 186 @ 8177 updates
[2024-09-18 16:30:17,384][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:30:20,911][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:30:21,035][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 186 @ 8177 updates, score None) (writing took 3.6515220119908918 seconds)
[2024-09-18 16:30:21,035][fairseq_cli.train][INFO] - end of epoch 186 (average epoch stats below)
[2024-09-18 16:30:21,036][train][INFO] - epoch 186 | loss 1105.71 | ntokens 10335.3 | nsentences 47.2727 | nll_loss 5.057 | wps 6194.1 | ups 0.6 | wpb 10335.3 | bsz 47.3 | num_updates 8177 | lr 0.0001 | gnorm 74.558 | loss_scale 4 | train_wall 70 | gb_free 37.2 | wall 13794
[2024-09-18 16:30:21,037][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:30:21,052][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 187
[2024-09-18 16:30:21,065][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:30:21,084][fairseq.trainer][INFO] - begin training epoch 187
[2024-09-18 16:30:21,085][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:30:58,414][train_inner][INFO] - epoch 187:     23 / 44 loss=1097.76, ntokens=10314.8, nsentences=47.6, nll_loss=5.066, wps=6145.8, ups=0.6, wpb=10314.8, bsz=47.6, num_updates=8200, lr=0.0001, gnorm=75.048, loss_scale=4, train_wall=316, gb_free=37.2, wall=13832
[2024-09-18 16:31:34,127][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 187 @ 8221 updates
[2024-09-18 16:31:34,128][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:31:37,699][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:31:37,822][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 187 @ 8221 updates, score None) (writing took 3.695169339000131 seconds)
[2024-09-18 16:31:37,823][fairseq_cli.train][INFO] - end of epoch 187 (average epoch stats below)
[2024-09-18 16:31:37,824][train][INFO] - epoch 187 | loss 1104.24 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 5.051 | wps 5922.3 | ups 0.57 | wpb 10335.2 | bsz 47.3 | num_updates 8221 | lr 0.0001 | gnorm 76.068 | loss_scale 4 | train_wall 73 | gb_free 37.1 | wall 13871
[2024-09-18 16:31:37,824][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:31:37,839][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 188
[2024-09-18 16:31:37,851][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:31:37,871][fairseq.trainer][INFO] - begin training epoch 188
[2024-09-18 16:31:37,872][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:32:47,852][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 188 @ 8265 updates
[2024-09-18 16:32:47,852][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:32:51,419][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:32:51,541][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 188 @ 8265 updates, score None) (writing took 3.6895262479956727 seconds)
[2024-09-18 16:32:51,541][fairseq_cli.train][INFO] - end of epoch 188 (average epoch stats below)
[2024-09-18 16:32:51,542][train][INFO] - epoch 188 | loss 1113.74 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 5.094 | wps 6168.8 | ups 0.6 | wpb 10335.2 | bsz 47.3 | num_updates 8265 | lr 0.0001 | gnorm 80.14 | loss_scale 4 | train_wall 70 | gb_free 37.3 | wall 13945
[2024-09-18 16:32:51,543][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:32:51,558][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 189
[2024-09-18 16:32:51,573][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:32:51,593][fairseq.trainer][INFO] - begin training epoch 189
[2024-09-18 16:32:51,594][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:34:01,045][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 189 @ 8309 updates
[2024-09-18 16:34:01,045][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:34:04,650][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:34:04,774][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 189 @ 8309 updates, score None) (writing took 3.7288747249986045 seconds)
[2024-09-18 16:34:04,774][fairseq_cli.train][INFO] - end of epoch 189 (average epoch stats below)
[2024-09-18 16:34:04,775][train][INFO] - epoch 189 | loss 1087.42 | ntokens 10335.2 | nsentences 47.2727 | nll_loss 4.974 | wps 6209.7 | ups 0.6 | wpb 10335.2 | bsz 47.3 | num_updates 8309 | lr 0.0001 | gnorm 69.482 | loss_scale 4 | train_wall 69 | gb_free 37.2 | wall 14018
[2024-09-18 16:34:04,776][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:34:04,791][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 190
[2024-09-18 16:34:04,805][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:34:04,824][fairseq.trainer][INFO] - begin training epoch 190
[2024-09-18 16:34:04,824][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:35:14,175][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 190 @ 8353 updates
[2024-09-18 16:35:14,175][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:35:17,740][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:35:17,865][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 190 @ 8353 updates, score None) (writing took 3.690282527997624 seconds)
[2024-09-18 16:35:17,865][fairseq_cli.train][INFO] - end of epoch 190 (average epoch stats below)
[2024-09-18 16:35:17,866][train][INFO] - epoch 190 | loss 1087.39 | ntokens 10335.8 | nsentences 47.2727 | nll_loss 4.973 | wps 6222.1 | ups 0.6 | wpb 10335.8 | bsz 47.3 | num_updates 8353 | lr 0.0001 | gnorm 73.782 | loss_scale 4 | train_wall 69 | gb_free 37.1 | wall 14091
[2024-09-18 16:35:17,867][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:35:17,883][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 191
[2024-09-18 16:35:17,898][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:35:17,918][fairseq.trainer][INFO] - begin training epoch 191
[2024-09-18 16:35:17,918][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:36:30,206][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 191 @ 8397 updates
[2024-09-18 16:36:30,207][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:36:33,861][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:36:33,985][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 191 @ 8397 updates, score None) (writing took 3.77881743600301 seconds)
[2024-09-18 16:36:33,985][fairseq_cli.train][INFO] - end of epoch 191 (average epoch stats below)
[2024-09-18 16:36:33,986][train][INFO] - epoch 191 | loss 1090.06 | ntokens 10335 | nsentences 47.2727 | nll_loss 4.986 | wps 5974.1 | ups 0.58 | wpb 10335 | bsz 47.3 | num_updates 8397 | lr 0.0001 | gnorm 64.588 | loss_scale 4 | train_wall 72 | gb_free 37.2 | wall 14167
[2024-09-18 16:36:33,987][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:36:34,002][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 192
[2024-09-18 16:36:34,015][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:36:34,036][fairseq.trainer][INFO] - begin training epoch 192
[2024-09-18 16:36:34,036][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:36:39,268][train_inner][INFO] - epoch 192:      3 / 44 loss=1104.56, ntokens=10347, nsentences=46.84, nll_loss=5, wps=6071.2, ups=0.59, wpb=10347, bsz=46.8, num_updates=8400, lr=0.0001, gnorm=72.319, loss_scale=4, train_wall=321, gb_free=37.3, wall=14173
[2024-09-18 16:37:45,969][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 192 @ 8441 updates
[2024-09-18 16:37:45,970][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:37:49,559][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:37:49,685][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 192 @ 8441 updates, score None) (writing took 3.715810470006545 seconds)
[2024-09-18 16:37:49,685][fairseq_cli.train][INFO] - end of epoch 192 (average epoch stats below)
[2024-09-18 16:37:49,686][train][INFO] - epoch 192 | loss 1077.06 | ntokens 10334.8 | nsentences 47.2727 | nll_loss 4.927 | wps 6007.1 | ups 0.58 | wpb 10334.8 | bsz 47.3 | num_updates 8441 | lr 0.0001 | gnorm 69.883 | loss_scale 4 | train_wall 72 | gb_free 37.6 | wall 14243
[2024-09-18 16:37:49,687][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:37:49,703][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 193
[2024-09-18 16:37:49,715][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:37:49,735][fairseq.trainer][INFO] - begin training epoch 193
[2024-09-18 16:37:49,735][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:38:59,710][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 193 @ 8485 updates
[2024-09-18 16:38:59,711][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:39:03,304][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:39:03,425][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 193 @ 8485 updates, score None) (writing took 3.714662883998244 seconds)
[2024-09-18 16:39:03,425][fairseq_cli.train][INFO] - end of epoch 193 (average epoch stats below)
[2024-09-18 16:39:03,426][train][INFO] - epoch 193 | loss 1091.65 | ntokens 10335 | nsentences 47.2727 | nll_loss 4.993 | wps 6166.9 | ups 0.6 | wpb 10335 | bsz 47.3 | num_updates 8485 | lr 0.0001 | gnorm 78.868 | loss_scale 4 | train_wall 70 | gb_free 37.1 | wall 14317
[2024-09-18 16:39:03,427][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:39:03,442][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 194
[2024-09-18 16:39:03,454][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:39:03,474][fairseq.trainer][INFO] - begin training epoch 194
[2024-09-18 16:39:03,474][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:40:13,446][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 194 @ 8529 updates
[2024-09-18 16:40:13,446][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:40:17,088][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:40:17,214][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 194 @ 8529 updates, score None) (writing took 3.768072516002576 seconds)
[2024-09-18 16:40:17,214][fairseq_cli.train][INFO] - end of epoch 194 (average epoch stats below)
[2024-09-18 16:40:17,215][train][INFO] - epoch 194 | loss 1083.75 | ntokens 10335.5 | nsentences 47.2727 | nll_loss 4.957 | wps 6163.1 | ups 0.6 | wpb 10335.5 | bsz 47.3 | num_updates 8529 | lr 0.0001 | gnorm 67.905 | loss_scale 4 | train_wall 70 | gb_free 35.6 | wall 14391
[2024-09-18 16:40:17,216][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:40:17,231][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 195
[2024-09-18 16:40:17,244][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:40:17,265][fairseq.trainer][INFO] - begin training epoch 195
[2024-09-18 16:40:17,265][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:41:27,753][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 195 @ 8573 updates
[2024-09-18 16:41:27,754][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:41:31,373][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:41:31,494][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 195 @ 8573 updates, score None) (writing took 3.7407199559966102 seconds)
[2024-09-18 16:41:31,494][fairseq_cli.train][INFO] - end of epoch 195 (average epoch stats below)
[2024-09-18 16:41:31,495][train][INFO] - epoch 195 | loss 1094 | ntokens 10335 | nsentences 47.2727 | nll_loss 5.004 | wps 6122.1 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 8573 | lr 0.0001 | gnorm 71.19 | loss_scale 4 | train_wall 70 | gb_free 37.2 | wall 14465
[2024-09-18 16:41:31,496][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:41:31,514][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 196
[2024-09-18 16:41:31,530][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:41:31,550][fairseq.trainer][INFO] - begin training epoch 196
[2024-09-18 16:41:31,550][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-18 16:42:14,939][train_inner][INFO] - epoch 196:     27 / 44 loss=1079.63, ntokens=10354.5, nsentences=47.64, nll_loss=4.967, wps=6169.7, ups=0.6, wpb=10354.5, bsz=47.6, num_updates=8600, lr=0.0001, gnorm=70.781, loss_scale=4, train_wall=320, gb_free=35.6, wall=14508
[2024-09-18 16:42:42,179][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 196 @ 8617 updates
[2024-09-18 16:42:42,179][fairseq.trainer][INFO] - Saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:42:45,863][fairseq.trainer][INFO] - Finished saving checkpoint to /media/ahmed/DATA 1/Research/Projects/finetune_w2v_fairseq/model_outputs/continued_pretraining/outputs/2024-09-18/12-40-21/ckpts/checkpoint_last.pt
[2024-09-18 16:42:45,982][fairseq.checkpoint_utils][INFO] - Saved checkpoint ckpts/checkpoint_last.pt (epoch 196 @ 8617 updates, score None) (writing took 3.803397679992486 seconds)
[2024-09-18 16:42:45,982][fairseq_cli.train][INFO] - end of epoch 196 (average epoch stats below)
[2024-09-18 16:42:45,983][train][INFO] - epoch 196 | loss 1084.86 | ntokens 10335 | nsentences 47.2727 | nll_loss 4.962 | wps 6105 | ups 0.59 | wpb 10335 | bsz 47.3 | num_updates 8617 | lr 0.0001 | gnorm 68.513 | loss_scale 4 | train_wall 71 | gb_free 35.7 | wall 14539
[2024-09-18 16:42:45,984][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-18 16:42:46,003][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 197
[2024-09-18 16:42:46,017][fairseq.data.iterators][INFO] - grouped total_num_itrs = 44
[2024-09-18 16:42:46,038][fairseq.trainer][INFO] - begin training epoch 197
[2024-09-18 16:42:46,038][fairseq_cli.train][INFO] - Start iterating over samples
